{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIC-3670 NLP UC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Versiones de librerías, python 3.8.10\n",
    "\n",
    "- numpy 1.20.3\n",
    "- flair 0.12\n",
    "- allennlp 0.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad en clase\n",
    "\n",
    "Vamos a hacer fine tuning para NER tagger usando el tagger preentrenado de **flair**. Para esto haga lo siguiente:\n",
    "\n",
    "- Importe el dataset CONLL_03_SPANISH.\n",
    "- Cree el diccionario de labels a partir del corpus.\n",
    "- Seleccione un pretrained BERT transformer encoder y cárguelo. Justifique su decisión. Revise la lista de modelos disponibles en https://huggingface.co/\n",
    "- Use los embeddings para hacer fine tuning del tagger preentrenado de flair.\n",
    "- Muestre un ejemplo de su ner funcionando.\n",
    "- Cuanto termine, me avisa para entregarle una **L (logrado)**.\n",
    "- Recuerde que las L otorgan un bono en la nota final de la asignatura.\n",
    "\n",
    "\n",
    "***Tiene hasta el final de la clase.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:41:59,455 Reading data from /home/marcelo/.flair/datasets/conll_03_spanish\n",
      "2024-04-17 13:41:59,456 Train: /home/marcelo/.flair/datasets/conll_03_spanish/esp.train\n",
      "2024-04-17 13:41:59,456 Dev: /home/marcelo/.flair/datasets/conll_03_spanish/esp.testa\n",
      "2024-04-17 13:41:59,456 Test: /home/marcelo/.flair/datasets/conll_03_spanish/esp.testb\n",
      "Corpus: 8323 train + 1915 dev + 1517 test sentences\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.data import Sentence\n",
    "import flair.datasets\n",
    "\n",
    "corpus = flair.datasets.CONLL_03_SPANISH()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:42:05,677 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8323it [00:00, 35528.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:42:05,937 Dictionary created for label 'ner' with 4 values: ORG (seen 7390 times), LOC (seen 4914 times), PER (seen 4321 times), MISC (seen 2173 times)\n",
      "Dictionary with 4 tags: ORG, LOC, PER, MISC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_dict = corpus.make_label_dictionary(label_type='ner', add_unk=False)\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = TransformerWordEmbeddings(model='xlm-roberta-base',\n",
    "                                       layers=\"-1\",\n",
    "                                       layer_mean=False,\n",
    "                                       subtoken_pooling=\"first\",\n",
    "                                       fine_tune=True,\n",
    "                                       use_context=True,\n",
    "                                       model_max_length=512,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:42:33,511 SequenceTagger predicts: Dictionary with 17 tags: O, S-ORG, B-ORG, E-ORG, I-ORG, S-LOC, B-LOC, E-LOC, I-LOC, S-PER, B-PER, E-PER, I-PER, S-MISC, B-MISC, E-MISC, I-MISC\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "tagger = SequenceTagger(\n",
    "    hidden_size=256,\n",
    "    embeddings=embeddings,\n",
    "    tag_dictionary=label_dict,\n",
    "    tag_type='ner',\n",
    "    tag_format=\"BIOES\",\n",
    "    use_crf=False,\n",
    "    use_rnn=False,\n",
    "    reproject_embeddings=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = ModelTrainer(tagger, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathered 16792 of total 250002\n",
      "Reducing vocab size by 93.2833%\n",
      "Reducing model size by 64.4163%\n",
      "Reducing training parameter count by 64.4163%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:42:41,959 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:42:41,961 Model: \"SequenceTagger(\n",
      "  (embeddings): TransformerWordEmbeddings(\n",
      "    (model): XLMRobertaModel(\n",
      "      (embeddings): XLMRobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(250003, 768)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): XLMRobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): XLMRobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (linear): Linear(in_features=768, out_features=17, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:42:41,963 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:42:41,964 Corpus: \"Corpus: 8323 train + 1915 dev + 1517 test sentences\"\n",
      "2024-04-17 13:42:41,965 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:42:41,965 Parameters:\n",
      "2024-04-17 13:42:41,966  - learning_rate: \"0.000020\"\n",
      "2024-04-17 13:42:41,966  - mini_batch_size: \"4\"\n",
      "2024-04-17 13:42:41,967  - patience: \"3\"\n",
      "2024-04-17 13:42:41,969  - anneal_factor: \"0.5\"\n",
      "2024-04-17 13:42:41,970  - max_epochs: \"8\"\n",
      "2024-04-17 13:42:41,971  - shuffle: \"False\"\n",
      "2024-04-17 13:42:41,973  - train_with_dev: \"False\"\n",
      "2024-04-17 13:42:41,974  - batch_growth_annealing: \"False\"\n",
      "2024-04-17 13:42:41,975 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:42:41,976 Model training base path: \"resources/taggers/ner-xlm-roberta-base\"\n",
      "2024-04-17 13:42:41,977 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:42:41,978 Device: cuda:0\n",
      "2024-04-17 13:42:41,979 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:42:41,980 Embeddings storage mode: none\n",
      "2024-04-17 13:42:41,981 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:43:02,914 epoch 1 - iter 208/2081 - loss 2.94742039 - time (sec): 20.93 - samples/sec: 1277.10 - lr: 0.000002\n",
      "2024-04-17 13:43:24,037 epoch 1 - iter 416/2081 - loss 1.70539631 - time (sec): 42.06 - samples/sec: 1310.14 - lr: 0.000005\n",
      "2024-04-17 13:43:44,861 epoch 1 - iter 624/2081 - loss 1.33577667 - time (sec): 62.88 - samples/sec: 1204.84 - lr: 0.000007\n",
      "2024-04-17 13:44:06,039 epoch 1 - iter 832/2081 - loss 1.06912779 - time (sec): 84.06 - samples/sec: 1209.54 - lr: 0.000010\n",
      "2024-04-17 13:44:27,243 epoch 1 - iter 1040/2081 - loss 0.87518612 - time (sec): 105.26 - samples/sec: 1230.27 - lr: 0.000012\n",
      "2024-04-17 13:44:49,271 epoch 1 - iter 1248/2081 - loss 0.74855415 - time (sec): 127.29 - samples/sec: 1241.14 - lr: 0.000015\n",
      "2024-04-17 13:45:10,387 epoch 1 - iter 1456/2081 - loss 0.65192876 - time (sec): 148.40 - samples/sec: 1256.47 - lr: 0.000017\n",
      "2024-04-17 13:45:31,702 epoch 1 - iter 1664/2081 - loss 0.58680856 - time (sec): 169.72 - samples/sec: 1249.78 - lr: 0.000020\n",
      "2024-04-17 13:45:52,927 epoch 1 - iter 1872/2081 - loss 0.52825636 - time (sec): 190.94 - samples/sec: 1263.84 - lr: 0.000020\n",
      "2024-04-17 13:46:13,749 epoch 1 - iter 2080/2081 - loss 0.49186651 - time (sec): 211.77 - samples/sec: 1249.45 - lr: 0.000019\n",
      "2024-04-17 13:46:13,858 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:46:13,860 EPOCH 1 done: loss 0.4916 - lr 0.000019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:12<00:00, 39.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:46:25,958 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:46:26,056 DEV : loss 0.11206331849098206 - f1-score (micro avg)  0.8252\n",
      "2024-04-17 13:46:26,118 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:46:46,831 epoch 2 - iter 208/2081 - loss 0.11012243 - time (sec): 20.71 - samples/sec: 1290.69 - lr: 0.000019\n",
      "2024-04-17 13:47:07,607 epoch 2 - iter 416/2081 - loss 0.10364780 - time (sec): 41.49 - samples/sec: 1328.06 - lr: 0.000019\n",
      "2024-04-17 13:47:28,325 epoch 2 - iter 624/2081 - loss 0.10068833 - time (sec): 62.21 - samples/sec: 1217.87 - lr: 0.000019\n",
      "2024-04-17 13:47:49,171 epoch 2 - iter 832/2081 - loss 0.09825677 - time (sec): 83.05 - samples/sec: 1224.19 - lr: 0.000018\n",
      "2024-04-17 13:48:10,443 epoch 2 - iter 1040/2081 - loss 0.09605382 - time (sec): 104.32 - samples/sec: 1241.32 - lr: 0.000018\n",
      "2024-04-17 13:48:31,669 epoch 2 - iter 1248/2081 - loss 0.10284990 - time (sec): 125.55 - samples/sec: 1258.34 - lr: 0.000018\n",
      "2024-04-17 13:48:52,237 epoch 2 - iter 1456/2081 - loss 0.09884655 - time (sec): 146.12 - samples/sec: 1276.14 - lr: 0.000018\n",
      "2024-04-17 13:49:12,753 epoch 2 - iter 1664/2081 - loss 0.09825852 - time (sec): 166.63 - samples/sec: 1272.93 - lr: 0.000017\n",
      "2024-04-17 13:49:32,988 epoch 2 - iter 1872/2081 - loss 0.09593925 - time (sec): 186.87 - samples/sec: 1291.41 - lr: 0.000017\n",
      "2024-04-17 13:49:53,807 epoch 2 - iter 2080/2081 - loss 0.09586483 - time (sec): 207.69 - samples/sec: 1273.99 - lr: 0.000017\n",
      "2024-04-17 13:49:53,911 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:49:53,913 EPOCH 2 done: loss 0.0958 - lr 0.000017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:15<00:00, 31.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:50:09,281 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:50:09,320 DEV : loss 0.1204131469130516 - f1-score (micro avg)  0.8823\n",
      "2024-04-17 13:50:09,351 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:50:30,213 epoch 3 - iter 208/2081 - loss 0.08337261 - time (sec): 20.86 - samples/sec: 1281.45 - lr: 0.000016\n",
      "2024-04-17 13:50:51,347 epoch 3 - iter 416/2081 - loss 0.07894309 - time (sec): 41.99 - samples/sec: 1312.02 - lr: 0.000016\n",
      "2024-04-17 13:51:12,524 epoch 3 - iter 624/2081 - loss 0.07657264 - time (sec): 63.17 - samples/sec: 1199.24 - lr: 0.000016\n",
      "2024-04-17 13:51:33,683 epoch 3 - iter 832/2081 - loss 0.07614358 - time (sec): 84.33 - samples/sec: 1205.61 - lr: 0.000016\n",
      "2024-04-17 13:51:54,490 epoch 3 - iter 1040/2081 - loss 0.07454950 - time (sec): 105.14 - samples/sec: 1231.71 - lr: 0.000015\n",
      "2024-04-17 13:52:15,695 epoch 3 - iter 1248/2081 - loss 0.08286486 - time (sec): 126.34 - samples/sec: 1250.43 - lr: 0.000015\n",
      "2024-04-17 13:52:36,520 epoch 3 - iter 1456/2081 - loss 0.07994908 - time (sec): 147.17 - samples/sec: 1267.03 - lr: 0.000015\n",
      "2024-04-17 13:52:57,539 epoch 3 - iter 1664/2081 - loss 0.08315875 - time (sec): 168.19 - samples/sec: 1261.18 - lr: 0.000014\n",
      "2024-04-17 13:53:18,610 epoch 3 - iter 1872/2081 - loss 0.08040031 - time (sec): 189.26 - samples/sec: 1275.10 - lr: 0.000014\n",
      "2024-04-17 13:53:39,729 epoch 3 - iter 2080/2081 - loss 0.08105785 - time (sec): 210.38 - samples/sec: 1257.70 - lr: 0.000014\n",
      "2024-04-17 13:53:39,839 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:53:39,841 EPOCH 3 done: loss 0.0810 - lr 0.000014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:14<00:00, 32.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:53:54,417 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:53:54,450 DEV : loss 0.12456652522087097 - f1-score (micro avg)  0.8861\n",
      "2024-04-17 13:53:54,477 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:54:15,251 epoch 4 - iter 208/2081 - loss 0.07728391 - time (sec): 20.77 - samples/sec: 1286.89 - lr: 0.000014\n",
      "2024-04-17 13:54:36,178 epoch 4 - iter 416/2081 - loss 0.06538405 - time (sec): 41.70 - samples/sec: 1321.29 - lr: 0.000013\n",
      "2024-04-17 13:54:56,688 epoch 4 - iter 624/2081 - loss 0.06475300 - time (sec): 62.21 - samples/sec: 1217.79 - lr: 0.000013\n",
      "2024-04-17 13:55:17,433 epoch 4 - iter 832/2081 - loss 0.06386773 - time (sec): 82.95 - samples/sec: 1225.61 - lr: 0.000013\n",
      "2024-04-17 13:55:38,362 epoch 4 - iter 1040/2081 - loss 0.06194995 - time (sec): 103.88 - samples/sec: 1246.58 - lr: 0.000013\n",
      "2024-04-17 13:55:58,926 epoch 4 - iter 1248/2081 - loss 0.06731973 - time (sec): 124.45 - samples/sec: 1269.47 - lr: 0.000012\n",
      "2024-04-17 13:56:19,917 epoch 4 - iter 1456/2081 - loss 0.06470174 - time (sec): 145.44 - samples/sec: 1282.09 - lr: 0.000012\n",
      "2024-04-17 13:56:40,738 epoch 4 - iter 1664/2081 - loss 0.06709802 - time (sec): 166.26 - samples/sec: 1275.79 - lr: 0.000012\n",
      "2024-04-17 13:57:01,592 epoch 4 - iter 1872/2081 - loss 0.06513649 - time (sec): 187.11 - samples/sec: 1289.71 - lr: 0.000011\n",
      "2024-04-17 13:57:23,050 epoch 4 - iter 2080/2081 - loss 0.06597493 - time (sec): 208.57 - samples/sec: 1268.58 - lr: 0.000011\n",
      "2024-04-17 13:57:23,182 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:57:23,184 EPOCH 4 done: loss 0.0659 - lr 0.000011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:14<00:00, 32.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:57:37,950 Evaluating as a multi-label problem: False\n",
      "2024-04-17 13:57:38,013 DEV : loss 0.1320403665304184 - f1-score (micro avg)  0.8966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:57:38,051 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:57:58,745 epoch 5 - iter 208/2081 - loss 0.05968133 - time (sec): 20.69 - samples/sec: 1291.86 - lr: 0.000011\n",
      "2024-04-17 13:58:19,746 epoch 5 - iter 416/2081 - loss 0.05171991 - time (sec): 41.69 - samples/sec: 1321.49 - lr: 0.000011\n",
      "2024-04-17 13:58:40,577 epoch 5 - iter 624/2081 - loss 0.04967908 - time (sec): 62.53 - samples/sec: 1211.64 - lr: 0.000010\n",
      "2024-04-17 13:59:01,404 epoch 5 - iter 832/2081 - loss 0.04954458 - time (sec): 83.35 - samples/sec: 1219.78 - lr: 0.000010\n",
      "2024-04-17 13:59:22,220 epoch 5 - iter 1040/2081 - loss 0.04699391 - time (sec): 104.17 - samples/sec: 1243.18 - lr: 0.000010\n",
      "2024-04-17 13:59:43,344 epoch 5 - iter 1248/2081 - loss 0.04909439 - time (sec): 125.29 - samples/sec: 1260.92 - lr: 0.000009\n",
      "2024-04-17 14:00:04,293 epoch 5 - iter 1456/2081 - loss 0.04715321 - time (sec): 146.24 - samples/sec: 1275.06 - lr: 0.000009\n",
      "2024-04-17 14:00:25,405 epoch 5 - iter 1664/2081 - loss 0.04742514 - time (sec): 167.35 - samples/sec: 1267.46 - lr: 0.000009\n",
      "2024-04-17 14:00:46,318 epoch 5 - iter 1872/2081 - loss 0.04783421 - time (sec): 188.27 - samples/sec: 1281.82 - lr: 0.000009\n",
      "2024-04-17 14:01:07,153 epoch 5 - iter 2080/2081 - loss 0.04865319 - time (sec): 209.10 - samples/sec: 1265.38 - lr: 0.000008\n",
      "2024-04-17 14:01:07,260 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 14:01:07,262 EPOCH 5 done: loss 0.0486 - lr 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:14<00:00, 33.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 14:01:21,727 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 14:01:21,766 DEV : loss 0.13759379088878632 - f1-score (micro avg)  0.8979\n",
      "2024-04-17 14:01:21,796 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 14:01:42,829 epoch 6 - iter 208/2081 - loss 0.04441864 - time (sec): 21.03 - samples/sec: 1271.01 - lr: 0.000008\n",
      "2024-04-17 14:02:03,733 epoch 6 - iter 416/2081 - loss 0.03891758 - time (sec): 41.94 - samples/sec: 1313.88 - lr: 0.000008\n",
      "2024-04-17 14:02:24,550 epoch 6 - iter 624/2081 - loss 0.03655828 - time (sec): 62.75 - samples/sec: 1207.24 - lr: 0.000008\n",
      "2024-04-17 14:02:45,448 epoch 6 - iter 832/2081 - loss 0.03705826 - time (sec): 83.65 - samples/sec: 1215.42 - lr: 0.000007\n",
      "2024-04-17 14:03:06,616 epoch 6 - iter 1040/2081 - loss 0.03533833 - time (sec): 104.82 - samples/sec: 1235.46 - lr: 0.000007\n",
      "2024-04-17 14:03:27,613 epoch 6 - iter 1248/2081 - loss 0.03708164 - time (sec): 125.82 - samples/sec: 1255.67 - lr: 0.000007\n",
      "2024-04-17 14:03:49,028 epoch 6 - iter 1456/2081 - loss 0.03634663 - time (sec): 147.23 - samples/sec: 1266.49 - lr: 0.000006\n",
      "2024-04-17 14:04:09,956 epoch 6 - iter 1664/2081 - loss 0.03815646 - time (sec): 168.16 - samples/sec: 1261.39 - lr: 0.000006\n",
      "2024-04-17 14:04:31,071 epoch 6 - iter 1872/2081 - loss 0.03752233 - time (sec): 189.27 - samples/sec: 1275.00 - lr: 0.000006\n",
      "2024-04-17 14:04:52,052 epoch 6 - iter 2080/2081 - loss 0.03787409 - time (sec): 210.25 - samples/sec: 1258.43 - lr: 0.000006\n",
      "2024-04-17 14:04:52,170 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 14:04:52,172 EPOCH 6 done: loss 0.0379 - lr 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:14<00:00, 32.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 14:05:06,858 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 14:05:06,940 DEV : loss 0.14882870018482208 - f1-score (micro avg)  0.8915\n",
      "2024-04-17 14:05:06,982 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 14:05:27,900 epoch 7 - iter 208/2081 - loss 0.03495998 - time (sec): 20.92 - samples/sec: 1278.05 - lr: 0.000005\n",
      "2024-04-17 14:05:48,741 epoch 7 - iter 416/2081 - loss 0.03061950 - time (sec): 41.76 - samples/sec: 1319.46 - lr: 0.000005\n",
      "2024-04-17 14:06:09,686 epoch 7 - iter 624/2081 - loss 0.02930267 - time (sec): 62.70 - samples/sec: 1208.22 - lr: 0.000005\n",
      "2024-04-17 14:06:30,837 epoch 7 - iter 832/2081 - loss 0.02913596 - time (sec): 83.85 - samples/sec: 1212.48 - lr: 0.000004\n",
      "2024-04-17 14:06:51,969 epoch 7 - iter 1040/2081 - loss 0.02776395 - time (sec): 104.99 - samples/sec: 1233.49 - lr: 0.000004\n",
      "2024-04-17 14:07:13,240 epoch 7 - iter 1248/2081 - loss 0.02945623 - time (sec): 126.26 - samples/sec: 1251.29 - lr: 0.000004\n",
      "2024-04-17 14:07:34,471 epoch 7 - iter 1456/2081 - loss 0.02930142 - time (sec): 147.49 - samples/sec: 1264.28 - lr: 0.000004\n",
      "2024-04-17 14:07:55,650 epoch 7 - iter 1664/2081 - loss 0.03043649 - time (sec): 168.67 - samples/sec: 1257.59 - lr: 0.000003\n",
      "2024-04-17 14:08:16,905 epoch 7 - iter 1872/2081 - loss 0.02994729 - time (sec): 189.92 - samples/sec: 1270.64 - lr: 0.000003\n",
      "2024-04-17 14:08:37,539 epoch 7 - iter 2080/2081 - loss 0.03075572 - time (sec): 210.56 - samples/sec: 1256.63 - lr: 0.000003\n",
      "2024-04-17 14:08:37,642 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 14:08:37,644 EPOCH 7 done: loss 0.0307 - lr 0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:14<00:00, 32.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 14:08:52,398 Evaluating as a multi-label problem: False\n",
      "2024-04-17 14:08:52,477 DEV : loss 0.16359102725982666 - f1-score (micro avg)  0.8935\n",
      "2024-04-17 14:08:52,517 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 14:09:13,426 epoch 8 - iter 208/2081 - loss 0.03400685 - time (sec): 20.91 - samples/sec: 1278.56 - lr: 0.000003\n",
      "2024-04-17 14:09:34,127 epoch 8 - iter 416/2081 - loss 0.02750880 - time (sec): 41.61 - samples/sec: 1324.20 - lr: 0.000002\n",
      "2024-04-17 14:09:54,675 epoch 8 - iter 624/2081 - loss 0.02727541 - time (sec): 62.16 - samples/sec: 1218.81 - lr: 0.000002\n",
      "2024-04-17 14:10:15,552 epoch 8 - iter 832/2081 - loss 0.02537403 - time (sec): 83.03 - samples/sec: 1224.44 - lr: 0.000002\n",
      "2024-04-17 14:10:37,179 epoch 8 - iter 1040/2081 - loss 0.02372918 - time (sec): 104.66 - samples/sec: 1237.32 - lr: 0.000001\n",
      "2024-04-17 14:10:58,142 epoch 8 - iter 1248/2081 - loss 0.02623760 - time (sec): 125.62 - samples/sec: 1257.59 - lr: 0.000001\n",
      "2024-04-17 14:11:18,827 epoch 8 - iter 1456/2081 - loss 0.02709434 - time (sec): 146.31 - samples/sec: 1274.47 - lr: 0.000001\n",
      "2024-04-17 14:11:39,636 epoch 8 - iter 1664/2081 - loss 0.02901127 - time (sec): 167.12 - samples/sec: 1269.24 - lr: 0.000001\n",
      "2024-04-17 14:12:00,798 epoch 8 - iter 1872/2081 - loss 0.02899574 - time (sec): 188.28 - samples/sec: 1281.72 - lr: 0.000000\n",
      "2024-04-17 14:12:21,720 epoch 8 - iter 2080/2081 - loss 0.02948840 - time (sec): 209.20 - samples/sec: 1264.76 - lr: 0.000000\n",
      "2024-04-17 14:12:21,851 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 14:12:21,853 EPOCH 8 done: loss 0.0295 - lr 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:14<00:00, 32.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 14:12:36,776 Evaluating as a multi-label problem: False\n",
      "2024-04-17 14:12:36,843 DEV : loss 0.16008912026882172 - f1-score (micro avg)  0.885\n",
      "2024-04-17 14:12:39,027 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 14:12:39,029 Testing using last state of model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [00:09<00:00, 38.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 14:12:48,916 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 14:12:48,957 0.8891\t0.9036\t0.8963\t0.8606\n",
      "2024-04-17 14:12:48,958 \n",
      "Results:\n",
      "- F-score (micro) 0.8963\n",
      "- F-score (macro) 0.8886\n",
      "- Accuracy 0.8606\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ORG     0.8640    0.9164    0.8894      1400\n",
      "         LOC     0.8930    0.8625    0.8775      1084\n",
      "         PER     0.9666    0.9837    0.9751       735\n",
      "        MISC     0.8160    0.8088    0.8124       340\n",
      "\n",
      "   micro avg     0.8891    0.9036    0.8963      3559\n",
      "   macro avg     0.8849    0.8929    0.8886      3559\n",
      "weighted avg     0.8894    0.9036    0.8961      3559\n",
      "\n",
      "2024-04-17 14:12:48,958 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.8963210702341137,\n",
       " 'dev_score_history': [0.8251685393258427,\n",
       "  0.8822727272727271,\n",
       "  0.8860615699193457,\n",
       "  0.8966461327857631,\n",
       "  0.8978519195612431,\n",
       "  0.8915110551036775,\n",
       "  0.8934707903780069,\n",
       "  0.8850245967280632],\n",
       " 'train_loss_history': [0.49164654559153087,\n",
       "  0.09582007639815401,\n",
       "  0.08102016101342534,\n",
       "  0.0659440910564966,\n",
       "  0.04863045349449287,\n",
       "  0.03785637262144272,\n",
       "  0.030741337178146148,\n",
       "  0.029474600166816564],\n",
       " 'dev_loss_history': [0.11206331849098206,\n",
       "  0.1204131469130516,\n",
       "  0.12456652522087097,\n",
       "  0.1320403665304184,\n",
       "  0.13759379088878632,\n",
       "  0.14882870018482208,\n",
       "  0.16359102725982666,\n",
       "  0.16008912026882172]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fine_tune(\n",
    "    base_path=\"resources/taggers/ner-xlm-roberta-base\",\n",
    "    train_with_dev=False,\n",
    "    max_epochs=8,\n",
    "    learning_rate=2.0e-5,\n",
    "    mini_batch_size=4,\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 14:13:16,297 SequenceTagger predicts: Dictionary with 17 tags: O, S-ORG, B-ORG, E-ORG, I-ORG, S-LOC, B-LOC, E-LOC, I-LOC, S-PER, B-PER, E-PER, I-PER, S-MISC, B-MISC, E-MISC, I-MISC\n",
      "Sentence[5]: \"George Washington fue a Washington\" → [\"George Washington\"/PER, \"Washington\"/LOC]\n"
     ]
    }
   ],
   "source": [
    "# load the model you trained\n",
    "model = SequenceTagger.load(\"resources/taggers/ner-xlm-roberta-base/final-model.pt\")\n",
    "\n",
    "# create example sentence\n",
    "from flair.data import Sentence\n",
    "sentence = Sentence(\"George Washington fue a Washington\")\n",
    "\n",
    "# predict tags and print\n",
    "model.predict(sentence)\n",
    "\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
