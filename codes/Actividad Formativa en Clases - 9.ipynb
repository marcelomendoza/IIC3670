{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIC-3670 NLP UC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________________________\n",
    "\n",
    "## Actividad en clase\n",
    "\n",
    "Vamos a hacer fine-tuning de Albert.\n",
    "\n",
    "- Descargue desde hugging face hub datasets el corpus **wikitext-2-raw-v1**.\n",
    "- Haga checkpoint a **albert-base-v2**.\n",
    "- Haga fine-tuning usando **MLM** con **mlm_probability=0.1**.\n",
    "- Guarde localmente el nuevo modelo.\n",
    "- Use su modelo para encontrar los encodings de la oración \"Replace me by any text you'd like.\".\n",
    "- Cuando termine, me avisa para entregarle una **L (logrado)**.\n",
    "- Recuerde que las L otorgan un bono en la nota final de la asignatura.\n",
    "\n",
    "\n",
    "***Tiene hasta el final de la clase.***\n",
    "\n",
    "_________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "send_example_telemetry(\"language_modeling_notebook\", framework=\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/marce/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265dff44b5b94563b7517e3a2c7b3804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"albert-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        return self.tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_wrapper = TokenizerWrapper(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-23a667b61071c3c9_*_of_00004.arrow\n",
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-b85be2eaf41864d8_*_of_00004.arrow\n",
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-8ac38eaf266f3faf_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenizer_wrapper.tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    block_size = 128\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-e8e4924714701766_*_of_00004.arrow\n",
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-46d7292d7411ffd9_*_of_00004.arrow\n",
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-d1875b302eed7f0b_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-wikitext2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marce\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20929\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7851\n",
      "  Number of trainable parameters = 11221680\n",
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7851' max='7851' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7851/7851 09:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.065900</td>\n",
       "      <td>2.097129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.960600</td>\n",
       "      <td>1.999555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.882500</td>\n",
       "      <td>1.931062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-500\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-500\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-1000\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-1000\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-1500\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-1500\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-1500\\pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-2000\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-2000\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-2000\\pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-2500\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-2500\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2166\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-3000\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-3000\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-3000\\pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-3500\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-3500\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-3500\\pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-4000\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-4000\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-4000\\pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-4500\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-4500\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-4500\\pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-5000\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-5000\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-5000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2166\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-5500\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-5500\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-5500\\pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-6000\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-6000\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-6000\\pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-6500\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-6500\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-6500\\pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-7000\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-7000\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-7000\\pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-finetuned-wikitext2\\checkpoint-7500\n",
      "Configuration saved in albert-base-v2-finetuned-wikitext2\\checkpoint-7500\\config.json\n",
      "Model weights saved in albert-base-v2-finetuned-wikitext2\\checkpoint-7500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2166\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7851, training_loss=2.0259476300789707, metrics={'train_runtime': 559.2199, 'train_samples_per_second': 112.276, 'train_steps_per_second': 14.039, 'total_flos': 352775162769408.0, 'train_loss': 2.0259476300789707, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to albert-v2-finetuned.h5\n",
      "Configuration saved in albert-v2-finetuned.h5\\config.json\n",
      "Model weights saved in albert-v2-finetuned.h5\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"albert-v2-finetuned.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--albert-base-v2\\snapshots\\eab127d1a4a42b10e2b130e1834c64f5844d541e\\spiece.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--albert-base-v2\\snapshots\\eab127d1a4a42b10e2b130e1834c64f5844d541e\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading configuration file albert-v2-finetuned.h5\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file albert-v2-finetuned.h5\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at albert-v2-finetuned.h5 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertModel were not initialized from the model checkpoint at albert-v2-finetuned.h5 and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model = AlbertModel.from_pretrained(\"albert-v2-finetuned.h5\")\n",
    "\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.0104,  0.1411, -0.0380,  ..., -0.0693,  0.0191,  0.0779],\n",
       "         [-0.9227, -1.7339, -1.1261,  ...,  0.1697,  0.7790, -0.5530],\n",
       "         [-0.3506,  0.7906, -0.1291,  ...,  1.1095,  1.0939,  0.2346],\n",
       "         ...,\n",
       "         [ 0.6949, -1.4691, -0.2719,  ...,  1.1329,  1.5574,  0.3873],\n",
       "         [-0.0698,  0.0725, -0.4419,  ...,  0.8144,  0.7928, -0.1241],\n",
       "         [ 0.0104,  0.1411, -0.0380,  ..., -0.0693,  0.0191,  0.0779]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-1.8701e-01, -5.9273e-01,  3.1621e-01, -3.0593e-01,  1.3583e-01,\n",
       "          6.4248e-02,  1.5055e-01, -4.1616e-02, -1.4342e-02,  2.3935e-01,\n",
       "          1.6858e-01,  3.4054e-01, -7.1437e-02, -1.0849e-01,  5.7066e-02,\n",
       "          8.4060e-02,  1.7361e-01, -1.1483e-01, -3.4953e-01,  5.2143e-02,\n",
       "          3.2306e-01, -1.2452e-01, -3.8437e-01, -4.3061e-01, -2.2502e-01,\n",
       "         -1.9869e-01,  1.7905e-01, -1.2352e-01,  6.7367e-02,  2.2645e-01,\n",
       "          2.4184e-01, -6.7862e-02, -3.3669e-01, -9.5551e-02,  1.3141e-01,\n",
       "         -2.5997e-01,  3.5679e-01,  1.1899e-01,  1.2607e-01, -2.6434e-01,\n",
       "         -1.9237e-01, -4.7794e-01, -1.7474e-01,  5.5360e-01, -1.1084e-01,\n",
       "          2.7569e-01,  2.3124e-01, -3.2692e-01, -5.3300e-01,  4.8988e-01,\n",
       "          3.5452e-01,  2.9213e-01,  1.3609e-02,  1.3443e-01, -2.6939e-01,\n",
       "         -1.0758e-01,  2.2332e-01,  1.2539e-01, -5.8280e-01, -5.6877e-01,\n",
       "          5.2222e-01,  2.0818e-01,  8.6813e-04,  2.1319e-01,  3.2085e-01,\n",
       "          2.8682e-01, -5.6411e-01,  1.4269e-01, -2.3652e-01,  3.3508e-01,\n",
       "         -1.5405e-01, -2.6400e-01,  4.0157e-01,  4.2457e-01, -2.7406e-01,\n",
       "         -3.9615e-01, -4.3776e-01, -3.4212e-01,  2.9757e-01,  3.3818e-01,\n",
       "          3.0837e-02, -4.4088e-01, -5.1341e-02,  3.3545e-01, -2.3013e-01,\n",
       "          2.0836e-01,  2.5634e-01, -5.6553e-01, -1.1081e-01,  1.6877e-01,\n",
       "          9.7288e-02,  2.4029e-01,  3.2508e-01, -8.8297e-02, -3.0858e-01,\n",
       "          3.3737e-01,  5.8197e-02, -7.5379e-03, -4.4736e-01,  2.1194e-01,\n",
       "         -3.4884e-01, -3.0845e-01,  9.8188e-02, -4.9270e-01,  2.4406e-01,\n",
       "         -1.8383e-02,  2.8152e-02,  6.2422e-04,  3.2615e-01, -2.7168e-01,\n",
       "          2.0587e-01, -4.6604e-01, -2.1694e-01,  1.3949e-02, -2.8104e-01,\n",
       "         -3.1962e-02,  1.5394e-01, -6.7449e-02,  3.7688e-01, -8.3446e-02,\n",
       "         -3.7440e-02, -8.2633e-02,  4.1315e-01, -6.0943e-01,  3.3717e-01,\n",
       "          3.2706e-01,  8.3456e-02, -2.4405e-01,  5.7708e-01,  1.5296e-01,\n",
       "         -4.4659e-01, -5.2216e-02,  2.0633e-01, -8.6597e-03, -1.0888e-01,\n",
       "         -3.9191e-02, -1.9310e-03,  5.1827e-01,  2.7406e-01, -1.9159e-01,\n",
       "         -2.4151e-01,  3.8499e-01, -5.9487e-01, -6.1879e-02,  4.0836e-02,\n",
       "         -1.7571e-01,  1.2001e-01,  2.6766e-01, -2.2062e-01, -3.4023e-01,\n",
       "         -1.4650e-01, -2.7997e-01,  1.1670e-01,  4.2962e-01, -5.4968e-01,\n",
       "         -2.7027e-01,  2.6559e-01, -3.0640e-01, -1.8717e-01,  1.4547e-01,\n",
       "         -3.0387e-01, -4.0078e-01, -1.7326e-01,  6.8203e-02, -3.5647e-01,\n",
       "          3.8434e-01,  3.7566e-01, -1.1681e-01,  1.5466e-01, -3.1481e-01,\n",
       "          1.8684e-01, -1.1510e-01,  8.8936e-02,  6.4758e-02, -8.4968e-02,\n",
       "         -3.7464e-01, -9.8391e-02, -9.0132e-02,  3.6073e-02, -3.5852e-01,\n",
       "         -3.7520e-01, -5.2766e-01,  1.9893e-01,  1.8647e-01, -4.9193e-01,\n",
       "         -3.6234e-01, -2.2670e-01,  6.1939e-02,  1.5855e-01,  3.5253e-02,\n",
       "          2.8529e-01,  2.3941e-01,  6.9081e-02,  1.3745e-01,  1.5578e-01,\n",
       "          1.0692e-01,  5.8024e-02, -1.0183e-01,  3.4810e-01, -4.6662e-01,\n",
       "          1.7343e-01,  2.3624e-01,  1.5180e-01,  5.1505e-01,  5.1507e-01,\n",
       "         -4.5186e-01, -2.2567e-01, -1.5218e-01, -8.5072e-03,  8.9597e-02,\n",
       "          2.6343e-01,  5.3967e-03, -5.4359e-01, -8.8895e-03, -8.1245e-02,\n",
       "          3.8260e-01,  7.2585e-02, -4.1682e-01, -5.5492e-01,  8.3909e-03,\n",
       "          2.7944e-01, -2.3186e-01, -2.4049e-01,  1.0182e-01,  3.5153e-01,\n",
       "         -1.4480e-01,  2.0954e-01, -5.9187e-01, -6.1422e-02,  3.5424e-01,\n",
       "         -7.5131e-02, -6.8783e-02,  6.2146e-01, -2.2102e-02,  2.7221e-01,\n",
       "         -2.3410e-01,  3.6544e-01, -1.1093e-01,  8.3164e-02,  3.3208e-01,\n",
       "         -3.0778e-01, -1.9305e-01, -1.3297e-02,  4.1012e-02,  1.5533e-01,\n",
       "         -5.1527e-02,  2.9924e-01, -3.0950e-01, -3.0569e-01, -2.6536e-01,\n",
       "         -7.6849e-02, -2.8474e-01,  7.5053e-02,  2.8340e-01, -5.8507e-01,\n",
       "          3.4496e-02, -1.4219e-01,  1.8836e-01,  1.0751e-01, -4.7130e-01,\n",
       "         -1.5733e-01,  4.4255e-01,  5.8406e-01,  1.0498e-01,  9.7873e-02,\n",
       "         -1.1831e-01,  1.9773e-01,  4.7111e-01, -1.0595e-01, -2.3821e-01,\n",
       "         -5.8791e-01, -6.3728e-02,  8.6185e-02,  1.7803e-02,  5.0385e-02,\n",
       "          2.1149e-01,  4.6799e-02, -2.2607e-01,  5.0030e-01,  3.0685e-02,\n",
       "          3.3445e-01,  2.6719e-01, -5.3226e-02, -3.5309e-01,  4.0976e-01,\n",
       "         -1.1818e-01,  2.6032e-01,  2.0602e-01, -3.8377e-01, -6.1118e-01,\n",
       "         -1.8662e-01,  4.5016e-01,  3.4266e-01, -1.7891e-01, -1.3478e-01,\n",
       "          4.2746e-01,  4.0371e-01,  3.5858e-02,  7.5222e-04,  1.0183e-01,\n",
       "          6.5392e-01,  2.2957e-01, -2.2965e-01,  1.7820e-01, -8.9146e-02,\n",
       "          4.5676e-02, -1.0662e-02, -3.8887e-01,  2.1544e-01, -5.1671e-01,\n",
       "         -2.5959e-01, -2.2810e-01,  6.9477e-02, -1.6933e-01, -1.6668e-02,\n",
       "          2.8096e-01, -4.7476e-02,  8.4939e-02,  6.2633e-01,  2.1732e-02,\n",
       "          2.7814e-01, -2.8540e-01,  2.1035e-02, -3.7444e-01,  4.7509e-02,\n",
       "          2.2337e-01, -4.2880e-01, -1.0190e-01, -1.0175e-01,  1.0260e-01,\n",
       "         -2.8236e-01,  3.1745e-01,  2.5935e-01,  3.3928e-01,  1.2382e-01,\n",
       "         -1.5189e-01,  7.7225e-02, -3.8697e-01,  3.1196e-01, -9.2330e-02,\n",
       "          2.2509e-03,  6.9773e-01, -3.9338e-01,  1.2797e-01,  4.0267e-01,\n",
       "         -3.0690e-02, -5.4937e-02, -1.4948e-01, -2.5739e-01, -2.3174e-02,\n",
       "          4.4048e-01, -2.0240e-01,  2.1913e-01, -2.3909e-01, -2.8189e-01,\n",
       "          5.8830e-02, -1.9126e-01,  9.3215e-02, -2.0758e-01,  2.6737e-01,\n",
       "          3.3191e-01, -1.9377e-01, -2.7158e-01, -2.0248e-01,  9.2179e-02,\n",
       "         -4.1409e-01, -2.1875e-01,  4.1875e-01, -1.4688e-01, -1.1766e-01,\n",
       "         -5.2852e-01, -6.1910e-03,  1.4899e-01, -4.3478e-01, -2.1316e-01,\n",
       "         -3.7116e-01, -1.5107e-01, -8.0989e-02, -3.6689e-01,  1.6756e-01,\n",
       "         -3.9778e-01,  3.0659e-01,  5.4797e-01, -3.4731e-01,  2.1505e-01,\n",
       "         -1.8760e-02, -3.9805e-01,  3.6262e-01,  4.4278e-01, -3.2294e-01,\n",
       "         -3.2869e-01,  5.7306e-01,  1.5937e-01,  2.1026e-01,  5.0799e-02,\n",
       "         -2.6244e-01,  3.1052e-01,  5.6095e-02,  2.8394e-01,  2.9895e-01,\n",
       "          1.1427e-01, -3.9788e-02, -2.7140e-01, -3.4327e-02,  3.8207e-01,\n",
       "         -1.6671e-01, -1.0119e-01, -4.3817e-01,  2.8019e-01,  2.2402e-01,\n",
       "         -2.7526e-01, -2.6316e-02,  3.8182e-01, -5.2409e-01,  1.5496e-01,\n",
       "         -1.1212e-01, -1.4804e-01, -6.0671e-02, -3.3514e-01, -2.6355e-01,\n",
       "          2.9806e-01,  1.5748e-01, -5.0965e-02, -8.4494e-02,  2.8215e-02,\n",
       "         -8.8966e-02,  1.1848e-01,  1.6750e-01, -8.6804e-02, -4.0277e-01,\n",
       "          2.8762e-01,  2.2350e-03, -5.0032e-01, -3.0845e-01, -7.6004e-02,\n",
       "          1.4270e-01, -1.3372e-01,  2.8967e-01,  2.4021e-01, -5.8416e-02,\n",
       "         -1.8520e-02, -1.5717e-01, -1.7105e-01,  3.2022e-01, -2.0022e-01,\n",
       "         -9.8004e-02,  1.4444e-01, -1.6095e-01, -1.5418e-01,  3.0607e-01,\n",
       "          2.5730e-01,  2.1348e-01, -1.4912e-01,  1.3875e-01,  9.9055e-02,\n",
       "         -7.2340e-02,  4.3151e-01, -1.5457e-01, -1.7485e-02, -1.4191e-01,\n",
       "          2.1105e-01, -4.6737e-01, -9.9871e-02, -4.1189e-02,  3.5034e-02,\n",
       "         -8.0149e-02,  3.0715e-01, -2.2199e-01,  1.5611e-01, -4.1074e-01,\n",
       "          4.6548e-01,  3.3225e-01,  2.5686e-01,  1.9824e-01,  1.4431e-01,\n",
       "         -1.1491e-02,  1.7203e-01, -7.6496e-02,  5.6410e-01, -1.9019e-01,\n",
       "         -1.9259e-02, -2.3556e-01, -5.0152e-02, -2.8486e-01,  1.8734e-01,\n",
       "         -1.8541e-01, -2.9638e-01, -4.7798e-01,  6.8409e-01, -2.7322e-01,\n",
       "         -4.1394e-01, -5.5021e-01, -3.2644e-01, -1.3549e-01, -8.8545e-02,\n",
       "         -2.7193e-01, -2.4402e-01, -3.6990e-01,  1.5298e-01, -1.3073e-01,\n",
       "          2.4389e-01, -1.3897e-02, -2.2233e-01,  3.7146e-01,  3.1028e-01,\n",
       "         -1.2372e-01,  1.0291e-01, -4.3960e-01, -4.1806e-01, -1.3689e-01,\n",
       "         -2.7460e-01, -2.8603e-01,  4.4794e-01,  1.5727e-01,  4.2712e-01,\n",
       "          1.7431e-01,  1.7927e-01, -8.2680e-03,  3.1859e-01, -2.1149e-01,\n",
       "         -1.5583e-01,  2.7326e-01, -3.5181e-01, -2.3943e-01,  3.2393e-01,\n",
       "          8.2399e-02, -2.3465e-01, -4.8037e-01, -6.4011e-02, -5.1541e-01,\n",
       "          7.1785e-02,  3.1096e-04,  5.6048e-01,  2.0151e-01, -3.0904e-01,\n",
       "          3.6324e-02,  2.5078e-01,  2.8101e-02,  1.3060e-01,  6.7059e-01,\n",
       "         -2.2172e-02,  8.8192e-03,  1.7600e-01,  3.6635e-01,  2.2008e-02,\n",
       "         -3.7699e-01,  2.7329e-01,  5.9459e-03,  5.7286e-01,  1.0875e-01,\n",
       "          6.3190e-01, -1.8054e-01,  4.4336e-01,  2.8248e-01, -2.1583e-01,\n",
       "          2.5250e-01,  3.8908e-02, -1.8703e-01,  2.3837e-01, -5.3770e-01,\n",
       "          1.8302e-01, -4.2637e-01, -2.1904e-01, -1.9777e-01, -8.4172e-02,\n",
       "         -1.1197e-01, -1.8948e-01, -4.1448e-02,  2.7457e-01, -3.8057e-01,\n",
       "         -2.9440e-01,  2.4510e-01, -5.3144e-02, -1.4499e-01,  2.2001e-01,\n",
       "         -1.9192e-01, -3.5992e-01,  1.3890e-01, -5.5520e-01, -2.1856e-01,\n",
       "         -2.2339e-01, -5.9104e-02,  3.1491e-01,  3.3520e-01, -2.9674e-01,\n",
       "         -5.9886e-01, -5.2012e-01, -3.5991e-03, -3.0239e-02,  2.0460e-01,\n",
       "          3.7634e-01,  1.7916e-01, -2.3299e-01, -4.2500e-01,  1.3357e-01,\n",
       "         -2.8680e-01, -2.9122e-01, -7.2955e-02, -4.7643e-02, -3.4902e-01,\n",
       "         -2.2721e-01,  6.8022e-02,  5.0020e-01,  1.0359e-01,  2.0983e-01,\n",
       "         -8.7610e-02,  1.8705e-01, -3.2900e-01, -2.6516e-01, -3.6423e-01,\n",
       "          2.0468e-01, -2.2433e-01,  2.4856e-01,  1.3385e-01, -2.9915e-01,\n",
       "         -1.4760e-01, -2.0145e-01, -4.7094e-01,  2.3569e-01,  6.9706e-01,\n",
       "         -1.9821e-01, -4.1349e-01,  3.6343e-01,  5.4908e-01,  2.5424e-01,\n",
       "          1.0708e-01, -4.3303e-01, -3.0577e-02, -3.2024e-01, -8.7042e-02,\n",
       "         -1.6926e-01,  1.3123e-01,  2.5302e-01,  1.6277e-01,  2.1846e-01,\n",
       "          4.2220e-01,  3.0732e-01,  6.0344e-02,  2.3801e-01, -4.0331e-01,\n",
       "          2.4343e-01, -2.9641e-01, -6.9279e-02,  3.9136e-01,  2.0039e-01,\n",
       "         -1.0311e-01,  1.9437e-01, -3.4162e-01, -5.3984e-02,  5.3236e-01,\n",
       "         -2.4686e-01,  4.5182e-03, -3.4921e-01, -3.9941e-01,  7.1093e-02,\n",
       "         -3.8350e-01,  5.2257e-02,  1.6162e-01, -2.9794e-01,  5.5380e-02,\n",
       "         -8.8353e-02, -1.7262e-01,  2.5850e-01, -4.4240e-02, -6.6917e-02,\n",
       "         -1.2410e-01, -2.9324e-01,  4.5591e-01,  1.6377e-01,  3.0186e-01,\n",
       "         -3.0425e-01, -7.5741e-02,  1.0307e-01,  2.6959e-01, -3.8629e-02,\n",
       "         -1.1588e-01, -3.3666e-01, -4.8973e-02, -4.3183e-01, -5.5321e-01,\n",
       "          3.2888e-01,  3.6143e-01, -3.0004e-01, -5.0228e-02, -2.0277e-01,\n",
       "         -3.8085e-01,  3.7611e-01,  3.2741e-01,  2.8351e-01, -3.3423e-01,\n",
       "          7.9363e-02, -1.3873e-02, -2.6478e-01, -1.0178e-01, -5.1207e-01,\n",
       "         -2.7845e-01, -5.9157e-02, -2.2643e-01, -2.9314e-01,  5.1599e-01,\n",
       "          1.8525e-01, -4.0712e-01, -3.7121e-01,  3.3926e-01,  1.0263e-01,\n",
       "         -1.2063e-01, -3.5561e-01, -2.3877e-01, -8.1417e-02, -1.4599e-01,\n",
       "         -2.8452e-01,  2.7850e-01,  2.6113e-01,  5.3178e-01, -8.1420e-02,\n",
       "         -3.7721e-01,  5.0074e-01, -8.2754e-02,  3.7350e-01, -2.3157e-01,\n",
       "         -1.8807e-01, -6.2378e-01, -5.3360e-02, -2.0368e-01, -1.3270e-01,\n",
       "          1.6748e-01, -3.3517e-01,  1.8224e-01,  7.8903e-02,  6.1191e-02,\n",
       "         -3.2406e-01,  2.8314e-01,  1.2534e-01, -7.6323e-02,  1.3421e-01,\n",
       "          1.7377e-02, -1.5067e-01, -3.7812e-01,  1.4642e-01, -2.5508e-01,\n",
       "          3.1051e-01,  1.8122e-01,  1.6089e-01,  1.9910e-01, -4.3359e-01,\n",
       "         -2.1804e-01, -1.9029e-01,  1.1683e-01,  9.8392e-02, -2.0403e-01,\n",
       "         -2.6612e-01,  9.6627e-03, -5.0282e-01,  1.4923e-03, -2.0988e-01,\n",
       "         -4.4922e-01,  2.5130e-01,  3.9170e-01, -4.7579e-01, -5.3472e-01,\n",
       "          2.9607e-01, -1.5806e-01,  3.6300e-01, -4.0218e-02,  8.5480e-02,\n",
       "          2.2781e-01,  2.1034e-01,  1.5042e-01]], grad_fn=<TanhBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
