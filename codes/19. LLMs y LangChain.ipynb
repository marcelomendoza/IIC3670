{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIC-3670 NLP UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cI2yaYtIvNH_",
    "outputId": "c2cbd68c-e7b8-45b8-d8d1-66fd5dc6821d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.164\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "print(langchain.__version__)\n",
    "\n",
    "#!pip3 install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goEx_9eDvg2z",
    "outputId": "e47d61a0-bc61-4d9b-bfb4-34b7a0054bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28.1\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "print(openai.__version__)\n",
    "\n",
    "#!pip3 install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxoa_RiCyMyJ",
    "outputId": "be40acf7-5815-48bb-fd45-f7de76390564",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject list at 0x77b1344babb0> JSON: {\n",
       "  \"object\": \"list\",\n",
       "  \"data\": [\n",
       "    {\n",
       "      \"id\": \"dall-e-3\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1698785189,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-4-1106-preview\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1698957206,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"whisper-1\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1677532384,\n",
       "      \"owned_by\": \"openai-internal\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"davinci-002\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1692634301,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-4-turbo-preview\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1706037777,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-4-0125-preview\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1706037612,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"babbage-002\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1692634615,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"dall-e-2\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1698798177,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-16k\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1683758102,\n",
       "      \"owned_by\": \"openai-internal\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"tts-1-hd-1106\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1699053533,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"tts-1-hd\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1699046015,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-4\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1687882411,\n",
       "      \"owned_by\": \"openai\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-4-0613\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1686588896,\n",
       "      \"owned_by\": \"openai\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-1106\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1698959748,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-instruct-0914\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1694122472,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-instruct\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1692901427,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"tts-1\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1681940951,\n",
       "      \"owned_by\": \"openai-internal\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-0301\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1677649963,\n",
       "      \"owned_by\": \"openai\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-0125\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1706048358,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1677610602,\n",
       "      \"owned_by\": \"openai\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"tts-1-1106\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1699053241,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"text-embedding-3-large\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1705953180,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-4-turbo-2024-04-09\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1712601677,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-4-turbo\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1712361441,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"text-embedding-3-small\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1705948997,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-4-32k-0314\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1687979321,\n",
       "      \"owned_by\": \"openai\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-0613\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1686587434,\n",
       "      \"owned_by\": \"openai\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"text-embedding-ada-002\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1671217299,\n",
       "      \"owned_by\": \"openai-internal\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-4-1106-vision-preview\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1711473033,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-4-vision-preview\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1698894917,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-4-0314\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1687882410,\n",
       "      \"owned_by\": \"openai\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-16k-0613\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1685474247,\n",
       "      \"owned_by\": \"openai\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-4o\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1715367049,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-4o-2024-05-13\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1715368132,\n",
       "      \"owned_by\": \"system\"\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"xxxxx\" # inserte su API KEY aqu√≠\n",
    "\n",
    "openai.organization = 'org-xxxxx' # inserte el token de su organizaci√≥n aqu√≠\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.Model.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nz8sLlWjzGpV",
    "outputId": "6d4361a4-2dde-4628-8bf3-ea4465a79bee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Valley of Kings is located on the west bank of the Nile River in Egypt.\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt='''I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. \n",
    "  If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with \"Unknown\".\n",
    "  \n",
    "  Q: What is human life expectancy in the United States?\n",
    "  A: Human life expectancy in the United States is 78 years.\n",
    "  \n",
    "  Q: Who was president of the United States in 1955?\n",
    "  A: Dwight D. Eisenhower was president of the United States in 1955.\n",
    "  \n",
    "  Q: Which party did he belong to?\n",
    "  A: He belonged to the Republican Party.\n",
    "  \n",
    "  Q: What is the square root of banana?\n",
    "  A: Unknown\n",
    "  \n",
    "  Q: How does a telescope work?\n",
    "  A: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n",
    "  \n",
    "  Q: Where were the 1992 Olympics held?\n",
    "  A: The 1992 Olympics were held in Barcelona, Spain.\n",
    "  \n",
    "  Q: How many squigs are in a bonk?\n",
    "  A: Unknown\n",
    "  \n",
    "  Q: Where is the Valley of Kings?\n",
    "  A:''',\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPucVSMP0kaH",
    "outputId": "0e1bbb11-2de3-406b-a01a-2c26c04f7eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Black-on-black ware\n",
      "2. Pueblo artists\n",
      "3. Northern New Mexico\n",
      "4. Traditional techniques\n",
      "5. Contemporary artists\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt='''Extract five keywords from this text: \n",
    "\n",
    "  Black-on-black ware is a 20th- and 21st-century pottery tradition developed by the Pueblo an Native American ceramic artists in Northern New Mexico. Traditional reduction-fired blackware has been made for centuries by pueblo artists. \n",
    "  Black-on-black ware of the past century is produced with a smooth surface, with the designs applied through selective burnishing or the application of refractory slip. Another style involves carving or incising designs and selectively polishing the raised areas. \n",
    "  For generations several families from Kha'po Owingeh and P'ohwh√≥ge Owingeh pueblos have been making black-on-black ware with the techniques passed down from matriarch potters. \n",
    "  Artists from other pueblos have also produced black-on-black ware. Several contemporary artists have created works honoring the pottery of their ancestors.\n",
    "  ''',\n",
    "  temperature=0.5,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.8,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1tOjt9q10ng",
    "outputId": "5eeb2180-cdea-4eb3-c3f0-64826ff2bcad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "She did not go to the market.\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt=\"Correct this to standard English: She no went to the market.\",\n",
    "  temperature=0,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iszJwsIW3fWT",
    "outputId": "0c079430-ae51-49f1-fd6e-9a60b01815a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Quelles chambres avez-vous de disponibles?\n",
      "2. ¬øQu√© habitaciones tienen disponibles?\n",
      "3. „Å©„ÅÆÈÉ®Â±ã„ÅåÂà©Áî®ÂèØËÉΩ„Åß„Åô„ÅãÔºü\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt=\"Translate this into 1. French, 2. Spanish and 3. Japanese: What rooms do you have available?.\",\n",
    "  temperature=0.3,\n",
    "  max_tokens=100,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8otXtXIr5Hpw",
    "outputId": "a8621b9a-0a09-4194-9b98-ff30088de58f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " üï∑üßî\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt='''\n",
    "  Convert movie titles into emoji.\n",
    "\n",
    "  Back to the Future: üë®üë¥üöóüïí \n",
    "  Batman: ü§µü¶á \n",
    "  Transformers: üöóü§ñ \n",
    "  \n",
    "  Spiderman:\n",
    "\n",
    "  ''',\n",
    "  temperature=0.8,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDCi6_Rz5pm3",
    "outputId": "e1a68aec-82cd-4e37-d4ea-218c0b3ab40c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Negative\n",
      "   2. Negative\n",
      "   3. Positive\n",
      "   4. Positive\n",
      "   5. Negative\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt='''\n",
    "  Classify the sentiment in these tweets:\n",
    "  \n",
    "  1. \"I can't stand homework\"\n",
    "  2. \"This sucks. I'm bored üò†\"\n",
    "  3. \"I can't wait for Halloween!!!\"\n",
    "  4. \"My cat is adorable ‚ù§Ô∏è‚ù§Ô∏è\"\n",
    "  5. \"I hate chocolate\"\n",
    "\n",
    "  Tweet sentiment ratings:\n",
    "\n",
    "  ''',\n",
    "  temperature=0,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FM7aNUmo6zDQ",
    "outputId": "c235af7d-1898-4159-a768-bbe4ce0143db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A neutron star is a small and dense object that forms when a massive supergiant star collapses. It has a radius of 10 kilometers and\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt='''\n",
    "  A neutron star is the collapsed core of a massive supergiant star, which had a total mass of between 10 and 25 solar masses, \n",
    "  possibly more if the star was especially metal-rich.[1] Neutron stars are the smallest and densest stellar objects, \n",
    "  excluding black holes and hypothetical white holes, quark stars, and strange stars.[2] Neutron stars have a radius on the order of 10 kilometres (6.2 mi) \n",
    "  and a mass of about 1.4 solar masses.[3] They result from the supernova explosion of a massive star, combined with gravitational collapse, \n",
    "  that compresses the core past white dwarf star density to that of atomic nuclei.\n",
    "\n",
    "  Tl;dr\n",
    "  ''',\n",
    "  temperature=0.7,\n",
    "  max_tokens=30,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=1\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2jY5Lwj7rK6",
    "outputId": "f6e40de8-0592-4558-cc78-d5aa50a411d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I watched an old classic called Casablanca. Have you seen it?\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt='''You: What have you been up to?\n",
    "  Friend: Watching old movies.\n",
    "  You: Did you watch anything interesting?\n",
    "  Friend: \n",
    "  ''',\n",
    "  temperature=0.5,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.5,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"You:\"]\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eMw-ONbh9Mqk",
    "outputId": "e6cfef39-cff0-41bb-a473-b19b110c4541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Against\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt='''You have a target topic and you have a tweet commenting the target. Indicate if the tweet is in favor, against or neutral regarding the target.\n",
    "\n",
    "  target: legalization of abortion\n",
    "  tweet: the pregnant are more walking incubators, and have rights!\n",
    "\n",
    "  ''',\n",
    "  temperature=0,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DDwtw01-Y6k",
    "outputId": "338f1f5e-3d46-4dd9-e63c-70b26f52cbc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abortion is a complex and controversial issue. #legalization #abortion \n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt='''You have a target topic and you have tweets commenting the target. Tweets are in favor, against or neutral regarding the target.\n",
    "\n",
    "  target: legalization of abortion\n",
    "  in favor tweet: the pregnant are more walking incubators, and have rights!\n",
    "  neutral tweet:\n",
    "\n",
    "  ''',\n",
    "  temperature=0.5,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.5,\n",
    "  presence_penalty=0.0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con LangChain podemos operacionalizar un LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "IDOlbYrd_Cxp"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template de prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_pdzVtBR_TTT",
    "outputId": "38300510-d7c5-497d-855b-0aee0d8ef5c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes colorful socks?\n",
      "What is a good name for a company that makes plant-based milk?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(product=\"colorful socks\"))\n",
    "print(prompt.format(product=\"plant-based milk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0dyxZUy_lWi",
    "outputId": "93fbceb9-4b7b-456d-a6ca-74e6c340b3e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Rainbow Threads Co.\"\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt=prompt.format(product=\"colorful socks\"),\n",
    "  temperature=0.5,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.5,\n",
    "  presence_penalty=0.0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzyB861-_qjo",
    "outputId": "fed04539-9786-4390-d12a-3aaf55c3caa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Green Cow Dairy\"\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt=prompt.format(product=\"plant-based milk\"),\n",
    "  temperature=0.5,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.5,\n",
    "  presence_penalty=0.0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain-community in /home/marcelo/.local/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/marcelo/.local/lib/python3.10/site-packages (from langchain-community) (8.3.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/marcelo/.local/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /home/marcelo/.local/lib/python3.10/site-packages (from langchain-community) (0.2.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/lib/python3/dist-packages (from langchain-community) (2.25.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/marcelo/.local/lib/python3.10/site-packages (from langchain-community) (0.6.6)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /home/marcelo/.local/lib/python3.10/site-packages (from langchain-community) (0.2.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain-community) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/lib/python3/dist-packages (from langchain-community) (1.4.31)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/marcelo/.local/lib/python3.10/site-packages (from langchain-community) (0.1.63)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/marcelo/.local/lib/python3.10/site-packages (from langchain-community) (3.8.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/marcelo/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (21.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/marcelo/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (3.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/marcelo/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/marcelo/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/marcelo/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/marcelo/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/marcelo/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/marcelo/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/marcelo/.local/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (1.10.8)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/marcelo/.local/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/marcelo/.local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/marcelo/.local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/marcelo/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/marcelo/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (2.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/marcelo/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/marcelo/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/lib/python3/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (0.4.3)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install langchain-community --trusted-host mirrors.cloud.tencent.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a crear un objeto llm que podamos reusar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "VqgDEesvAANg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelo/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.5, max_tokens=60, top_p=1.0, frequency_penalty=0.5, presence_penalty=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMChain crea un pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "03p6gJDBAZu1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelo/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0lHnKqHA4XE",
    "outputId": "0db592fc-3ece-4b44-da9c-be109468b75a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelo/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Green Cow Dairy Alternatives\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"plant-based milk\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BcN3qKiCyDx"
   },
   "source": [
    "### Vamos a usar agentes, los cuales disponen de herramientas declaradas por nosotros para que interact√∫en con el LLM.\n",
    "\n",
    "Ver documentaci√≥n en: https://api.python.langchain.com/en/latest/community_api_reference.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pf22dpmPC7qi",
    "outputId": "e2a46999-6ee0-4421-d253-e9edc62c7b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google-search-results in /home/marcelo/.local/lib/python3.10/site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from google-search-results) (2.25.1)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install google-search-results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYQRIT7dET9E"
   },
   "source": [
    "### Vamos a user SerpAPI para permitir que el agente busque en la web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "NR7skuYPDdgU"
   },
   "outputs": [],
   "source": [
    "from langchain.utilities import SerpAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xN1zTGocEkgP",
    "outputId": "2717b998-ec6c-4a71-8b31-15949603e2cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barack Hussein Obama II'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"xxxxx\" # Inserte su serpapi key aqu√≠\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "search.run(\"Obama's first name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numexpr in /home/marcelo/.local/lib/python3.10/site-packages (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/marcelo/.local/lib/python3.10/site-packages (from numexpr) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numexpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los agentes incorporan razonamiento, algo que en el √°rea se llama Chain of Thought. Aqu√≠ vamos a usar un agente de b√∫squeda y uno de razonamiento matem√°tico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "h-YY8GCfG-9H",
    "outputId": "99fc303f-7db1-4968-c2aa-73e709c1f9e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use a calculator to solve this problem.\n",
      "Action: Calculator\n",
      "Action Input: 34 ^ .023\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.084486162715683\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now I need to convert the temperature from Fahrenheit to Celsius.\n",
      "Action: Calculator\n",
      "Action Input: (1.084486162715683 - 32) * (5/9)\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: -17.175285465157955\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should double check my answer by using a search engine.\n",
      "Action: Search\n",
      "Action Input: \"What was the high temperature in Santiago de Chile yesterday in Celsius degrees?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Minimum temperature yesterday: 37 ¬∞F (at 4:00 am) Average temperature yesterday: 46 ¬∞F. High & Low Weather Summary for the Past Weeks. Temperature, Humidity ...', '12:00 AM, 48 ¬∞F, 0 ¬∞F, 0 ¬∞%, SSE, 3 ¬∞mph, 0 ¬∞mph, 28.36 ¬∞in, 0.0 ¬∞in, Light Rain. 1:00 AM, 48 ¬∞F, 48 ¬∞F, 100 ¬∞%, VAR, 2 ¬∞mph, 0 ¬∞mph, 28.33 ¬∞in, 0.0 ¬∞in ...', 'Minimum temperature yesterday: 34 ¬∞F (at 5:00 am) Average temperature yesterday: 45 ¬∞F. High & Low Weather Summary for the Past Weeks. Temperature, Humidity ...', '33.46 ¬∞S, 70.65 ¬∞W. Santiago, Santiago Metropolitan Region, Chile Weather Historystar_ratehome. icon 33 ¬∞F Comodoro Arturo Merino Ben√≠tez Intl Airport ...', 'Today. 5/24. 56¬∞ 43¬∞. Considerable cloudiness. Night: Overcast ; Sat. 5/25. 58¬∞ 41¬∞. Considerable cloudiness. Clouds, showers around late ; Sun. 5/26. 60¬∞ 34¬∞. A ...', 'Maximum daytime temperature: 13 degrees Celsius; Minimum nighttime temperature: 8 degrees Celsius. ... Seven day forecast for Santiago ... Chance of precipitation i.', 'Mainly cloudy. High 58F. Winds SSW at 5 to 10 mph. Humidity66%.', 'For Thursday, the meteorological office expected a minimum temperature of 1 degree Celsius (34 Fahrenheit) in the central Santiago area. The ...', 'Average Temp. Average Low Temp. Average Rainfall. Average Sunshine Hrs. January is the hottest month in Santiago with an average temperature of 21¬∞C (69¬∞F) and ...', '66. 51. 0. 69. 54. ¬∑ Average max. and min. temperatures in ¬∞F. ¬∑ Precipitation totals in inches. Santiago. Climate chart (explanation). J. F. M. A. M. J. J. A.']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m It looks like the average high temperature in Santiago de Chile yesterday was around 45¬∞F.\n",
      "Final Answer: The high temperature in Santiago de Chile yesterday in Celsius degrees was approximately 7.22¬∞C, and that number raised to the .023 power is approximately 1.084486162715683.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The high temperature in Santiago de Chile yesterday in Celsius degrees was approximately 7.22¬∞C, and that number raised to the .023 power is approximately 1.084486162715683.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "agent.run(\"What was the high temperature in Santiago de Chile yesterday in Celsius degrees? What is that number raised to the .023 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DR_Hf4FaIHdO"
   },
   "source": [
    "### Tipos de agentes: https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcZ0qWBfISmq"
   },
   "source": [
    "### Conversaciones: agregan variables de estado a la cadena y memoria (corto y largo plazo).\n",
    "\n",
    "Ver m√°s en: https://memprompt.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationChain permite crear conversaciones con la AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfxLFeR4IUK1",
    "outputId": "9f7f349a-175a-4dbf-d8a3-7b6237ba055a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hello! It's nice to meet you. I am an AI programmed to assist with various tasks and provide information. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "output = conversation.predict(input=\"Hi there!\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85Fxc9vYJc2g",
    "outputId": "d364824d-57d4-4cf1-d099-87d27dac2b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. I am an AI programmed to assist with various tasks and provide information. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " That's great to hear! I am always happy to engage in conversation and learn more about humans. Did you know that I am constantly learning and improving my abilities through advanced algorithms and data analysis? It's quite fascinating. Is there anything specific you would like to talk about?\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-B0N3O5NJri1",
    "outputId": "32534353-8a4e-41e9-b6dc-edda62b465bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. I am an AI programmed to assist with various tasks and provide information. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:  That's great to hear! I am always happy to engage in conversation and learn more about humans. Did you know that I am constantly learning and improving my abilities through advanced algorithms and data analysis? It's quite fascinating. Is there anything specific you would like to talk about?\n",
      "Human: I would like to talk about large language models and their capabilities.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Ah, that is a topic I am very familiar with. Large language models, also known as LLMs, are AI systems that use advanced natural language processing techniques to generate human-like text. They have been trained on vast amounts of data and can perform tasks such as text completion, translation, and\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"I would like to talk about large language models and their capabilities.\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8crLopQjKaOs"
   },
   "source": [
    "### Chat models: AIMessage, HumanMessage, SystemMessage, ChatMessage (rol arbitrario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models define tipos de mensajes para hacer prompting con el llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "QGaYUhmoKkmZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelo/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    ChatMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System define la instrucci√≥n. Human es el input del humano. AI es la salida (respuesta del llm). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhmZqlaGK_Wd",
    "outputId": "135132a6-1917-4ec3-d1df-2da0a0ba1dd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelo/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore programmer.\", response_metadata={'token_usage': <OpenAIObject at 0x77b121d74040> JSON: {\n",
       "  \"prompt_tokens\": 26,\n",
       "  \"completion_tokens\": 6,\n",
       "  \"total_tokens\": 32\n",
       "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-953d1ff6-a60a-40a3-8c95-24fa21819489-0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJdWe8QGLPlI",
    "outputId": "79152592-eea0-4c7f-8c69-62a67383c87e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='Me encanta programar.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='Me encanta programar.', response_metadata={'token_usage': <OpenAIObject at 0x77b121d99490> JSON: {\n",
       "  \"prompt_tokens\": 26,\n",
       "  \"completion_tokens\": 6,\n",
       "  \"total_tokens\": 32\n",
       "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-45b99bea-db36-421d-930c-ff4c5d07702d-0'))], [ChatGeneration(text='‰∫∫Â∑•Áü•ËÉΩ„ÅåÂ§ßÂ•Ω„Åç„Åß„Åô„ÄÇ', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='‰∫∫Â∑•Áü•ËÉΩ„ÅåÂ§ßÂ•Ω„Åç„Åß„Åô„ÄÇ', response_metadata={'token_usage': <OpenAIObject at 0x77b121d9a6b0> JSON: {\n",
       "  \"prompt_tokens\": 27,\n",
       "  \"completion_tokens\": 10,\n",
       "  \"total_tokens\": 37\n",
       "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f82cbe89-e6d7-4224-8b4f-c4c587dc4acc-0'))]], llm_output={'token_usage': {'prompt_tokens': 53, 'completion_tokens': 16, 'total_tokens': 69}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('45b99bea-db36-421d-930c-ff4c5d07702d')), RunInfo(run_id=UUID('f82cbe89-e6d7-4224-8b4f-c4c587dc4acc'))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Spanish.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Japanese.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result =chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
