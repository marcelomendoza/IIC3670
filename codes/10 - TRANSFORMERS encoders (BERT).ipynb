{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIC-3670 NLP UC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Versiones de librerías, python 3.8.10\n",
    "\n",
    "- numpy 1.20.3\n",
    "- flair 0.12\n",
    "- allennlp 0.9.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voy a trabajar con bert base uncased de la librería flair. Necesita torch 1.9.0 o mayor. Cargo el encoder y construyo el embedding de una oración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence[6]: \"George Washington was born in Washington\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "embedding = TransformerWordEmbeddings('bert-base-uncased')\n",
    "#embedding = TransformerWordEmbeddings('roberta-base')\n",
    "\n",
    "sentence = Sentence('George Washington was born in Washington')\n",
    "\n",
    "embedding.embed(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vemos el embedding de cada palabra de la oración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"George\"\n",
      "tensor([-7.7858e-02,  5.0599e-01, -1.7383e-01, -5.7768e-01,  8.5991e-01,\n",
      "        -4.4526e-01,  5.1470e-01,  1.9235e-01,  1.1062e-01, -8.2717e-01,\n",
      "        -1.7893e-01, -5.7098e-01, -3.0730e-02,  1.1882e-01, -6.7864e-01,\n",
      "        -1.3466e-01,  7.5936e-01,  1.0573e-02, -1.1145e-01,  1.5357e-01,\n",
      "        -8.8367e-01,  3.9509e-01, -4.4996e-01,  2.6870e-01,  6.0829e-01,\n",
      "         2.0158e-01,  1.2647e-01,  7.0656e-01,  3.7739e-02, -8.3689e-01,\n",
      "         4.5149e-01, -5.0183e-01,  2.2537e-01,  5.7245e-01, -5.3343e-01,\n",
      "        -2.9224e-01, -2.3169e-01,  8.5069e-01,  5.1448e-01, -3.4850e-01,\n",
      "         1.7520e-01, -3.5871e-01,  7.0467e-01, -4.2313e-01,  1.7307e-01,\n",
      "        -1.6502e-01,  2.0721e-01, -1.0091e+00,  6.8144e-02, -8.5671e-01,\n",
      "         3.5520e-01, -1.4831e-01, -3.1613e-01,  7.2204e-01,  3.6080e-01,\n",
      "         4.0647e-01, -2.5118e-01, -1.2226e-01,  5.5348e-02, -4.6344e-01,\n",
      "        -1.1319e-02,  8.2398e-01,  6.5795e-01, -1.5961e-01, -4.9053e-01,\n",
      "        -6.0648e-02, -1.5673e-01, -3.7897e-01, -1.4432e-01,  1.0388e+00,\n",
      "         4.7132e-01, -4.3393e-01, -1.6608e-01,  1.3084e-01,  5.7115e-01,\n",
      "         9.6314e-04,  8.1874e-01,  1.0926e+00, -5.8858e-01, -6.6390e-01,\n",
      "        -7.8015e-01,  1.9022e-01,  2.8423e-01,  7.8506e-01, -1.1169e+00,\n",
      "         1.8726e-01, -5.6424e-01, -4.2918e-01,  1.8045e-01,  2.4799e-01,\n",
      "        -6.8197e-01,  2.9507e-01,  2.7288e-01, -3.2184e-01, -3.7894e-01,\n",
      "        -2.7925e-01, -6.3352e-01, -4.7196e-01,  4.6535e-01, -8.7373e-01,\n",
      "        -2.9880e-01, -9.4944e-01, -3.7810e-01,  1.5694e-01, -1.0420e+00,\n",
      "         2.8620e-01,  3.4529e-01,  5.8898e-02, -4.4456e-01, -1.4733e+00,\n",
      "         1.5447e-01,  5.3284e-02, -2.3310e-01, -6.9807e-01, -1.2999e-01,\n",
      "         1.0906e+00,  2.9579e-01, -2.8162e-01,  2.7062e-01,  5.0760e-01,\n",
      "        -3.1283e-01,  1.3722e+00,  1.5180e-01,  6.5229e-01, -8.3554e-01,\n",
      "        -7.0322e-01,  3.8134e-01,  8.6078e-02, -1.9286e-01, -5.0053e-01,\n",
      "        -4.7568e-01,  1.2095e-01, -7.9956e-01, -2.4077e-01, -5.2250e-01,\n",
      "         1.0240e+00, -5.1001e-02,  2.1356e-01, -1.9271e-01,  5.5075e-01,\n",
      "         1.1046e+00,  2.0750e-03, -2.4049e-01, -2.8273e-01,  2.5277e-01,\n",
      "        -1.1462e-02, -1.9934e-01,  2.4255e-01, -1.7127e-01,  7.1357e-01,\n",
      "         3.4591e-01,  2.8181e-01,  1.9052e-02,  6.6297e-01, -3.9951e-01,\n",
      "        -5.4889e-01,  3.3032e-01,  7.5597e-01,  9.2932e-01,  3.2533e-01,\n",
      "         5.8959e-01, -5.5106e-01, -1.8193e-02,  5.1034e-01, -1.4547e+00,\n",
      "         2.5898e-01, -2.9389e-02,  8.9169e-02, -1.4583e-01,  4.6426e-02,\n",
      "        -3.9400e-02,  4.4375e-02,  4.7380e-01, -2.2854e-02,  3.9950e-01,\n",
      "         2.8889e-01,  6.0839e-02,  1.4226e-01, -1.4429e-01,  5.4467e-01,\n",
      "        -4.6382e-01,  8.7975e-01,  9.9741e-01, -4.8031e-01,  1.0741e+00,\n",
      "        -4.7238e-01,  4.8261e-01, -1.1910e+00,  4.7948e-01,  7.8550e-01,\n",
      "         7.0044e-01, -1.3182e-01, -3.4756e-01, -1.0747e+00,  1.0249e+00,\n",
      "        -3.7002e-01,  3.4990e-01, -6.2066e-01, -7.5647e-01, -3.7165e-01,\n",
      "        -9.4727e-01, -6.9916e-01,  1.0672e+00,  1.1883e-01,  3.4550e-02,\n",
      "         1.3716e+00,  3.4309e-02, -2.2297e-01,  9.0698e-01, -1.0871e-01,\n",
      "        -3.4394e-01, -6.9932e-01,  1.6675e-01, -2.1670e-01, -5.2182e-02,\n",
      "        -2.1258e-01,  3.2405e-02,  5.5544e-01,  7.7544e-01, -4.7370e-02,\n",
      "         4.2412e-01,  9.0500e-01, -8.6496e-01,  8.8402e-01, -6.4859e-01,\n",
      "         1.5203e+00, -9.7751e-01,  4.6081e-01, -2.1465e-01, -5.6033e-01,\n",
      "        -2.7837e-01, -3.4632e-01,  6.1177e-01,  7.5398e-01, -2.1370e-01,\n",
      "        -1.7591e-01, -5.8717e-01,  4.7233e-02,  5.0507e-01, -3.0110e-01,\n",
      "         1.0908e-01,  1.1036e-01, -5.3850e-01,  1.0849e+00, -1.0472e+00,\n",
      "         1.5444e-01, -7.7866e-02, -7.0345e-01, -1.9421e-01, -4.2249e-01,\n",
      "        -3.6066e-01, -8.4057e-01, -1.1188e-01, -3.5944e-01,  5.1147e-01,\n",
      "        -7.4978e-01, -4.4234e-02,  1.8910e-01, -4.7036e-01,  6.7732e-01,\n",
      "        -9.4140e-01,  3.2820e-01,  5.2177e-02,  6.5686e-01, -3.2805e-01,\n",
      "        -1.1716e+00, -2.6324e-01,  7.2335e-01,  5.3814e-01,  8.3804e-01,\n",
      "         1.1945e-01,  3.1183e-01,  1.2380e-01, -1.7299e+00,  7.6693e-02,\n",
      "        -5.8388e-01,  1.1949e+00,  1.1049e-01,  6.3185e-02, -2.1567e-01,\n",
      "        -4.5599e-01,  1.9235e+00,  1.3756e-01,  2.9515e-01,  2.3846e-01,\n",
      "        -6.8651e-01, -5.1649e-01, -6.6488e-01,  1.2264e-01,  6.2619e-01,\n",
      "         2.8987e-01, -2.4244e-01, -5.6565e-01, -3.1673e-01,  7.7391e-01,\n",
      "         4.3624e-01,  9.1485e-01, -2.1961e-01,  5.0056e-01, -3.2580e-02,\n",
      "        -1.2092e+00, -3.4083e-01,  6.2488e-01, -2.2231e-01,  5.8210e-01,\n",
      "        -2.5296e-01, -3.0224e-01,  4.6656e-01, -2.3897e+00, -5.8087e-01,\n",
      "         1.9562e-01, -9.9350e-02, -4.8747e-01, -7.3417e-01, -3.9935e-01,\n",
      "        -8.3991e-01, -9.8665e-01,  6.6400e-01,  3.1706e-01,  2.1305e-01,\n",
      "        -2.3880e-01,  3.8496e-01,  1.2948e-01,  1.3542e-02,  2.7082e-01,\n",
      "         5.2931e-01,  4.0782e-01,  4.1804e-01, -1.7152e-01,  4.6720e-01,\n",
      "        -8.4677e-01, -5.6537e-01,  1.3365e-01, -1.4564e-01, -2.4042e-01,\n",
      "        -8.7559e-01, -7.5101e-02,  1.9433e-01,  1.1241e-01,  4.4855e-01,\n",
      "         1.8855e-02,  1.9434e-01,  2.1357e-01,  1.2109e-01, -1.1395e+00,\n",
      "         6.6373e-02,  3.1506e-01, -3.0975e-01,  4.8244e-01, -9.3395e-01,\n",
      "         1.8497e-01,  4.5814e-02,  8.8066e-01,  4.1171e-01,  2.5289e-01,\n",
      "        -3.6286e-01, -8.4428e-02,  1.8503e-02,  2.5835e-01,  8.7979e-02,\n",
      "         2.4238e-01, -1.2623e-01,  1.1967e+00,  4.7031e-01, -4.7782e-01,\n",
      "         1.0449e+00, -4.9993e-01, -1.0666e-01, -4.2634e-02, -1.9845e-02,\n",
      "        -2.9487e-01,  9.3120e-01,  3.5013e-02, -8.3106e-01,  2.6994e-01,\n",
      "         1.2935e-01,  2.1307e-01,  5.7983e-02,  1.3927e-01,  1.6699e-02,\n",
      "         3.3063e-01, -1.5913e+00, -8.2100e-01, -6.6721e-01, -1.1618e-01,\n",
      "         6.9392e-01, -1.3642e-01, -1.5048e-01, -1.7954e-01, -2.6469e-01,\n",
      "         6.0518e-01,  6.2546e-01, -6.0656e-01,  7.7047e-01, -4.9240e-01,\n",
      "        -5.4241e-01, -1.1862e+00,  1.1805e-01, -8.0731e-01,  2.2409e-01,\n",
      "         6.9800e-01,  2.0238e-01, -2.5125e-01,  2.3030e-01, -1.9440e-02,\n",
      "        -8.6410e-01,  3.0527e-01, -6.4292e-01, -8.5135e-01, -3.2052e-01,\n",
      "         2.0395e-02, -8.4169e-01,  4.7521e-01,  4.0150e-01, -3.7576e-01,\n",
      "        -8.6667e-01,  2.1625e-01,  1.3226e+00, -1.7691e-01, -1.0717e+00,\n",
      "         7.5332e-01, -6.6502e-01, -4.9308e-01, -5.6400e-01,  1.3231e-01,\n",
      "         3.6210e-01, -8.7659e-01, -1.4443e-01,  5.9748e-01,  2.8819e-01,\n",
      "        -7.2176e-02, -7.3252e-01,  6.0415e-01,  5.3692e-01, -5.2253e-01,\n",
      "        -8.6675e-01, -8.6729e-01, -4.0941e-01, -3.9481e-01, -8.3481e-01,\n",
      "         2.1437e-01, -6.6507e-01,  7.7902e-02, -7.4538e-02,  2.4917e-02,\n",
      "        -6.5881e-01, -6.3853e-01, -5.6836e-01, -6.6129e-01, -3.5702e-01,\n",
      "        -3.1139e-01,  6.8291e-01,  2.7783e-01, -2.1620e-01,  3.7623e-01,\n",
      "        -3.9560e-01,  4.9194e-01, -2.2180e-01,  2.7969e-01, -3.8105e-01,\n",
      "        -3.9669e-01,  6.8574e-01, -4.5060e-02, -1.0148e+00,  7.4753e-01,\n",
      "        -9.8876e-01, -3.8595e-01,  8.9628e-02, -8.5558e-02,  1.4800e-01,\n",
      "        -6.3356e-01, -4.3728e-01,  1.1947e-01, -1.7470e-01, -6.0642e-02,\n",
      "         3.7217e-01,  1.5535e-01, -2.3507e-01,  4.2961e-01, -5.6843e-02,\n",
      "         7.6310e-01, -4.1192e-01, -4.1384e-01,  3.6837e-01, -3.2435e-01,\n",
      "         9.3231e-02,  1.5954e-01,  5.4674e-01,  4.3522e-01,  9.9165e-01,\n",
      "        -2.5244e-01, -1.0148e+00, -5.2613e-01,  1.3293e+00, -8.6239e-01,\n",
      "        -2.5222e-01,  9.4811e-02,  9.3831e-01, -1.7810e-01,  6.0361e-01,\n",
      "        -1.9503e-01, -3.3237e-01,  3.0524e-02, -1.0102e+00,  4.2401e-01,\n",
      "         8.4583e-01,  2.6921e-01, -4.5590e-01,  3.5157e-01,  1.5553e-01,\n",
      "         4.9009e-01,  2.5205e-01, -2.2463e-02, -2.3081e-01,  1.3075e-01,\n",
      "         6.0013e-01, -3.8004e-01,  5.9474e-01, -1.2047e+00,  1.5721e-01,\n",
      "        -4.2957e-01, -4.1757e-01, -5.7850e-02, -3.5521e-01,  2.5466e-01,\n",
      "        -6.1938e-02,  5.4563e-01, -6.9271e-01,  5.3303e-01,  4.2605e-01,\n",
      "         2.2243e-01,  4.1916e-01,  1.7614e-01, -3.9953e-01, -3.7227e-01,\n",
      "        -5.7784e-01,  1.1879e-01, -8.1895e-01, -6.7902e-01, -9.5025e-02,\n",
      "         5.3963e-01,  3.8009e-01,  1.1195e+00,  1.9358e-01, -5.7831e-01,\n",
      "         3.8946e-01,  5.3675e-01,  2.8927e-02,  6.1680e-01,  2.1566e-01,\n",
      "        -9.4177e-01,  2.0046e-01,  1.1754e+00,  1.5158e-02,  2.3372e-01,\n",
      "         1.3424e+00,  6.5542e-02,  6.8474e-01,  9.4478e-01, -3.5721e-02,\n",
      "         5.8761e-01, -1.0233e+00,  8.1808e-01, -2.9976e-01, -3.1791e-01,\n",
      "        -1.5851e-01,  3.1661e-01, -3.6034e-01,  4.4866e-02,  5.9526e-01,\n",
      "        -5.1382e-01,  3.6107e-01, -2.3390e-01,  3.4078e-01, -6.6480e-01,\n",
      "         8.7837e-01, -1.1356e+00, -2.8725e-01, -1.0119e-01, -4.7573e-01,\n",
      "        -2.8068e-01, -2.4444e-02, -3.4944e-01, -1.4629e-01, -1.8313e-01,\n",
      "         4.4707e-01, -1.0434e+00,  8.3563e-02,  7.8190e-01,  1.1903e+00,\n",
      "        -3.6712e-01, -7.1614e-01, -2.6711e-01,  5.8435e-02, -5.7409e-02,\n",
      "         6.2534e-01,  4.7723e-01,  1.7540e-02, -3.2068e-01, -1.6046e-01,\n",
      "        -1.6391e-01, -9.9720e-02, -8.6417e-01,  2.1908e-01,  9.3302e-02,\n",
      "         1.4190e-01, -1.0589e+00,  1.1268e-01, -1.1446e+00,  5.2339e-01,\n",
      "         3.6151e-01,  1.0115e-01, -3.2598e-01, -2.2397e-01,  6.0662e-01,\n",
      "        -1.2777e-01,  2.3632e-01,  1.3400e-01, -1.4257e-01, -8.3540e-01,\n",
      "        -9.3726e-03, -4.4390e-01, -2.8843e-01,  2.1232e-02,  6.3025e-01,\n",
      "        -8.1679e-02,  4.1266e-02,  2.2402e-01, -5.2317e-02,  2.2655e-01,\n",
      "        -5.3888e-01, -7.9479e-01,  8.0033e-01,  8.2993e-02, -8.2065e-01,\n",
      "        -9.6806e-01, -1.2384e-01,  3.3909e-01, -1.4749e-01,  7.2800e-01,\n",
      "         4.6488e-01,  5.6540e-01,  2.0737e-01,  1.0062e-01,  1.1241e+00,\n",
      "        -6.4832e-01, -8.7439e-01,  6.4425e-01,  4.3269e-01,  8.5543e-01,\n",
      "        -3.9398e-01,  1.2247e+00, -2.7624e-01, -2.4081e-01, -1.9132e-01,\n",
      "         8.6905e-02,  7.7562e-01,  3.2819e-02,  1.0269e+00,  4.0816e-01,\n",
      "        -5.9349e-02, -8.3861e-01,  1.7764e-01, -3.4172e-01,  6.5016e-01,\n",
      "         8.2779e-01,  5.0885e-01, -1.5972e+00, -1.9100e-01, -3.3700e-01,\n",
      "         4.7322e-01, -1.0921e+00,  9.6032e-01,  8.9161e-01, -4.1936e-01,\n",
      "        -7.3911e-02,  3.2418e-01, -2.7927e-01,  4.0170e-01,  1.7668e-01,\n",
      "        -2.9485e-01,  1.3514e-03, -1.6331e-01,  1.0460e+00,  8.7084e-01,\n",
      "        -4.3970e-01,  7.3745e-01, -6.8017e-01,  4.0695e-01, -8.9187e-02,\n",
      "        -1.2088e-01, -2.5051e-01,  2.5269e-01, -5.2072e-02,  3.6825e-02,\n",
      "         2.1796e-02, -1.0383e+00, -1.9412e-01,  1.3799e+00, -5.4218e-01,\n",
      "        -8.0074e-01, -3.1303e-01,  7.1903e-02, -4.1527e-01, -1.5818e-01,\n",
      "         2.8806e-01, -2.5580e-01, -1.1500e-01,  1.1965e-01,  4.3155e-02,\n",
      "        -6.1211e-01, -2.1877e-01,  1.8336e-01,  1.4564e+00, -5.6291e-01,\n",
      "         1.4033e+00, -1.2433e+00, -3.3950e-01, -1.6713e-02, -2.4085e-01,\n",
      "        -7.3276e-01, -2.9256e-02, -1.3104e+00,  7.6737e-02,  1.2367e+00,\n",
      "         6.3062e-02,  6.7410e-01, -2.1082e-01, -4.4899e-01,  5.8611e-01,\n",
      "         2.3924e-01,  1.8873e-01, -5.8942e-03, -1.1248e+00,  3.3871e-01,\n",
      "         4.6703e-01, -4.1029e-01, -2.1425e-01, -4.6438e-01, -4.1825e-01,\n",
      "         1.0534e+00, -2.5812e-01, -5.9875e-01, -7.6585e-01,  8.4406e-01,\n",
      "         2.2330e+00, -6.0906e-01, -7.7412e-01, -7.0250e-02, -2.9600e-01,\n",
      "        -8.6268e-02,  2.4638e-01, -3.7448e+00, -9.2325e-01, -3.8932e-01,\n",
      "        -4.0045e-01,  1.2579e+00,  7.4168e-01,  3.7584e-01,  3.1752e-01,\n",
      "         4.5083e-01,  4.8560e-01,  8.4262e-01,  1.6004e-01,  1.8867e-01,\n",
      "        -8.0793e-01,  3.3206e-01,  6.0188e-01], device='cuda:0')\n",
      "Token[1]: \"Washington\"\n",
      "tensor([ 6.3381e-02,  7.2031e-01, -6.0794e-02, -1.0293e-01,  6.5148e-01,\n",
      "         4.2003e-01,  8.8936e-01,  9.2006e-01, -1.0925e+00, -2.3685e-01,\n",
      "         1.8024e-03, -4.9206e-01,  8.6291e-02,  1.6308e-01, -1.0227e+00,\n",
      "        -1.1294e-01,  4.6446e-01, -8.1369e-02,  4.9322e-01, -3.2804e-01,\n",
      "        -9.3864e-01,  1.8021e-01, -3.7178e-01,  4.3298e-01,  8.2238e-01,\n",
      "         4.1436e-02,  5.9463e-02,  2.1285e-01, -1.4842e-01, -5.6964e-01,\n",
      "        -7.6523e-02, -4.0511e-01,  4.4726e-01,  9.1014e-02,  8.9674e-02,\n",
      "        -2.5563e-01,  2.9980e-01,  6.2208e-01, -1.4501e-01, -3.7384e-01,\n",
      "        -9.4703e-01, -9.9503e-01,  8.1239e-02, -9.5795e-01, -2.5513e-01,\n",
      "        -3.6431e-01,  1.6621e-01,  3.6063e-01,  6.7962e-01, -4.2366e-01,\n",
      "         4.8519e-02,  3.1896e-01, -9.4870e-01,  5.9727e-01,  4.6361e-02,\n",
      "         1.0228e+00,  4.1185e-01, -1.1833e+00, -7.7504e-01, -3.0868e-02,\n",
      "         1.6697e-01,  3.4578e-01,  5.5157e-01, -5.0453e-01,  7.7046e-01,\n",
      "        -4.2830e-01,  2.2851e-01,  4.4950e-01,  5.3446e-03,  3.4201e-01,\n",
      "        -1.0134e+00,  1.4601e-01, -7.2993e-01,  2.1849e-01,  6.1391e-01,\n",
      "         1.1552e-02,  7.6238e-01,  1.5727e+00, -4.5879e-01, -6.3491e-01,\n",
      "        -1.6055e+00,  9.4321e-01,  7.0474e-01,  9.7519e-01, -7.3853e-01,\n",
      "         1.2196e+00, -5.2300e-01, -1.7934e-01,  2.8979e-01,  4.2468e-01,\n",
      "        -5.2539e-01, -9.5656e-02,  8.4720e-02, -4.8055e-01, -5.7001e-01,\n",
      "         2.8917e-01,  6.8557e-02, -3.2590e-01,  3.5750e-02,  4.6932e-01,\n",
      "        -2.9927e-01, -7.4489e-01,  3.6621e-01,  3.7296e-01, -3.3724e-02,\n",
      "         1.8064e-01,  4.3057e-01,  6.5580e-01, -3.0424e-01,  5.4135e-01,\n",
      "        -3.2545e-01,  5.7291e-01,  3.8400e-01, -1.1782e+00, -2.4570e-01,\n",
      "         8.9526e-01,  9.1766e-02, -1.2790e+00,  2.4119e-01,  1.3657e-01,\n",
      "         1.1736e+00, -1.6836e-01, -3.8854e-01,  1.0155e+00, -4.4003e-01,\n",
      "        -3.3081e-01,  3.5351e-01, -3.4216e-02, -9.0039e-01,  5.1776e-02,\n",
      "        -3.8842e-01,  5.3939e-01, -3.7171e-01,  6.8004e-01,  4.1245e-02,\n",
      "         2.9537e-01, -4.7972e-01,  1.3712e-01, -3.3495e-01,  2.6641e-01,\n",
      "         8.8701e-01,  1.3183e+00,  6.9257e-01, -4.5021e-01,  1.4767e-01,\n",
      "         1.2649e-01,  6.3769e-01,  9.6535e-01,  3.0224e-01, -2.2255e-01,\n",
      "         1.2474e+00, -1.4147e-02, -3.1916e-01,  6.6659e-01,  3.2455e-01,\n",
      "        -4.5708e-01,  4.3409e-01,  9.9370e-01,  7.8992e-01,  2.5698e-01,\n",
      "         1.8645e-02, -1.9592e-02,  9.3746e-02,  1.0363e+00, -1.9354e-01,\n",
      "         1.6853e-01,  2.4255e-01,  1.1764e+00, -6.1243e-01, -3.3451e-02,\n",
      "         2.8723e-01, -3.8763e-01,  3.2303e-01,  5.9321e-01,  7.9087e-01,\n",
      "         3.2739e-01,  4.8304e-01,  4.4048e-01,  4.4927e-01,  4.3905e-01,\n",
      "        -4.8403e-01,  3.0070e-01,  1.2134e+00,  4.1335e-01,  5.3095e-01,\n",
      "        -3.7590e-01, -4.1972e-01, -2.5933e-01,  3.6420e-01,  3.9727e-01,\n",
      "         8.9556e-01, -2.0886e-01, -1.3980e+00, -8.1935e-01,  5.6020e-01,\n",
      "        -5.9027e-01, -2.3547e-01, -7.7920e-02, -8.7068e-01, -7.0544e-01,\n",
      "         5.4165e-01, -4.7147e-01,  4.9558e-01, -4.1117e-01, -8.8164e-01,\n",
      "         3.6888e-01,  2.4908e-02, -3.3109e-01,  1.1537e+00,  4.0377e-01,\n",
      "        -1.3400e-01,  4.4586e-01, -4.1973e-01,  4.3436e-01, -3.6976e-01,\n",
      "        -5.5710e-01,  8.6644e-02, -1.4473e-01,  7.6846e-01,  3.3279e-01,\n",
      "        -5.1806e-01,  6.2876e-01, -6.0758e-01,  8.4267e-01,  1.3355e-01,\n",
      "         1.4181e+00, -2.8116e-01, -7.3761e-01, -6.5881e-01, -1.9173e-01,\n",
      "        -3.1357e-01, -8.6357e-01,  9.4153e-01, -1.2029e-01, -7.4387e-02,\n",
      "        -6.6468e-01,  1.9231e-01, -8.3436e-01, -1.0019e-01, -4.7315e-01,\n",
      "        -2.1546e-01,  2.1723e-01, -4.3246e-01,  7.0523e-01,  5.0084e-01,\n",
      "        -1.6776e-01, -7.8898e-01, -3.5352e-01,  4.0001e-01, -5.4826e-01,\n",
      "        -8.7491e-01, -4.6759e-03,  1.1670e-01, -2.5493e-01, -1.1107e+00,\n",
      "        -8.8604e-01,  4.1102e-01,  7.4474e-02, -2.0627e-01,  1.2305e+00,\n",
      "        -4.3386e-01,  4.1450e-01, -1.6372e-01,  6.3343e-01, -5.3928e-01,\n",
      "        -8.9440e-01, -2.6268e-01,  1.4673e+00,  1.5935e-01, -7.4419e-01,\n",
      "         7.2651e-01, -3.1578e-01,  1.2365e-01,  4.1419e-01,  7.8938e-01,\n",
      "        -8.3272e-01,  1.4852e-01, -2.1412e-01, -7.1993e-01, -6.9672e-01,\n",
      "        -2.4378e-01,  1.5065e+00, -3.8324e-01, -4.3394e-01, -2.7774e-01,\n",
      "        -7.2357e-01, -4.4536e-01,  8.7996e-01, -3.3448e-01,  4.8480e-01,\n",
      "         2.4570e-01, -7.8205e-01, -2.2210e-01, -1.0218e+00,  1.1636e-01,\n",
      "         1.1992e+00,  4.5884e-01,  3.3691e-01,  8.7671e-01,  5.6238e-01,\n",
      "        -1.2127e+00, -5.8191e-01,  5.5548e-01, -2.3375e-01,  1.5648e-02,\n",
      "        -1.0141e+00,  1.5323e-01,  2.4814e-02, -4.5184e-01,  1.1796e-01,\n",
      "         6.9099e-01, -3.8476e-01, -3.8382e-01,  3.5412e-01,  3.0435e-03,\n",
      "        -1.9552e-01, -1.5185e+00,  1.0715e+00,  8.6893e-02, -5.5791e-01,\n",
      "         4.9162e-01, -2.9573e-02,  1.9865e-01,  4.1606e-01,  3.2854e-01,\n",
      "        -1.3648e-03,  3.3141e-01,  3.4346e-01, -1.5039e-01, -2.0154e-01,\n",
      "        -7.3095e-01, -7.3174e-01,  2.9833e-01,  3.7954e-03, -2.0690e-03,\n",
      "        -3.2257e-01,  1.0211e-01, -4.6925e-01, -8.2623e-02, -3.9189e-01,\n",
      "         7.2053e-01, -1.5121e-01,  4.7235e-01,  6.8570e-02, -8.3833e-01,\n",
      "        -1.3357e+00,  6.8303e-02, -3.9339e-01,  3.9477e-02, -2.2884e+00,\n",
      "        -1.4401e-02, -4.8000e-01,  1.9039e+00, -4.1131e-01,  1.7761e-01,\n",
      "        -2.6271e-01, -4.9701e-01,  1.9959e-02,  4.2430e-01,  6.7619e-02,\n",
      "        -7.3927e-01, -1.0624e+00,  8.1978e-01,  9.6761e-01,  5.9477e-01,\n",
      "         1.3735e+00, -1.3129e+00, -2.2260e-02, -3.5087e-01,  2.3919e-01,\n",
      "        -1.3534e+00,  5.2891e-01,  3.4272e-01, -1.3862e+00, -5.1275e-01,\n",
      "         4.1007e-03, -1.6634e-01,  5.5867e-01,  2.8490e-01,  3.9721e-01,\n",
      "         4.9837e-01, -1.3044e+00, -5.6438e-01, -9.9968e-01, -2.1614e-01,\n",
      "         2.1664e-02, -6.1949e-01,  2.3410e-02, -5.1632e-01, -4.8880e-01,\n",
      "         7.3878e-01,  4.2183e-01, -4.2979e-01,  2.2024e-01, -2.5319e-01,\n",
      "         4.4599e-01, -3.1026e-01, -7.6300e-01, -9.9270e-01,  6.8562e-01,\n",
      "         1.0103e-01, -1.7582e-01, -4.7677e-01,  5.5386e-01,  6.3303e-01,\n",
      "        -5.4584e-01,  6.9510e-01, -1.8337e-01, -6.2084e-01, -4.1892e-01,\n",
      "        -3.0853e-01, -9.0130e-01, -6.1374e-01, -2.4535e-01, -6.5025e-01,\n",
      "        -4.3305e-01, -2.9389e-02,  4.2680e-01, -3.4930e-01, -1.2481e+00,\n",
      "         8.2601e-01, -1.1016e+00, -8.5874e-01, -1.1582e+00,  1.0892e+00,\n",
      "         5.9412e-01, -3.3389e-02, -4.0727e-02, -2.5443e-01,  4.7813e-01,\n",
      "        -1.8592e-01, -1.4211e+00,  5.9494e-02,  2.3529e-01, -9.5155e-01,\n",
      "        -5.1745e-01, -1.8993e-01, -2.8634e-01, -1.0006e-01, -6.1455e-01,\n",
      "        -1.6311e-01, -2.5341e-01, -3.7947e-01, -7.0108e-03,  1.0391e+00,\n",
      "        -1.7208e+00, -9.4633e-01, -7.9077e-01, -4.7755e-01, -3.3789e-01,\n",
      "        -7.6647e-01, -7.5075e-01,  3.5404e-01, -4.7435e-01,  2.7094e-01,\n",
      "         7.9025e-01,  1.5215e-01, -4.6490e-01, -3.2558e-02, -5.8236e-01,\n",
      "        -4.9101e-01,  5.9811e-01,  1.7811e-01, -3.1098e-01,  9.8128e-01,\n",
      "        -7.4003e-01, -2.3389e-01, -3.3038e-01,  5.4171e-01,  5.9418e-01,\n",
      "        -5.4398e-01, -1.8131e-01, -2.9109e-01,  1.3297e+00, -8.3629e-01,\n",
      "        -6.5267e-01, -1.6784e-01, -2.7424e-01,  3.3210e-01, -8.1543e-01,\n",
      "         2.6528e-01,  7.8616e-02, -6.9443e-01,  6.5680e-01, -3.6947e-01,\n",
      "        -2.9228e-01, -2.3730e-01, -1.7676e-01,  2.7600e-01,  1.1900e-01,\n",
      "        -4.4310e-01, -8.2591e-01,  5.5596e-01,  1.3884e+00, -1.5888e-01,\n",
      "        -7.4681e-01, -1.8276e-01,  6.9256e-01, -8.8696e-01,  8.2823e-01,\n",
      "        -9.7186e-02,  2.4546e-01,  3.9023e-01, -1.8518e+00,  6.7631e-01,\n",
      "        -8.4517e-01,  6.8999e-02, -9.1579e-01,  6.4734e-01, -1.8585e-01,\n",
      "         6.7784e-01,  5.7781e-01, -8.4618e-01,  2.8940e-01, -3.0710e-01,\n",
      "         4.3817e-01,  1.5009e-01,  3.7573e-01, -2.6178e-01,  1.7800e-01,\n",
      "        -1.2272e+00, -4.1834e-01,  2.5419e-01, -3.6051e-01, -3.6757e-01,\n",
      "        -1.5818e-01,  6.6216e-02,  5.0965e-02, -8.7190e-01, -2.5043e-01,\n",
      "         6.1893e-01,  3.2320e-01, -9.5636e-01, -1.3617e+00,  7.1791e-02,\n",
      "         4.3468e-01, -7.9896e-01, -7.3110e-01, -7.7418e-01, -1.0307e-01,\n",
      "        -2.6846e-01, -2.8729e-01,  5.9338e-01,  5.4972e-01, -3.7046e-01,\n",
      "        -2.7494e-01,  4.3559e-01,  3.6790e-02,  2.3100e-01, -3.3573e-02,\n",
      "        -1.7401e+00, -6.5177e-01,  1.1174e+00, -5.1130e-01,  2.4645e-01,\n",
      "         1.2020e+00, -3.0990e-01,  3.4894e-02,  1.3055e+00, -4.2547e-02,\n",
      "        -1.6783e-01, -5.0185e-01,  9.2122e-01,  1.5410e-01, -1.1094e-01,\n",
      "         3.7282e-01,  3.4569e-01, -9.5961e-01, -2.0548e-01,  2.2805e-01,\n",
      "        -8.0774e-01,  2.9656e-01,  1.4870e-01, -8.5645e-03,  2.2295e-01,\n",
      "         3.2922e-01, -3.2229e-01, -7.2723e-01, -8.7997e-01, -2.0564e-01,\n",
      "         2.2837e-01,  7.4434e-01, -4.5386e-01,  5.1486e-01, -7.9399e-01,\n",
      "        -1.2730e-01,  1.2891e+00, -5.1392e-01,  3.5292e-02,  1.8379e+00,\n",
      "        -7.8621e-01, -9.1308e-01,  1.0234e+00, -3.3266e-01, -2.9719e-01,\n",
      "         9.4565e-01,  9.0012e-01, -3.3109e-01,  7.4090e-01,  1.9942e-01,\n",
      "        -7.1064e-02, -7.0692e-01, -1.0321e+00,  1.9558e-01,  9.9924e-01,\n",
      "         1.4264e+00, -1.6230e+00,  1.9482e-01, -4.9167e-01,  5.4753e-01,\n",
      "        -3.2362e-01,  7.6589e-01, -1.0384e+00,  2.8876e-01, -3.4344e-01,\n",
      "         1.5801e-01,  1.6179e-01, -7.4757e-02, -8.8109e-01, -1.1832e+00,\n",
      "         5.9813e-01,  4.5873e-01, -4.8600e-02,  2.0190e-01,  6.3585e-01,\n",
      "        -3.7146e-01, -1.1585e-01,  2.8398e-01, -3.9978e-01, -1.7011e-01,\n",
      "         2.0922e-02,  2.0926e-01,  5.4060e-01, -1.2463e-01, -2.3304e-01,\n",
      "        -8.7350e-02, -4.8320e-01,  2.4414e-01, -3.4679e-02,  5.2434e-01,\n",
      "         1.3767e-01,  1.0223e+00,  6.7778e-02, -6.9083e-01, -8.9155e-01,\n",
      "         2.2808e-01,  2.4608e-02,  7.4511e-01,  6.8306e-01,  2.5060e-01,\n",
      "         1.8889e-01,  9.4631e-01,  8.1251e-01, -3.6288e-01, -2.5327e-01,\n",
      "        -3.2034e-01,  1.2282e+00,  9.4743e-01,  9.6658e-01,  1.1398e+00,\n",
      "         2.7042e-01, -8.2572e-01,  4.0291e-01, -6.7017e-01, -2.6857e-01,\n",
      "         3.8543e-01, -2.2488e-01,  5.5169e-01,  2.9134e-01, -1.2108e+00,\n",
      "         8.5789e-01, -5.7195e-01,  5.7062e-01,  5.3949e-01, -3.5478e-02,\n",
      "         2.4763e-01, -3.7966e-01, -1.2778e-01,  5.3071e-01,  3.8100e-01,\n",
      "        -3.8893e-01, -1.3604e+00, -8.0327e-01,  4.4151e-01,  9.6306e-01,\n",
      "        -1.7008e-01,  7.7634e-01, -1.0112e+00, -8.3015e-01, -3.0105e-01,\n",
      "        -4.7746e-01, -3.7858e-01, -1.0709e-01,  7.4033e-02,  3.1632e-02,\n",
      "        -2.1187e-01, -3.4323e-01, -1.1220e+00,  8.9203e-01, -1.0132e+00,\n",
      "        -7.1611e-01, -1.5287e-01, -5.6746e-01, -7.5084e-01,  5.9903e-01,\n",
      "         3.9308e-01,  6.7148e-01, -6.0141e-01, -4.6431e-01, -1.2814e-01,\n",
      "        -2.6608e-01,  3.4747e-01,  3.0510e-01,  1.0717e+00, -6.9221e-01,\n",
      "         1.1016e+00, -1.3309e+00, -4.0121e-01, -3.8781e-01,  8.3600e-01,\n",
      "        -8.3689e-01,  4.6700e-01, -1.6417e+00,  5.8599e-01,  8.4960e-01,\n",
      "         2.6167e-01,  9.5159e-02,  4.5915e-01,  1.6646e-01,  5.7236e-01,\n",
      "         6.2095e-01,  1.2327e+00, -5.3219e-01, -8.3578e-01, -1.0748e-02,\n",
      "         6.1107e-01,  5.9073e-01,  3.9317e-01,  9.3707e-02, -3.0392e-01,\n",
      "         6.9351e-02, -3.9748e-01,  1.4557e-01,  1.4867e-01, -1.0570e-02,\n",
      "         1.5220e+00, -9.8907e-01, -3.3552e-01,  3.2322e-01,  3.4998e-02,\n",
      "         2.0908e-01,  1.1831e+00,  1.1352e+00, -5.6320e-01,  1.1206e-01,\n",
      "        -4.4526e-01,  5.2539e-02,  2.9129e-01,  7.6975e-02, -1.4120e-01,\n",
      "        -9.5651e-02, -2.1306e-01,  5.6755e-01,  5.9708e-01,  2.3081e-01,\n",
      "        -1.1360e+00, -3.7364e-01,  1.0167e+00], device='cuda:0')\n",
      "Token[2]: \"was\"\n",
      "tensor([ 0.0428,  0.2444, -0.5477,  0.1994,  0.0081,  0.2105,  0.4619,  1.1330,\n",
      "        -0.4868,  0.4702,  0.7332, -0.3424, -0.5837, -0.0128, -0.6834,  0.0873,\n",
      "         0.1165, -0.3993, -0.5210, -0.3553, -0.2056,  0.4220,  0.2924,  0.7351,\n",
      "         1.3786,  0.6901,  0.3272, -0.3961,  0.8712, -0.6878,  0.2423,  0.2357,\n",
      "         0.1771, -0.1409, -0.7948, -0.0931,  0.3967,  0.1499, -0.2705,  0.5161,\n",
      "        -1.6282, -0.8383,  0.4670, -0.1092, -0.6229, -0.2476,  0.3055, -0.3641,\n",
      "         0.5623, -0.9495,  0.0827,  0.6771, -0.8862,  0.6371, -0.3898,  1.4374,\n",
      "        -0.0540, -1.4664, -0.6481,  0.5304, -0.1330,  0.2373,  0.0948, -0.2625,\n",
      "         0.1589, -0.1747,  0.0943,  1.1723, -1.0309, -0.4910, -0.3053, -0.5099,\n",
      "        -0.1152,  0.5793,  0.0688, -0.0572,  0.3961,  1.3017,  0.4144, -0.9300,\n",
      "        -1.0008,  0.6022,  0.0651,  1.4704, -0.5298,  0.7528, -0.0314, -0.2229,\n",
      "        -0.3345,  0.4354, -0.6922, -0.9268, -0.0321, -0.2876,  0.4364,  0.3794,\n",
      "        -0.5204, -0.5987, -1.0914, -0.3656, -0.2992, -0.6494,  0.4499,  0.5499,\n",
      "        -0.7025,  0.4000,  0.8745,  1.0961, -0.6434,  0.4828,  0.6547, -0.2539,\n",
      "        -0.1256, -0.9726,  0.1656,  0.7632,  0.6707, -1.0149, -0.2161,  0.6667,\n",
      "        -0.0268, -0.7077, -0.7556,  0.0809, -0.4547, -0.4178, -0.2300,  0.0441,\n",
      "        -0.0821, -0.1869, -0.4699,  0.3077, -0.3705, -0.6518,  0.0126,  0.9543,\n",
      "        -0.1233, -0.0059, -0.5426,  0.8027,  0.4691,  0.1617,  1.4780, -0.5927,\n",
      "         0.3954, -0.0810, -0.1988,  0.6187,  0.3721, -0.0511,  0.6174, -0.4504,\n",
      "        -0.1318,  0.3951, -0.6402, -0.7589,  0.3828,  1.1790, -0.2125,  0.3279,\n",
      "         0.5561,  0.1090,  0.4020,  0.5382, -0.0825, -0.2211,  0.4066, -0.0269,\n",
      "        -0.0328,  0.5499, -0.3062, -0.5537,  1.1663,  0.0533,  0.9703,  0.7709,\n",
      "         0.2532,  0.3363,  0.1589,  0.3662, -0.5002, -0.6652,  0.8958,  0.3246,\n",
      "         0.2382, -0.3347, -0.5593,  0.1078, -0.4897, -0.2040, -0.1491,  0.1010,\n",
      "        -1.2188, -0.0818,  0.5829,  0.3322,  0.0976, -0.6456, -0.6794,  0.2307,\n",
      "         0.2415, -0.4140,  0.7012,  0.1984, -0.4679, -0.2150, -0.1746, -0.0186,\n",
      "         0.7305,  0.3936, -0.5602,  0.7611, -0.2165, -0.0777, -0.6006, -0.2139,\n",
      "        -0.6342,  0.4818,  0.2091, -0.0834, -0.1325,  0.7967, -1.2032,  0.8929,\n",
      "        -0.2575,  1.7102, -0.2382,  0.0821, -0.3753,  0.7235, -0.4598, -0.7068,\n",
      "         0.1801,  0.2506, -0.4299,  0.1446, -0.7652, -0.2984, -0.1495, -1.2337,\n",
      "        -0.3791,  0.5014, -0.2167, -0.2843,  0.5053, -0.6616, -0.8416, -0.4727,\n",
      "         0.5734, -0.7615, -0.3035, -1.1802,  0.1636, -0.4350, -0.4214, -1.1365,\n",
      "         0.7754,  0.1402, -0.7564, -0.2038,  0.5930,  0.7846,  0.2220,  0.4703,\n",
      "         0.2479, -1.2040, -0.6100,  1.6570,  0.3881, -0.0912, -0.4537, -0.0769,\n",
      "         0.4446,  1.1973,  0.7955, -0.4830,  0.5923,  0.5744,  0.7220,  0.4510,\n",
      "         0.6210,  0.7486, -1.0855, -0.2118, -0.4111,  0.0036, -0.5189,  1.3072,\n",
      "         0.3151, -0.4019,  0.3352, -1.0364, -0.4992, -0.9198,  0.3990,  1.4252,\n",
      "         0.7117,  0.1019,  0.2386,  0.0360, -0.5382, -0.5339,  0.4370,  0.2609,\n",
      "        -0.0036, -0.5849,  0.3033,  0.0477, -1.7325,  0.4425,  0.1978, -0.3376,\n",
      "         0.0160, -0.4205, -0.5342, -0.4575, -1.0028, -0.0882,  0.0830, -0.7000,\n",
      "         0.4924,  0.1294,  1.1198,  0.4817,  0.6147,  0.2642, -0.9499,  0.8558,\n",
      "         0.3599, -0.0676, -0.5730, -0.5465,  0.5172,  0.1754,  0.0971, -0.4103,\n",
      "        -0.7412, -0.0447,  0.3876, -0.2991,  0.5324, -0.1958, -0.4392, -0.0677,\n",
      "        -0.0180, -0.1667,  0.6131,  0.1303, -0.3299, -1.4112,  0.1639, -0.6451,\n",
      "         1.2510, -0.1884, -0.1183,  0.1399, -0.2948, -0.7616, -0.0969, -0.4446,\n",
      "        -0.7979, -0.9531,  0.8245,  0.7345,  0.5483,  0.8439, -0.7144, -0.1561,\n",
      "         0.7346, -0.0235, -0.7026,  0.1182,  0.7343, -0.6660,  0.0308,  0.3199,\n",
      "         0.1982,  0.7617, -0.0753, -0.0412, -0.8451, -1.1611,  0.2889, -0.4263,\n",
      "        -0.3479, -0.0087,  0.5281, -0.5569, -0.0173, -0.7108,  0.1019,  0.3710,\n",
      "        -0.1077,  0.6743, -0.6881,  0.3551, -0.7770, -1.1405,  1.0119,  0.2986,\n",
      "        -0.5405,  0.1436, -0.1691, -0.5095, -0.3482,  0.4428,  0.1080, -0.4754,\n",
      "         0.0316, -0.3532,  0.1196, -0.0725,  0.4950,  0.5373, -0.7405,  0.3510,\n",
      "        -0.5915,  0.4563, -0.3015, -1.7142,  1.0082, -1.1205, -0.9578, -0.6663,\n",
      "        -0.1132,  0.6982,  0.1248,  0.2463,  0.0517,  1.4201,  0.0175, -1.0801,\n",
      "         0.4673,  0.1272, -0.1665, -1.1949, -0.6518, -0.1736,  0.5847,  0.1586,\n",
      "         0.5037,  0.7821, -0.3617, -0.5478, -0.5417, -1.2531,  0.6345,  0.1187,\n",
      "        -0.4686,  0.3108,  0.6057, -0.0958, -0.0227,  0.1949,  0.1319, -0.0593,\n",
      "         0.1346, -0.2921, -0.7577, -0.2834, -0.0105,  0.4513, -0.0821, -0.1946,\n",
      "         0.0514, -0.6064, -0.6430, -0.8497,  0.5182,  0.2151,  1.2267,  0.1468,\n",
      "        -0.0512,  0.6551, -0.2062, -0.5590, -0.1585, -0.1167,  0.7283, -0.6181,\n",
      "         0.2642, -0.4224, -0.2543,  0.8008, -0.4678, -0.3953, -0.3170,  0.3506,\n",
      "        -0.0423,  0.1096, -0.2650, -0.9418, -0.2475, -0.0273,  0.0298, -0.3966,\n",
      "         0.0996,  0.4061, -1.3862,  0.7828, -0.4724,  0.5221, -0.0517, -1.2730,\n",
      "         0.4091, -0.3674,  0.3292, -0.0028, -0.1329, -0.6311,  0.2352,  0.3569,\n",
      "        -0.8318, -0.1976, -1.2100,  0.2540, -0.0563,  0.9311,  0.0415,  0.4371,\n",
      "        -0.4958, -0.0604,  0.3325, -0.3611,  0.4338,  0.8404, -0.2840, -0.7055,\n",
      "        -0.5934, -0.2070,  0.6961, -0.2106, -0.7970, -0.3201,  0.4899,  0.3930,\n",
      "        -1.1783, -0.9740, -0.7467, -0.0384, -0.9194,  0.4292, -0.1719, -0.9371,\n",
      "         0.5221, -0.2837, -0.1004, -0.0297,  0.4292,  0.2913,  0.1197, -0.0427,\n",
      "         1.3646, -0.4058, -0.1371,  0.9485, -0.2755,  1.4189,  0.1107,  0.1488,\n",
      "        -0.4038, -0.5280,  0.9516, -0.8155,  0.0952,  0.4049, -0.1651, -1.1060,\n",
      "        -0.3417,  0.5322, -0.8265,  0.3604,  0.1839, -0.6482,  0.2001,  0.3776,\n",
      "        -0.1639, -1.6769, -0.4981,  0.2277, -0.0734,  0.4641, -0.2835,  0.0115,\n",
      "         1.1764, -0.2841,  0.2466, -0.0637, -1.2950, -0.2338, -0.6914, -1.1048,\n",
      "         0.4874, -0.6762, -0.4376,  0.3303,  0.0897, -0.1151,  0.2160,  0.2928,\n",
      "        -0.8322,  0.2817,  0.3331, -0.2581,  0.0506,  0.2005,  0.0864, -0.2021,\n",
      "        -0.2654,  0.2152,  0.4945,  0.5676,  0.2845, -0.6395,  0.0805, -0.7710,\n",
      "         1.4887,  0.2523, -0.6433, -0.6988,  1.0624, -0.0708, -0.8308, -0.1651,\n",
      "         0.8214,  0.0522, -0.4481,  0.2215, -0.1984, -0.3223,  0.6976,  0.4898,\n",
      "         0.6511,  0.0961, -1.3656, -0.9894, -0.8754, -0.2731, -0.4002, -0.0399,\n",
      "        -0.3821,  1.0808, -0.2268, -0.0895, -0.2124,  1.0215, -0.2369,  0.2288,\n",
      "         0.7394, -0.0995,  0.2058,  0.6047,  0.3152,  0.3541, -0.1313,  0.2558,\n",
      "         0.2249,  0.7090,  0.3965,  0.9385,  0.4455, -0.5006,  0.7037,  0.2563,\n",
      "         0.9068,  1.1797,  0.1443,  1.1337,  0.7336, -0.2304,  1.3153, -1.3456,\n",
      "        -0.2778,  1.1260, -0.5447,  0.3205, -0.1520,  0.1820, -0.3347, -0.2138,\n",
      "        -0.3717, -0.3892, -0.5072,  0.4790,  0.9296, -0.4082,  0.1453,  0.1300,\n",
      "        -1.2921,  0.2681,  0.0505,  0.0830,  0.3167,  0.3790,  0.2886, -0.1351,\n",
      "        -1.0344, -1.2027,  0.7419,  0.3931,  0.5434, -0.4871, -0.0575, -0.3653,\n",
      "         0.9793, -0.1263,  0.2529, -0.6734, -0.6292, -1.0390, -1.1739,  0.7306,\n",
      "        -0.0219,  0.9260, -1.0356,  1.7475, -1.3361, -0.0199, -0.4490,  0.2472,\n",
      "        -0.6120,  0.5463, -1.0804,  0.2550,  0.3696,  0.3606, -0.2388,  0.5984,\n",
      "        -0.6077,  0.5457, -0.1905,  1.1290, -0.9723, -1.1710,  0.8630,  0.5968,\n",
      "        -0.3855, -0.2120, -0.2325, -0.1730, -0.2160,  0.0645,  0.3684, -0.6320,\n",
      "         0.0728,  1.6486, -0.8596, -0.5275,  0.0189,  0.0881, -0.3135, -0.1295,\n",
      "         1.5882, -0.4096,  0.0858, -0.2059,  0.6077, -0.3947,  0.7533, -0.1847,\n",
      "        -0.0645, -0.5264,  0.4754,  0.3530,  0.3232, -0.3535, -0.5050,  0.9348],\n",
      "       device='cuda:0')\n",
      "Token[3]: \"born\"\n",
      "tensor([ 2.8487e-01,  3.3005e-01, -5.1277e-01,  2.0579e-01,  1.1245e-01,\n",
      "        -1.0174e-02, -2.9421e-01,  1.3699e+00, -9.2065e-01,  2.8002e-01,\n",
      "         8.2970e-01, -4.8789e-01, -5.3307e-03,  5.5093e-01, -5.6994e-01,\n",
      "         8.0894e-02,  5.3062e-02,  2.2635e-01, -5.2690e-01, -1.5710e-01,\n",
      "         4.1034e-01,  4.0775e-01,  3.6772e-01,  9.0898e-01,  2.0125e+00,\n",
      "         5.3214e-01,  4.0313e-01,  2.2549e-01,  7.7804e-01, -9.2400e-01,\n",
      "        -5.2485e-01,  3.1613e-01,  9.5907e-02,  1.6652e-01, -7.3604e-01,\n",
      "        -4.8573e-01, -7.5031e-01,  6.1280e-01, -5.4153e-02,  3.6604e-01,\n",
      "        -9.3423e-01, -7.8139e-01,  7.7196e-02,  1.9958e-01, -7.0778e-01,\n",
      "        -2.0569e-01,  6.4921e-02,  9.2220e-02,  4.2795e-01, -7.6091e-01,\n",
      "        -6.8123e-01, -3.3405e-02, -5.4789e-01,  8.9365e-01,  1.2832e-01,\n",
      "         7.4512e-01, -4.8709e-01, -8.0507e-01,  2.3023e-02,  3.9811e-01,\n",
      "        -1.7736e-01,  7.4856e-01,  1.6828e-02, -2.0982e-01, -2.9138e-01,\n",
      "         1.2319e-01, -6.0526e-01,  6.7329e-01, -2.8782e-01,  1.6322e-01,\n",
      "         6.4506e-05,  3.5729e-01, -4.1003e-01,  5.9868e-01,  4.1768e-01,\n",
      "         1.6755e-01,  2.0739e-01,  5.8248e-01,  4.7408e-02, -1.1091e+00,\n",
      "        -3.8213e-01, -3.3664e-01, -2.6161e-01,  1.1376e+00,  4.2304e-01,\n",
      "         6.5476e-01,  5.1848e-01,  1.8979e-01,  2.2567e-01,  3.7567e-01,\n",
      "        -3.2688e-01, -3.7540e-01, -5.4012e-01,  1.5768e-01, -3.4474e-01,\n",
      "         1.0549e+00, -8.7214e-01, -9.0219e-01, -1.1863e+00, -3.4362e-01,\n",
      "        -9.5463e-03, -3.3922e-01,  6.6567e-02,  3.6685e-01, -4.4370e-01,\n",
      "         4.8590e-01,  5.0369e-01,  8.9290e-01, -7.7509e-01, -9.2879e-02,\n",
      "         2.1224e-01,  3.6560e-01, -1.0887e-02, -7.4672e-01, -4.5036e-01,\n",
      "         1.2678e+00,  4.4602e-01, -9.2442e-01, -5.9049e-04, -5.3761e-02,\n",
      "        -5.1305e-01, -3.8354e-01, -4.5713e-01, -1.6831e-01, -7.1992e-01,\n",
      "         5.1006e-01,  8.5700e-02,  2.0033e-01,  1.8720e-01, -6.5579e-02,\n",
      "        -4.4234e-02,  2.7106e-01, -1.1050e-01,  2.7249e-01,  6.4909e-03,\n",
      "         4.8110e-01,  8.6042e-01,  4.8587e-01, -2.3744e-01,  4.0532e-01,\n",
      "         4.5125e-02, -6.2939e-01,  5.6657e-01, -6.2499e-01,  6.0523e-01,\n",
      "         4.6862e-01, -8.4842e-01, -5.7253e-02, -3.1412e-01, -2.8795e-01,\n",
      "         4.6431e-02,  2.0497e-01,  1.3107e-01,  8.6123e-01, -9.9449e-01,\n",
      "        -9.6443e-02,  3.6408e-01,  5.9168e-01, -3.8716e-01,  4.4901e-01,\n",
      "         5.9799e-01,  4.3425e-01,  7.0246e-01,  2.0282e-01,  3.5570e-01,\n",
      "         4.8027e-01, -1.4915e-02, -3.8523e-01, -1.2458e+00,  4.3288e-01,\n",
      "         2.5222e-01, -5.5123e-01,  6.1198e-01, -2.9704e-02,  1.0947e+00,\n",
      "         5.5067e-01, -6.4553e-01, -1.0336e-01,  2.9757e-01,  4.1290e-01,\n",
      "        -3.1912e-01, -5.9481e-01,  6.9180e-01,  3.6459e-01,  1.9751e-01,\n",
      "        -2.3919e-01, -8.0305e-01,  3.0962e-02, -2.3543e-01, -5.0834e-01,\n",
      "        -3.7837e-01, -3.1586e-03, -1.1017e+00, -1.7553e-01,  4.1581e-01,\n",
      "         4.9524e-01, -2.2430e-01, -8.0472e-01, -1.3752e-01,  3.9316e-01,\n",
      "        -1.5720e-01, -1.6868e-01,  3.7605e-01,  1.0134e+00, -4.3525e-01,\n",
      "        -6.7278e-02, -4.6111e-01, -1.0017e-01,  4.9269e-01,  6.4379e-01,\n",
      "        -1.5395e-01,  5.3591e-01, -3.1589e-01, -2.7225e-01, -7.1085e-01,\n",
      "         6.9948e-02, -7.0927e-01, -5.9231e-01, -8.1201e-01,  9.6442e-01,\n",
      "        -1.2562e-01,  5.7254e-01, -3.2082e-01, -1.5926e-02, -6.5110e-01,\n",
      "         1.1803e+00,  3.1138e-01, -5.9208e-01,  1.4917e-01,  2.3907e-01,\n",
      "        -5.1324e-01, -8.0094e-01,  3.3085e-01,  3.8667e-01, -9.5638e-02,\n",
      "        -1.5081e-02, -7.6921e-01,  1.4166e-01,  6.0481e-02, -5.7559e-01,\n",
      "        -2.0636e-01,  6.1342e-01, -3.9113e-01, -7.5762e-01,  2.7921e-01,\n",
      "        -4.1914e-01,  2.7401e-02, -4.7680e-01,  9.4637e-02, -8.6765e-01,\n",
      "        -3.0565e-01, -5.3738e-01, -5.1980e-01,  8.2086e-02, -2.6827e-01,\n",
      "        -1.5760e+00,  8.2705e-02,  2.4107e-01, -6.6216e-01,  7.1823e-01,\n",
      "         1.6679e-01,  6.0459e-01,  1.7325e-01, -4.3215e-01, -5.1232e-02,\n",
      "        -9.7800e-01, -6.5889e-01,  6.9834e-01,  7.2435e-02, -2.4828e-01,\n",
      "         1.6687e-01,  6.4702e-02, -2.4666e-01,  1.4075e+00,  1.1689e+00,\n",
      "        -6.5378e-02,  6.0948e-01,  1.7730e-01,  1.2535e+00,  3.3775e-01,\n",
      "         1.2593e-01,  1.0743e+00, -1.2965e+00,  4.8959e-01, -7.3601e-01,\n",
      "        -2.8819e-01,  3.6894e-01,  8.0972e-01, -1.8741e-01, -1.1496e-01,\n",
      "         6.1542e-01, -7.6231e-01, -1.1579e-01, -1.3388e-01,  1.0346e+00,\n",
      "         4.9621e-01,  1.9595e-01, -3.2101e-02, -9.5952e-02, -3.2479e-01,\n",
      "        -1.6213e+00, -4.1252e-01,  4.1829e-01, -1.3239e-01, -3.0717e-01,\n",
      "        -1.8190e-01,  5.1916e-01, -1.6353e-01, -2.7758e+00,  1.0484e+00,\n",
      "         5.8706e-01, -2.7697e-01,  4.6437e-02,  1.2207e-01, -5.9836e-01,\n",
      "        -7.1537e-01, -8.2701e-01,  4.3802e-01,  6.2035e-01, -1.5760e-01,\n",
      "         2.1417e-01,  1.7264e-01,  8.1804e-01,  6.2226e-01, -2.5296e-01,\n",
      "         1.8383e-01, -7.5255e-01,  6.4463e-01, -8.4145e-01,  5.0035e-01,\n",
      "        -6.0312e-01,  1.3314e-01,  3.4544e-01,  3.5661e-01, -8.3597e-01,\n",
      "        -4.2745e-01, -9.9811e-01,  6.4142e-02,  2.1417e-01,  7.4874e-02,\n",
      "         5.8777e-01, -3.2292e-01, -3.0719e-01, -2.3731e-01, -1.0322e-01,\n",
      "        -3.2275e-01,  6.8658e-01,  1.2247e-01, -6.3243e-01, -6.4139e-01,\n",
      "        -4.8868e-01,  2.2354e-01, -1.6674e-01,  1.2163e-01, -1.5928e-01,\n",
      "         7.6592e-01, -1.5793e-01, -3.9579e-01,  3.4182e-01, -2.5266e-01,\n",
      "         9.2286e-03, -3.3647e-01,  1.1650e+00,  7.4235e-02,  4.1234e-01,\n",
      "         2.6129e-01, -1.6429e+00, -4.7874e-01,  7.8122e-01,  7.8701e-01,\n",
      "        -1.0615e+00,  6.6154e-01,  4.7963e-01, -1.6953e+00, -6.2221e-02,\n",
      "        -1.7885e-01, -8.1898e-01,  7.7121e-01,  3.9078e-01, -7.2225e-01,\n",
      "        -5.6670e-01, -6.2543e-01, -1.7854e-02, -6.9996e-01, -3.4908e-01,\n",
      "        -8.5443e-02,  5.2912e-01, -8.2405e-01,  3.0860e-01, -1.9358e-02,\n",
      "        -2.4433e-01,  7.3346e-01,  5.6357e-01,  7.7393e-01, -1.0312e+00,\n",
      "         8.7376e-02, -3.2043e-01, -7.9667e-01,  7.7217e-01,  7.8292e-01,\n",
      "        -1.9202e-01, -2.4928e-01,  4.5562e-01, -8.7535e-01, -2.3511e-01,\n",
      "         8.8395e-02,  3.0504e-01, -5.9682e-01,  3.0845e-01, -3.8723e-01,\n",
      "        -7.1641e-01,  3.4988e-01, -1.3186e-02,  9.9686e-01, -7.1985e-01,\n",
      "         3.5000e-01, -4.7798e-02, -1.2545e-01, -2.4118e-01, -7.7861e-01,\n",
      "         2.2921e-01, -1.3084e+00, -4.7586e-01,  4.3878e-02,  4.3128e-02,\n",
      "         6.2277e-01,  4.9037e-01,  2.3351e-01, -1.1876e-01,  8.4528e-01,\n",
      "        -6.9901e-01, -1.6194e+00,  3.5118e-01, -1.0584e+00, -4.8646e-02,\n",
      "        -1.0078e+00, -7.6266e-02, -2.4295e-03, -7.5634e-02,  7.2338e-01,\n",
      "         2.1735e-01,  6.5679e-01,  3.3510e-01, -6.2973e-01,  6.2009e-03,\n",
      "        -9.5788e-01,  3.8250e-01,  5.1528e-01, -3.4024e-01,  1.3161e-01,\n",
      "         9.5018e-01,  1.4052e-01,  8.1393e-01,  2.2982e-01,  1.0493e-02,\n",
      "         3.0337e-02, -5.2546e-01,  2.1902e-01, -6.5869e-01, -1.4374e-02,\n",
      "         2.5656e-01,  2.0449e-01, -9.7794e-01, -7.1083e-01,  4.8917e-01,\n",
      "        -1.9643e-02,  2.4836e-01, -4.9669e-01, -1.7918e-01, -1.2431e-01,\n",
      "         9.7112e-01, -3.2483e-01, -3.7819e-01,  1.5129e+00, -1.7450e-01,\n",
      "         4.0504e-01, -1.0061e+00,  9.4177e-02,  4.9372e-01, -4.2151e-01,\n",
      "         1.9101e-01, -7.9169e-02,  4.4426e-02,  4.3838e-01,  2.4704e-01,\n",
      "         3.1531e-01,  1.1883e+00, -3.1311e-01, -4.6844e-01,  2.3409e-01,\n",
      "        -2.4356e-01, -8.0554e-01,  1.7403e-01,  4.9910e-01, -8.7597e-01,\n",
      "        -5.1854e-02,  4.0984e-01, -5.1269e-01, -1.1293e+00, -1.5385e-01,\n",
      "        -7.6766e-01, -6.9216e-03, -1.9419e-01, -9.4209e-01,  7.2019e-01,\n",
      "        -6.4392e-01,  4.1163e-01,  2.6519e-01, -6.9765e-01, -6.9422e-01,\n",
      "        -2.8369e-01,  4.0104e-01, -4.7738e-01,  1.6878e-01, -1.5264e+00,\n",
      "         3.9900e-01, -3.8349e-01,  1.1164e+00,  8.6776e-02, -1.1439e-01,\n",
      "        -9.1071e-02, -7.8175e-02,  6.7040e-01, -1.9434e-01,  1.0028e+00,\n",
      "         6.8060e-01, -4.4572e-02, -3.5479e-01,  5.7956e-02,  3.5572e-02,\n",
      "         8.0615e-01, -9.4650e-01, -3.2850e-01, -4.1692e-01,  1.8335e-01,\n",
      "         3.1881e-01, -5.4585e-01, -3.1146e-01, -3.4494e-01,  6.0240e-04,\n",
      "        -9.7379e-01,  2.0904e-01, -2.0868e-01, -8.0584e-01,  5.8496e-02,\n",
      "        -1.4610e+00, -5.7389e-01,  8.4793e-02,  4.6608e-01,  2.9824e-01,\n",
      "        -2.5578e-01,  5.9333e-01,  1.7285e+00, -1.6917e-01,  5.1519e-01,\n",
      "         7.6701e-01, -1.7767e-01,  1.2810e+00,  1.5120e-01, -8.1087e-02,\n",
      "        -4.3939e-01, -8.0108e-01,  6.1619e-01, -9.2059e-01,  2.6035e-01,\n",
      "        -3.8827e-01,  1.6898e-02,  1.3474e-02, -3.1976e-01,  5.4257e-01,\n",
      "        -2.3885e-01,  4.7060e-01, -3.0440e-02, -5.5941e-01,  2.8334e-01,\n",
      "         5.5845e-01, -1.0896e-01, -1.3068e+00, -1.0371e+00, -4.3319e-01,\n",
      "        -2.7271e-01,  2.6028e-01, -5.0761e-01, -3.0899e-01,  6.4342e-01,\n",
      "        -4.6425e-01,  3.4446e-02,  4.5933e-01, -5.6996e-01, -5.0866e-01,\n",
      "        -3.0022e-01, -8.3186e-01,  2.3393e-01, -4.5349e-01,  9.7390e-02,\n",
      "         5.7486e-01, -1.0930e-01,  1.2730e-02,  5.3212e-01,  2.6832e-01,\n",
      "         2.4812e-01,  1.0420e+00, -2.0390e-01, -7.4887e-02, -7.1138e-01,\n",
      "         8.6654e-03,  4.0939e-01,  7.1637e-01, -3.0167e-01,  5.4906e-01,\n",
      "         6.0150e-01, -1.5747e-01, -3.9071e-01, -7.2122e-01, -2.1122e-01,\n",
      "        -1.0515e+00,  9.8522e-01,  2.2449e-01, -1.9138e+00, -2.3350e-01,\n",
      "         9.5846e-01,  4.4190e-01, -1.3257e+00,  1.8328e-01,  3.2338e-01,\n",
      "        -2.7615e-01, -7.2269e-01, -1.7563e-01,  8.8874e-03, -5.1001e-01,\n",
      "         1.1964e+00,  9.9255e-01,  8.0031e-01,  5.7153e-01, -1.1902e+00,\n",
      "        -7.7246e-01,  5.5602e-01, -7.3195e-02, -1.7997e-01, -5.1739e-01,\n",
      "        -5.9410e-01,  8.0945e-01,  2.5728e-02, -1.5122e-01,  2.3323e-01,\n",
      "         5.9481e-01, -8.2279e-01,  6.7914e-01,  7.7924e-01,  1.1600e-01,\n",
      "         6.6231e-01, -1.4183e-01,  1.1952e-01,  9.5376e-02, -4.0418e-01,\n",
      "         3.6491e-01,  5.9893e-01,  1.5258e-01,  2.9640e-01,  3.2130e-01,\n",
      "         9.9728e-01, -8.2307e-01,  8.6899e-01,  2.5387e-01,  1.0461e+00,\n",
      "         9.4819e-01,  4.6640e-02,  1.0369e+00, -1.4457e-01, -6.7634e-01,\n",
      "         1.1268e+00, -1.4623e+00, -3.7738e-01,  5.6042e-01,  1.7241e-01,\n",
      "        -6.6575e-01,  4.5838e-01,  6.5733e-02,  4.3321e-01, -2.0157e-01,\n",
      "        -7.6617e-02, -3.8291e-01, -2.8171e-01,  2.9643e-01, -6.0922e-01,\n",
      "        -5.7276e-01,  1.3894e-01, -5.0467e-03, -9.6528e-01, -5.7945e-01,\n",
      "        -2.2417e-01,  1.0699e-01, -1.2096e-02,  6.3325e-01,  1.6667e-01,\n",
      "        -6.7843e-01, -5.3590e-01, -9.2842e-01,  7.6228e-01,  7.3194e-02,\n",
      "         9.5523e-01, -6.5624e-01, -1.1741e-01,  5.4214e-01,  3.0320e-01,\n",
      "        -7.4898e-01,  1.0559e-01, -7.3389e-01, -1.2493e-01, -1.5824e-01,\n",
      "        -7.3768e-01,  5.9686e-01,  5.1031e-01,  4.8160e-01,  1.3904e-01,\n",
      "         9.9506e-01, -4.2441e-01,  6.9978e-02,  7.7593e-02,  1.2288e-02,\n",
      "        -5.0386e-01,  5.4251e-01,  4.6281e-02,  4.7221e-02,  3.1083e-01,\n",
      "         2.0906e-01,  1.9882e-02, -6.3667e-01,  8.8721e-02,  6.3591e-01,\n",
      "        -4.2534e-01,  2.9761e-01, -4.7746e-01, -1.5732e+00,  8.3140e-01,\n",
      "         5.7303e-01, -1.0704e+00, -6.6792e-01, -4.2433e-01, -4.1121e-01,\n",
      "         4.8841e-01,  7.2243e-02, -5.6278e-02, -4.3965e-01,  8.5112e-01,\n",
      "         5.3428e-01, -1.2521e-01, -5.9668e-01,  1.2595e+00,  5.3838e-01,\n",
      "         7.6565e-02,  5.2993e-02,  1.2816e+00, -2.9631e-01, -1.7530e-01,\n",
      "        -6.3916e-01, -2.1369e-01, -2.3231e-01,  6.9006e-01, -5.6187e-01,\n",
      "         1.1327e-02,  6.9910e-02,  4.8789e-01, -1.5144e-02, -1.2571e-01,\n",
      "         1.1436e-01, -5.0533e-02,  1.7544e-01], device='cuda:0')\n",
      "Token[4]: \"in\"\n",
      "tensor([-9.0405e-01, -6.5346e-01, -6.4217e-01, -4.8417e-02, -2.2277e-01,\n",
      "         3.2540e-01, -1.0848e+00,  2.0969e+00, -9.0227e-02,  3.8669e-01,\n",
      "        -4.1059e-02, -1.3409e+00, -1.8685e-01,  1.2757e+00, -1.4562e+00,\n",
      "         5.7691e-01,  4.9381e-01,  5.6172e-01, -4.0549e-01,  3.0305e-02,\n",
      "         7.7117e-02,  2.8790e-01, -1.4951e-01,  4.3911e-01,  7.9187e-01,\n",
      "         2.6734e-01,  9.7605e-01, -1.0695e+00,  2.3394e-01,  1.7582e-02,\n",
      "         1.0329e-01,  8.9677e-02, -8.8037e-02,  2.1109e-02, -5.7578e-01,\n",
      "        -5.9727e-02, -3.0756e-01,  2.0109e-01, -3.4658e-01,  4.7103e-01,\n",
      "        -9.7465e-01, -9.0435e-01, -1.0601e-01, -3.9990e-01, -7.0111e-02,\n",
      "        -6.5933e-01,  4.8753e-01,  4.6091e-01,  5.1199e-01, -1.1564e+00,\n",
      "        -1.0025e+00,  5.8238e-01, -5.6821e-01,  1.3329e-01,  7.5784e-02,\n",
      "         1.3852e+00, -6.8852e-02, -8.5047e-01,  2.7291e-01,  9.7940e-01,\n",
      "         8.1318e-01,  7.8547e-01, -8.6029e-01, -4.1667e-01,  3.7258e-01,\n",
      "         2.4923e-01, -5.8479e-01,  8.3255e-01, -3.4488e-02, -1.2853e+00,\n",
      "        -6.5884e-01, -2.3696e-02, -4.3318e-01,  2.6818e-01, -7.2521e-02,\n",
      "         8.4171e-02, -4.5369e-01,  1.0304e+00, -8.9694e-01, -1.2701e+00,\n",
      "         8.7119e-02, -4.2798e-01, -1.0545e-01,  1.1239e-01, -9.4480e-02,\n",
      "         2.0373e-01, -2.9212e-01,  4.7477e-01,  3.4789e-01, -1.3296e-01,\n",
      "        -6.5276e-02,  2.2295e-01, -1.3405e+00,  3.3827e-01, -7.4718e-01,\n",
      "         6.7891e-01, -8.4740e-01,  9.4139e-01, -7.1143e-01,  6.6291e-01,\n",
      "         2.5406e-01, -2.8845e-01, -8.8146e-02,  5.0894e-01,  3.2340e-01,\n",
      "         7.0910e-01,  8.4475e-01,  6.1689e-02, -5.6825e-01,  5.4459e-01,\n",
      "        -1.2875e+00, -5.5096e-01, -4.0584e-02, -5.2920e-01, -4.4151e-01,\n",
      "         1.7335e-01,  2.4910e-03, -6.9403e-01, -5.5145e-01, -6.6566e-01,\n",
      "         3.6073e-03, -3.9752e-01, -5.8913e-01,  1.3007e+00, -1.8336e-01,\n",
      "        -1.0626e-01,  4.2617e-01,  6.0277e-01, -2.7669e-01, -1.0934e-01,\n",
      "        -9.3092e-01, -3.5078e-01,  6.3601e-01,  3.1615e-01,  2.6578e-01,\n",
      "        -2.1492e-01,  1.5063e-01, -1.0684e-01, -5.1250e-01,  1.0560e-01,\n",
      "         8.9376e-01, -1.2138e-02,  1.2890e+00,  6.5438e-01, -3.8704e-01,\n",
      "         1.0045e-01, -7.9205e-01, -6.4891e-01,  5.3997e-01, -3.2400e-01,\n",
      "         5.3988e-01, -4.4942e-01,  2.1861e-01,  3.2558e-01, -1.4429e-02,\n",
      "        -2.0587e-01,  1.0357e+00,  7.6524e-01,  7.3609e-01,  6.5141e-01,\n",
      "         6.4420e-01, -1.6240e-01,  5.4195e-01, -6.2341e-02,  1.5308e-01,\n",
      "        -7.4113e-01,  7.7855e-02,  2.4654e-01, -3.5785e-01,  5.9760e-01,\n",
      "        -3.2319e-01,  1.2830e-01,  1.2611e+00, -1.6122e-01,  1.3957e-01,\n",
      "        -4.7244e-01,  8.8675e-01,  2.0794e-01,  1.0236e+00,  3.1047e-01,\n",
      "        -4.4467e-01, -4.1606e-01,  3.7072e-01,  3.0012e-01,  7.7178e-01,\n",
      "         7.7689e-01, -6.7859e-03,  1.1657e-01,  9.8100e-02,  1.7352e-01,\n",
      "        -2.9627e-01, -3.2680e-01, -9.9989e-01, -7.5401e-01, -2.4929e-02,\n",
      "        -8.8933e-01,  5.6204e-01, -7.7075e-01, -3.8451e-01,  7.8978e-01,\n",
      "        -1.1488e-01, -5.4264e-02,  4.2505e-01,  3.4193e-01, -2.7280e-02,\n",
      "        -3.7401e-01, -3.0321e-01, -3.8672e-01,  8.2688e-01, -6.8620e-03,\n",
      "         1.0405e+00,  2.4442e-01,  9.2615e-01,  9.8252e-02, -5.4792e-01,\n",
      "        -1.8651e-01, -6.4254e-01, -2.5631e-01,  2.6328e-01,  9.0723e-02,\n",
      "        -1.5108e-01,  5.9672e-01,  1.9813e-01,  5.0432e-01,  3.8695e-01,\n",
      "         6.3540e-01, -4.0476e-01, -4.8082e-01, -9.4523e-01,  9.9662e-01,\n",
      "         5.1667e-01, -8.9573e-01, -8.8459e-02, -1.4378e-01, -1.3662e+00,\n",
      "         4.1486e-01, -8.0017e-01, -1.8022e-01,  3.9564e-01, -3.0780e-01,\n",
      "        -2.0037e-01, -7.4656e-04,  3.9475e-02, -6.8115e-02,  1.0227e+00,\n",
      "        -3.4086e-01, -4.1249e-01, -4.2168e-01, -1.6775e-01, -1.3518e+00,\n",
      "         1.8717e-02, -1.4934e+00, -6.3521e-02, -8.7778e-01, -4.8826e-01,\n",
      "        -8.3086e-01, -6.2501e-02,  2.3955e-01, -5.3057e-01,  8.8061e-02,\n",
      "        -4.3980e-01,  3.3490e-01, -1.0906e-01,  2.1253e-01, -9.7130e-01,\n",
      "         8.4817e-02, -1.4687e-01,  5.8772e-01, -2.2913e-01,  3.5397e-01,\n",
      "        -2.0394e-01, -2.1537e-01,  5.9899e-01,  1.3476e+00, -2.1380e-01,\n",
      "         3.9754e-01,  2.4804e-01,  2.6497e-01,  9.7481e-02,  8.3678e-01,\n",
      "         3.4434e-01,  1.8202e+00, -1.0273e+00,  3.7858e-01, -4.4979e-01,\n",
      "        -3.5384e-01, -2.3351e-01,  5.6111e-01, -2.6904e-01, -1.2533e-01,\n",
      "        -2.0941e-01,  4.9242e-01,  2.8402e-01,  3.5693e-01,  7.1875e-01,\n",
      "         1.9253e-01,  1.7471e-01,  9.0801e-01, -1.2496e-01, -1.7483e+00,\n",
      "        -9.2656e-01,  1.5441e-01,  5.7039e-01,  8.2800e-01,  1.9347e-01,\n",
      "        -3.6086e-01,  7.4215e-01, -5.5304e-01, -2.2326e+00,  5.7424e-01,\n",
      "         6.9122e-01, -2.7815e-01,  5.0478e-01, -1.9242e-01, -3.5560e-01,\n",
      "        -6.9434e-01, -1.2384e+00,  5.0738e-01, -1.3682e-01, -1.4947e-01,\n",
      "         2.5534e-01, -4.2398e-01, -1.4114e-02,  3.3849e-01,  3.4040e-01,\n",
      "        -5.4361e-01, -4.0678e-01,  9.3312e-01, -2.6649e-02, -6.9880e-01,\n",
      "        -3.5923e-01,  3.7868e-02,  6.3490e-01,  7.0698e-01, -3.4595e-01,\n",
      "        -1.1072e-01, -6.7327e-01,  1.8113e-01,  3.2212e-01,  8.4742e-01,\n",
      "         7.0605e-01, -3.0388e-01,  7.0938e-03, -2.5996e-01, -1.3034e-01,\n",
      "        -1.0195e+00,  4.2961e-01, -7.4331e-01,  9.9077e-02,  5.4207e-01,\n",
      "        -7.4476e-01, -9.5525e-02,  6.8362e-01,  9.4880e-02, -6.1118e-01,\n",
      "        -9.8823e-02, -1.6270e-01,  5.0211e-01,  9.2743e-01, -7.5407e-01,\n",
      "        -6.8186e-01,  1.3625e-01,  8.5143e-01, -5.7170e-01,  2.1802e-01,\n",
      "         6.5953e-01,  5.9534e-02, -1.0358e+00, -1.0676e+00,  1.8623e-01,\n",
      "        -5.6551e-01,  5.6677e-01, -2.0846e-01, -5.2437e-01, -7.8121e-01,\n",
      "        -2.0454e-01,  3.2356e-01,  6.1935e-01,  2.6936e-01,  9.7556e-01,\n",
      "        -8.6581e-02, -6.6022e-01, -1.9296e-01, -1.2505e-01,  4.7380e-02,\n",
      "         5.3544e-02,  1.4345e-01, -3.9958e-02,  3.4392e-01,  2.7986e-01,\n",
      "        -1.1571e-01, -4.4011e-02,  2.5860e-01, -5.4507e-01,  1.0143e-01,\n",
      "        -8.4545e-01, -6.2722e-01, -9.8763e-01,  6.9593e-01,  1.1646e+00,\n",
      "         5.4092e-02,  7.7594e-02, -7.3693e-03,  6.9366e-02,  2.0302e-01,\n",
      "         2.1229e-01,  1.8016e-01, -9.4605e-01,  3.5024e-01, -1.3271e-01,\n",
      "         6.7128e-01,  1.5219e-01, -5.4402e-01,  7.9943e-01, -1.8442e+00,\n",
      "         1.0295e+00,  6.3340e-01, -3.2810e-01, -4.3598e-01, -8.7826e-01,\n",
      "         3.7586e-01, -2.2230e-02,  1.9629e-01, -3.0708e-01,  6.7240e-01,\n",
      "         2.3309e-01,  6.7140e-01, -5.6436e-02,  1.4255e-01,  8.3004e-01,\n",
      "        -8.5265e-01, -2.1901e-01,  1.6414e-03, -5.5117e-01,  1.1829e-01,\n",
      "        -4.4154e-02, -2.3827e-01,  1.1572e+00, -9.5339e-02, -1.7951e-01,\n",
      "        -4.6085e-01,  2.8931e-03, -5.9890e-02,  3.8958e-01, -8.3071e-01,\n",
      "        -3.3801e-01,  3.0455e-01,  8.6074e-01,  1.1729e-01,  7.8787e-01,\n",
      "         3.2205e-02, -6.8396e-01,  1.2941e-01,  6.1545e-01,  1.5014e+00,\n",
      "         2.2639e-01, -3.1560e-01, -1.6531e-01, -7.4870e-01, -7.2499e-01,\n",
      "         1.2549e-01,  3.6083e-01,  5.8344e-01, -1.0473e+00,  7.7658e-02,\n",
      "        -3.1612e-01,  9.3119e-01, -4.9246e-01, -8.3056e-01,  6.8805e-01,\n",
      "         1.1359e+00,  2.5570e-01, -8.0819e-01,  4.5417e-01,  1.6092e-01,\n",
      "         8.7716e-02, -9.7779e-01,  4.6550e-01,  6.5953e-01, -4.6490e-01,\n",
      "        -6.6962e-01, -8.8550e-01, -1.2248e-01,  3.4940e-01,  3.1255e-01,\n",
      "         7.5175e-01,  3.1600e-01,  4.2869e-01,  5.9877e-01,  2.4446e-01,\n",
      "        -1.2000e+00,  2.8850e-02,  4.4959e-01, -8.4192e-02, -7.0626e-01,\n",
      "        -3.4623e-01,  5.5259e-01, -6.5293e-01, -2.2443e-01,  3.9965e-01,\n",
      "        -7.5203e-04, -1.0738e-01, -1.3062e+00, -3.1815e-01, -4.1145e-01,\n",
      "        -1.6686e+00, -5.2883e-01, -2.1461e-01, -7.4230e-01,  4.4430e-01,\n",
      "        -4.2547e-01, -2.7832e-01, -2.0574e-01,  1.1469e-01,  2.4088e-01,\n",
      "         4.7814e-01,  3.2576e-01,  5.3578e-01, -6.0721e-01, -1.7019e-01,\n",
      "         4.6958e-01,  6.3234e-01,  7.2768e-01,  5.8788e-01,  1.0971e+00,\n",
      "         7.8335e-01, -1.4071e-01,  3.2020e-01, -7.8903e-01,  4.8318e-01,\n",
      "         5.6471e-01, -1.7230e-01,  1.7308e-01, -4.5075e-01, -5.5172e-02,\n",
      "         6.8412e-02,  8.8997e-01, -8.6814e-01, -4.8632e-01, -2.9114e-01,\n",
      "        -6.0256e-01, -6.7162e-01,  1.0723e+00, -6.1762e-01, -2.6780e-01,\n",
      "        -1.3447e+00, -5.9715e-02, -6.3387e-01,  3.3325e-01, -3.2581e-01,\n",
      "         3.5723e-01,  4.3068e-01, -6.9062e-01,  1.0319e-01,  4.6029e-01,\n",
      "         1.1003e+00, -5.2284e-01,  2.9134e-01, -1.0432e+00, -2.5580e-01,\n",
      "         3.1918e-01, -2.6282e-01,  3.8467e-01, -6.6935e-01,  1.5026e-01,\n",
      "         8.8105e-01, -2.9864e-01, -1.9016e-01,  5.7029e-01, -3.9883e-01,\n",
      "        -3.1521e-01,  6.3053e-01,  5.9989e-01, -1.2195e+00,  2.9541e-01,\n",
      "         3.0299e-02,  1.8858e-01, -9.0150e-01,  5.6981e-01, -2.7428e-01,\n",
      "         3.0644e-01,  1.6893e-01,  1.2058e-01,  4.7609e-01,  1.3343e-01,\n",
      "        -4.3369e-01,  6.0878e-01, -1.0323e-01, -2.1884e-01,  1.1488e-01,\n",
      "        -6.2126e-01, -7.0942e-01,  8.6922e-01, -5.0407e-01, -3.5491e-01,\n",
      "         3.6461e-01,  2.5104e-01, -4.0697e-01, -8.4891e-01,  8.6884e-01,\n",
      "        -2.2815e-01,  7.3896e-01, -2.3728e-01, -7.5558e-01, -5.8041e-01,\n",
      "         6.4333e-01,  3.5890e-01,  2.4771e-01, -1.2017e+00,  2.9518e-01,\n",
      "         1.3826e-01,  2.3533e-01,  7.2155e-01,  2.5542e-01, -1.3586e-01,\n",
      "        -1.3760e+00, -1.2333e-01,  1.2295e-01, -2.7800e-01, -6.5778e-01,\n",
      "         4.1281e-01,  1.0459e+00, -1.0212e-01, -4.5064e-01,  6.7476e-01,\n",
      "        -4.1808e-01, -1.4713e-01,  3.5077e-01, -9.1563e-01, -7.0774e-01,\n",
      "         1.4639e+00,  1.3376e+00,  8.7366e-01,  9.8177e-01, -2.8283e-01,\n",
      "         4.4426e-01, -5.0496e-01, -3.7430e-01, -3.1501e-01, -1.7232e-01,\n",
      "        -8.2315e-01,  2.0864e-01, -6.7090e-01,  1.5396e-01, -5.1665e-01,\n",
      "         4.4071e-01, -2.7275e-01,  1.5710e-02,  6.9863e-01, -2.3071e-01,\n",
      "         2.6769e-01,  9.7226e-01,  7.3189e-01, -6.1163e-01, -1.4036e-01,\n",
      "         6.8656e-01,  3.9899e-01, -8.8650e-01,  8.1905e-02,  6.4234e-02,\n",
      "         5.2397e-01, -2.2666e-01, -2.6331e-01,  6.8685e-02,  2.4219e-01,\n",
      "         9.6709e-01,  3.5307e-01,  5.1699e-01,  5.8791e-01, -3.7975e-01,\n",
      "         1.3910e+00, -1.1468e+00, -1.4242e-01,  5.1940e-01,  4.6065e-02,\n",
      "        -2.7482e-02, -1.0783e+00, -5.8669e-02,  2.3314e-01, -1.5898e-01,\n",
      "         4.2404e-01, -4.2044e-01, -5.7601e-01,  1.2491e+00, -3.8632e-01,\n",
      "         2.2003e-01,  2.8087e-01,  4.4272e-01, -9.2166e-02, -1.0478e+00,\n",
      "        -5.8206e-01, -2.9792e-01, -3.4379e-01, -5.0558e-01,  4.7157e-01,\n",
      "        -8.9902e-01, -4.1113e-01, -1.8227e+00,  7.2859e-01, -5.8924e-01,\n",
      "         5.8355e-01,  1.7709e-01, -8.5162e-02, -9.3604e-01,  4.2481e-01,\n",
      "         3.0625e-03,  1.9871e-01,  2.9730e-01,  8.3091e-01, -6.6088e-01,\n",
      "        -8.1073e-01,  1.5223e-01,  8.0947e-01, -1.9270e-01, -5.3334e-01,\n",
      "         4.3395e-01, -7.0535e-01,  2.2561e-01,  2.6318e-02,  9.8710e-01,\n",
      "        -3.3904e-01, -3.3368e-01,  9.5152e-02, -7.2917e-01,  4.7287e-02,\n",
      "        -1.2990e-01, -9.0268e-01,  1.0896e+00, -1.2763e-01, -4.6251e-01,\n",
      "        -1.3726e-01,  2.4862e-02, -5.4352e-01, -1.1669e+00,  1.7898e-01,\n",
      "         3.3610e-01, -3.8207e-01, -1.2034e+00, -1.3865e-01, -6.1976e-01,\n",
      "         1.1058e+00,  3.5835e-01, -1.6924e-01,  2.0894e-01, -7.5273e-01,\n",
      "        -2.2758e-01, -2.9870e-01,  2.0397e-01, -1.6660e-01,  1.2157e-01,\n",
      "        -6.1506e-01, -1.5662e-01,  1.2142e+00,  2.5447e-01, -6.2844e-02,\n",
      "        -2.4972e-01,  5.9285e-01, -2.2398e-01,  7.6094e-01, -8.1764e-01,\n",
      "        -9.8503e-01, -4.9322e-01,  4.2097e-01,  1.6745e-01, -4.3759e-01,\n",
      "        -7.7425e-01, -8.6315e-02,  3.5897e-01], device='cuda:0')\n",
      "Token[5]: \"Washington\"\n",
      "tensor([-3.2762e-01,  2.8569e-01, -6.4598e-01,  1.2565e-01,  1.2554e-01,\n",
      "         3.6998e-01,  2.6697e-02,  1.0353e+00, -5.3932e-01, -5.2051e-01,\n",
      "        -5.4362e-01, -2.3121e-01,  3.8159e-01,  4.5512e-01, -1.2959e+00,\n",
      "        -5.3279e-01,  9.8424e-02,  4.9789e-01,  1.1247e+00, -1.4448e-03,\n",
      "        -2.0132e-01,  2.5456e-02,  3.6753e-01,  4.1814e-01,  2.4605e-01,\n",
      "         4.7660e-02, -8.1270e-02, -7.9981e-01,  1.5288e-01,  4.2073e-01,\n",
      "         4.8875e-02, -6.9061e-01,  1.5932e-01,  3.5538e-01, -4.0643e-01,\n",
      "        -2.7476e-01,  4.3216e-01,  4.2946e-01, -2.6836e-01, -6.7904e-01,\n",
      "        -1.6816e-02, -4.9021e-01,  5.0992e-01, -1.2380e+00,  4.0738e-01,\n",
      "         2.7077e-01,  1.2563e+00,  3.7698e-01,  6.3612e-02, -5.6018e-01,\n",
      "        -1.5494e+00,  9.5468e-01, -1.2296e+00,  6.1923e-01, -4.9913e-01,\n",
      "        -1.5000e-01,  7.1612e-01, -1.5221e-01,  2.2581e-01, -2.3643e-01,\n",
      "         9.8719e-01,  2.2592e-01, -7.5065e-01, -1.7877e-01,  1.5142e+00,\n",
      "        -3.9986e-01,  8.0465e-01, -5.8883e-02, -4.0205e-01,  2.0502e-01,\n",
      "        -7.0612e-01, -6.2880e-03, -1.1386e+00,  2.4558e-03,  6.4971e-01,\n",
      "         6.7828e-01,  1.1473e-01,  3.4532e-01, -7.1320e-02, -9.7272e-01,\n",
      "         2.7239e-02,  8.9924e-01,  3.6963e-01, -2.9798e-01, -1.1933e-02,\n",
      "         2.6602e-01,  9.4609e-02, -1.2999e-01,  1.1697e-01,  6.2468e-01,\n",
      "        -7.2373e-01, -4.3464e-02, -8.6627e-01, -5.5279e-02, -6.8271e-01,\n",
      "         1.3100e-01, -6.4212e-01,  9.7843e-01,  4.3400e-01,  1.1643e-01,\n",
      "        -1.2191e+00, -1.2148e+00, -2.6915e-01,  3.5512e-01, -1.7335e-01,\n",
      "         8.6656e-02, -1.9618e-01, -1.7921e-01,  6.9147e-02, -7.4866e-02,\n",
      "        -6.5006e-01, -2.8683e-01,  8.6963e-01, -1.1008e+00, -3.7397e-01,\n",
      "         1.5642e-01, -3.9933e-01, -1.0434e+00,  6.3244e-02,  4.0916e-02,\n",
      "        -2.9050e-01, -3.3751e-01, -7.5901e-01,  1.0025e+00, -3.4800e-01,\n",
      "         3.6549e-01,  9.9083e-02, -3.7843e-01, -6.4418e-01, -5.7562e-01,\n",
      "         2.0036e-02,  4.4291e-01, -8.1072e-02,  8.2376e-01, -5.3600e-01,\n",
      "        -3.4099e-01, -1.9989e-01,  1.2639e-01,  1.5484e-01, -6.6437e-01,\n",
      "         4.1210e-01,  1.2314e+00,  2.7410e-01,  1.3097e-01,  1.3702e-01,\n",
      "         3.6651e-01,  4.4872e-01, -6.8653e-01,  1.4925e-01,  3.4457e-01,\n",
      "        -8.3095e-02,  6.9235e-01, -4.1849e-02,  2.3183e-01,  6.2722e-01,\n",
      "         4.1099e-01,  3.7073e-01,  8.7491e-01,  1.2895e+00,  2.8216e-01,\n",
      "         6.3762e-01,  2.4316e-01,  2.9054e-01,  1.0690e+00,  9.1114e-02,\n",
      "        -1.6453e-01, -4.0909e-01,  1.4410e+00,  1.1896e-01, -5.5188e-01,\n",
      "         5.3643e-02,  4.9509e-01,  7.2562e-01,  7.3832e-01, -7.0441e-02,\n",
      "         5.5991e-01,  7.8473e-01, -1.8461e-01,  1.2607e-01,  2.2780e-02,\n",
      "        -3.2410e-01,  8.6836e-02,  4.9043e-01, -5.8354e-01,  3.6754e-01,\n",
      "         4.6922e-01,  6.5575e-01, -7.4430e-01,  2.4142e-01, -1.5805e-01,\n",
      "        -3.6194e-01, -3.8746e-01, -8.4381e-01, -5.9856e-01, -9.7523e-02,\n",
      "        -9.2632e-01, -3.7625e-01, -4.5751e-01,  4.9105e-01,  3.6120e-01,\n",
      "         1.1019e+00, -6.1330e-01,  1.9157e-01,  9.9243e-03, -2.8142e-01,\n",
      "         5.0976e-01,  5.9207e-02, -5.9654e-01,  7.1765e-01,  7.8307e-02,\n",
      "         3.9632e-03,  3.1459e-01,  1.3848e-01,  1.1231e+00,  3.8002e-02,\n",
      "         1.2445e-01, -3.7667e-01, -2.1740e-01, -2.7288e-01,  2.7351e-01,\n",
      "         1.8356e-01,  2.3880e-01,  3.5931e-01,  1.8886e-01, -5.6741e-01,\n",
      "         7.2108e-01,  4.4591e-01, -7.5275e-01, -4.4534e-01, -3.3417e-01,\n",
      "         2.5880e-02, -1.1948e+00,  3.0050e-01, -4.2513e-01, -1.1792e+00,\n",
      "         3.2536e-01,  6.1567e-01, -5.1709e-01,  4.2714e-02, -4.7961e-01,\n",
      "        -4.6156e-01,  2.7716e-01, -6.4476e-02,  4.8553e-02,  6.3011e-01,\n",
      "         8.5093e-02,  3.9260e-01, -6.7249e-01,  1.2325e-01, -8.4040e-01,\n",
      "         8.8409e-02, -4.3314e-01,  2.2496e-01, -4.5901e-01, -9.1049e-01,\n",
      "        -8.3239e-01, -5.9438e-02, -2.3807e-01,  6.2994e-02,  1.1738e+00,\n",
      "        -6.7931e-01,  5.5769e-01,  6.1989e-01, -8.9884e-02, -2.4286e-01,\n",
      "         3.3187e-01,  6.5159e-01,  1.6104e-01,  2.7896e-01, -5.1783e-01,\n",
      "        -2.0277e-01, -7.3667e-01,  3.3962e-01,  3.2530e-01,  6.1425e-01,\n",
      "         3.2348e-01,  4.6737e-01, -3.6366e-01,  1.2494e-01, -2.5713e-01,\n",
      "        -8.2694e-02,  9.8546e-01, -8.6248e-01,  2.8811e-01,  1.0324e-01,\n",
      "        -1.0122e+00, -7.8790e-02, -2.8689e-01,  1.0929e-01,  2.4431e-01,\n",
      "         4.7182e-02,  2.8323e-01,  1.6477e-01, -7.7155e-02,  4.8648e-01,\n",
      "         3.9736e-01, -2.3404e-01,  4.9519e-01, -2.0835e-01, -6.9896e-02,\n",
      "        -1.0796e+00, -2.5931e-01,  8.1451e-01,  1.4399e-01,  1.2064e+00,\n",
      "        -3.7215e-01,  5.2721e-01,  1.7359e-01, -3.4763e+00,  4.8023e-01,\n",
      "         4.5000e-01, -4.3997e-01,  4.4083e-01, -3.0981e-01,  4.7920e-01,\n",
      "        -6.1422e-01, -7.6471e-01,  6.6219e-01,  6.7290e-01,  5.2276e-02,\n",
      "         7.5582e-01, -3.8523e-01,  2.1174e-01, -1.0855e-01,  7.2278e-01,\n",
      "        -1.0057e+00, -5.3968e-01,  5.2595e-01, -4.5380e-01, -4.8535e-01,\n",
      "         6.5380e-01, -3.3275e-01,  4.3354e-01,  5.1840e-01, -1.1059e-01,\n",
      "         4.7653e-01, -1.0757e+00, -6.5788e-01,  2.5689e-01,  6.2194e-01,\n",
      "         8.5791e-01,  4.9494e-01,  6.4782e-01,  7.9795e-02, -6.7595e-01,\n",
      "        -5.4679e-01,  1.0718e+00, -1.0828e+00,  7.1729e-01, -5.5204e-01,\n",
      "        -1.8388e-01, -1.3785e-01,  6.1823e-01,  2.5934e-01,  3.4948e-01,\n",
      "        -1.1077e-01, -5.7581e-01,  3.7695e-01,  4.0515e-01, -8.5066e-02,\n",
      "        -2.9065e-01, -9.1417e-01, -3.4575e-01, -4.5739e-01,  2.7817e-01,\n",
      "         1.4463e+00, -3.3800e-01, -3.9148e-01, -3.0660e-01,  2.2305e-01,\n",
      "        -3.3701e-01,  5.9046e-01, -5.8330e-01, -4.9432e-01, -4.1539e-01,\n",
      "        -1.8734e-01,  3.9374e-01,  2.0507e-01,  3.2742e-02,  7.1333e-02,\n",
      "         4.7944e-01, -1.3727e+00, -8.2156e-01,  1.0157e-01, -9.0132e-01,\n",
      "         5.7347e-02, -9.9779e-04, -2.5654e-01,  4.1212e-01,  4.1244e-01,\n",
      "         5.3484e-01,  7.3644e-01,  5.9065e-01,  8.6245e-01,  6.2282e-01,\n",
      "        -6.6342e-01, -1.4114e-01, -8.9236e-01, -3.0554e-01,  1.3256e-01,\n",
      "         9.1171e-01, -5.0787e-03,  4.0024e-01,  2.1601e-01,  4.0122e-01,\n",
      "        -8.8142e-02, -4.0481e-01, -2.3397e-02, -5.7323e-01, -2.3390e-01,\n",
      "        -5.9563e-01,  8.1733e-02, -8.6961e-01, -7.4384e-01, -1.1227e+00,\n",
      "         3.8584e-01,  2.6719e-01,  5.4835e-02, -2.9910e-01, -1.2055e+00,\n",
      "         2.8516e-01, -7.1219e-01, -5.7855e-01, -8.6606e-01,  8.1144e-01,\n",
      "         5.2205e-01,  5.9391e-01,  9.3854e-02, -1.8957e-03,  1.0137e+00,\n",
      "        -7.6079e-02, -4.0642e-01, -5.7181e-01, -3.2577e-01, -9.6317e-02,\n",
      "        -6.2392e-01, -3.8673e-01,  7.2623e-01,  5.0560e-01, -4.6037e-01,\n",
      "        -4.4740e-01, -7.1534e-01, -3.2061e-01, -3.3487e-01,  8.2305e-02,\n",
      "        -6.3494e-01, -3.1084e-01,  1.3811e+00,  2.6921e-01, -7.5150e-02,\n",
      "        -5.4303e-01,  8.8273e-02,  7.7213e-01, -4.3415e-01,  2.3374e-01,\n",
      "         8.2018e-01, -2.6566e-01, -1.0646e-01,  4.0276e-01, -7.0709e-01,\n",
      "         6.3590e-01,  1.3676e-01, -1.4845e-01, -1.0402e-01, -2.7992e-03,\n",
      "        -5.1749e-01,  3.6472e-01, -2.9208e-01,  2.2934e-01,  4.4388e-01,\n",
      "         8.9121e-01,  8.1319e-02, -2.8738e-01,  8.8355e-01, -1.3859e-01,\n",
      "         7.8850e-01, -1.3013e+00, -8.1996e-02,  1.6296e+00, -5.6413e-01,\n",
      "        -6.1389e-01,  3.7356e-01, -3.7321e-01,  4.7072e-01, -4.9538e-02,\n",
      "        -9.2221e-02,  3.7396e-01, -2.1872e-01,  4.0329e-01, -8.7484e-02,\n",
      "        -6.3766e-01, -4.3391e-01,  4.8349e-01,  8.9592e-01, -1.2383e+00,\n",
      "         6.0928e-02,  2.1481e-01,  6.4121e-01,  3.9291e-01, -8.0424e-01,\n",
      "        -6.5835e-01,  1.6456e-01,  6.0860e-03, -9.6840e-01,  2.5058e-02,\n",
      "        -7.6169e-01, -1.5141e-01, -6.0832e-01, -1.7879e-01,  5.9064e-02,\n",
      "        -6.1179e-01, -8.2370e-02, -1.9386e-01, -3.2101e-01,  5.7347e-02,\n",
      "         6.4654e-01, -3.2480e-01,  2.1711e-03,  4.7056e-01, -1.2011e-01,\n",
      "        -1.0861e+00, -1.0528e+00, -3.8107e-01,  1.2867e+00,  2.1181e-01,\n",
      "         5.7008e-01,  3.3730e-01, -5.1821e-01, -6.8256e-01, -3.3038e-01,\n",
      "         7.6031e-01, -1.6233e-01,  7.7163e-01, -3.3090e-01, -1.9149e-01,\n",
      "        -4.2881e-01,  9.8354e-01, -3.9032e-01, -2.4433e-01,  7.2785e-02,\n",
      "        -7.0202e-01, -5.3465e-02,  7.6675e-01,  4.1107e-01, -1.0966e+00,\n",
      "        -1.9157e-01,  3.2273e-01, -7.0140e-01,  5.4448e-01, -6.2632e-01,\n",
      "        -1.3973e+00,  1.1299e-01,  3.9770e-02, -8.3503e-03,  2.0344e-02,\n",
      "         7.2789e-01,  5.8245e-01,  7.3528e-02, -1.8227e-01,  4.0775e-01,\n",
      "         5.5060e-01, -3.0913e-02,  7.2169e-02, -5.1938e-01,  2.9060e-01,\n",
      "         3.7672e-01,  6.8462e-01, -3.0060e-01,  2.1816e-01,  1.9877e-01,\n",
      "        -5.4113e-01,  6.0346e-02,  2.8691e-02,  3.1190e-01, -1.7686e-01,\n",
      "         2.7599e-01,  5.7627e-01, -3.3121e-01, -1.0209e+00, -4.1015e-01,\n",
      "        -3.0796e-01,  9.7871e-01, -1.1506e+00, -6.3323e-02,  1.8209e-01,\n",
      "        -6.8356e-01,  2.9173e-01, -8.8103e-01,  1.7125e-01,  8.8410e-01,\n",
      "        -4.4810e-02,  4.4256e-02,  8.6678e-01, -5.1839e-01,  2.2129e-01,\n",
      "         3.3742e-01,  5.2742e-01, -1.1524e+00,  6.0437e-01, -2.5545e-01,\n",
      "        -9.4205e-02, -5.4673e-01, -5.9871e-01,  9.7834e-03, -9.6911e-01,\n",
      "         7.2543e-01, -1.6946e+00,  3.6883e-01, -3.3974e-01, -1.3887e-01,\n",
      "        -1.1442e-01,  4.9734e-02, -2.4533e-01,  5.0610e-01, -5.5084e-01,\n",
      "        -5.0936e-01,  6.1843e-02, -1.4084e-01, -3.1115e-01, -1.0729e+00,\n",
      "         3.2674e-01,  1.4235e+00, -7.7231e-01,  4.1018e-01,  1.9507e-01,\n",
      "        -4.6893e-01, -5.5319e-01,  3.3421e-01, -5.3657e-01,  2.4775e-01,\n",
      "         6.1343e-01,  4.8321e-01,  8.5340e-01,  1.1323e+00, -8.9227e-02,\n",
      "        -1.9147e-01, -3.0158e-01,  9.6424e-01, -1.9880e-01, -1.7900e-01,\n",
      "        -9.5282e-01,  3.4334e-01,  1.2902e-01, -3.9887e-01, -4.7055e-02,\n",
      "        -2.2895e-01, -7.6545e-01, -6.1717e-01,  4.3975e-01, -4.0279e-01,\n",
      "         9.7352e-01,  1.1863e-01,  4.4854e-02, -6.3212e-01, -3.4033e-01,\n",
      "         2.3669e-01,  2.4668e-01,  6.4914e-01, -5.9673e-01,  3.2871e-01,\n",
      "        -2.0723e-02, -3.2670e-01, -4.1385e-01,  8.0052e-02, -6.4307e-02,\n",
      "         3.8216e-01, -9.0179e-01,  4.3333e-01,  5.3630e-01,  3.3011e-02,\n",
      "         1.0749e+00, -4.9539e-01, -1.4668e-01,  9.4852e-01, -3.1293e-01,\n",
      "         2.9481e-01,  3.8085e-02, -2.3884e-01,  2.2874e-01,  5.4349e-01,\n",
      "         5.8491e-01, -4.8349e-01,  1.0006e-01,  9.1109e-01, -1.2479e-02,\n",
      "         3.4273e-01, -4.6182e-01, -1.3918e+00, -3.3790e-01, -7.7678e-01,\n",
      "        -8.5857e-01, -3.2042e-01,  1.3526e+00, -5.8904e-01,  4.8699e-02,\n",
      "        -1.2401e-01,  6.9790e-01, -1.2249e+00,  1.2834e+00, -3.8426e-01,\n",
      "        -1.0320e+00, -8.6657e-02,  4.4117e-01, -1.0871e-01,  2.9934e-01,\n",
      "         2.8859e-01, -2.3440e-01,  6.4807e-01,  9.1801e-01, -9.9725e-01,\n",
      "        -2.0566e-01, -3.3378e-01, -1.8684e-02,  6.1473e-01,  3.9013e-01,\n",
      "        -5.6506e-02, -8.3622e-01, -6.2906e-01,  1.9012e-01,  1.0653e+00,\n",
      "        -4.9079e-01,  3.0421e-01, -1.3705e-01,  1.0261e-01,  5.7137e-01,\n",
      "         7.1051e-02, -5.3358e-01, -6.2749e-02, -4.1352e-01, -6.9166e-02,\n",
      "        -3.1240e-01,  2.0886e-01,  2.8488e-01, -8.0277e-01, -7.4939e-01,\n",
      "         1.3700e-02, -4.3710e-02, -3.5252e-01, -4.0568e-01,  1.0945e-01,\n",
      "         1.8303e-01, -2.5840e-02, -6.8093e-01,  5.5531e-01, -1.5088e-01,\n",
      "         1.5746e-01, -4.8650e-01, -4.5205e-02,  1.0330e-01, -3.4812e-01,\n",
      "        -6.0842e-02, -4.7228e-01,  6.3146e-01, -6.5449e-01, -3.8944e-01,\n",
      "        -7.1446e-02, -4.3674e-02,  3.4975e-01,  6.8410e-01,  4.8293e-02,\n",
      "         2.9303e-01,  5.5541e-02,  3.8766e-01,  2.1648e-01,  8.8105e-02,\n",
      "        -2.2174e-01,  7.9019e-03,  3.5951e-01], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puedo elegir cuales capas del transformer encoder voy a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([1536])\n",
      "torch.Size([9984])\n"
     ]
    }
   ],
   "source": [
    "embeddings = TransformerWordEmbeddings('bert-base-uncased', layers='-1', layer_mean=False)\n",
    "embeddings.embed(sentence)\n",
    "print(sentence[0].embedding.size())\n",
    "\n",
    "sentence.clear_embeddings()\n",
    "\n",
    "embeddings = TransformerWordEmbeddings('bert-base-uncased', layers='-1,-2', layer_mean=False)\n",
    "embeddings.embed(sentence)\n",
    "print(sentence[0].embedding.size())\n",
    "\n",
    "sentence.clear_embeddings()\n",
    "\n",
    "\n",
    "embeddings = TransformerWordEmbeddings('bert-base-uncased', layers='all', layer_mean=False)\n",
    "embeddings.embed(sentence)\n",
    "print(sentence[0].embedding.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voy a leer un dataset para NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:41:53,432 Reading data from /home/marcelo/.flair/datasets/conll_03_spanish\n",
      "2024-04-17 12:41:53,433 Train: /home/marcelo/.flair/datasets/conll_03_spanish/esp.train\n",
      "2024-04-17 12:41:53,433 Dev: /home/marcelo/.flair/datasets/conll_03_spanish/esp.testa\n",
      "2024-04-17 12:41:53,434 Test: /home/marcelo/.flair/datasets/conll_03_spanish/esp.testb\n",
      "Corpus: 8323 train + 1915 dev + 1517 test sentences\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.data import Sentence\n",
    "import flair.datasets\n",
    "\n",
    "corpus = flair.datasets.CONLL_03_SPANISH()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voy a usar la primera capa de ROBERTA como embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:41:57,649 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8323it [00:00, 45280.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:41:57,859 Dictionary created for label 'ner' with 4 values: ORG (seen 7390 times), LOC (seen 4914 times), PER (seen 4321 times), MISC (seen 2173 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary with 4 tags: ORG, LOC, PER, MISC\n"
     ]
    }
   ],
   "source": [
    "label_dict = corpus.make_label_dictionary(label_type='ner', add_unk=False)\n",
    "\n",
    "#label_dict = corpus.make_tag_dictionary(tag_type='ner')\n",
    "print(label_dict)\n",
    "\n",
    "embeddings = TransformerWordEmbeddings(model='roberta-base',\n",
    "                                       layers=\"-1\",\n",
    "                                       layer_mean=False,\n",
    "                                       subtoken_pooling=\"first\",\n",
    "                                       fine_tune=True,\n",
    "                                       use_context=True,\n",
    "                                       model_max_length=512,\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entreno usando los embeddings de ROBERTA con el sequence tagger de flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:42:06,279 SequenceTagger predicts: Dictionary with 17 tags: O, S-ORG, B-ORG, E-ORG, I-ORG, S-LOC, B-LOC, E-LOC, I-LOC, S-PER, B-PER, E-PER, I-PER, S-MISC, B-MISC, E-MISC, I-MISC\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "tagger = SequenceTagger(\n",
    "    hidden_size=256,\n",
    "    embeddings=embeddings,\n",
    "    tag_dictionary=label_dict,\n",
    "    tag_format=\"BIOES\",\n",
    "    tag_type='ner',\n",
    "    use_crf=False,\n",
    "    use_rnn=False,\n",
    "    reproject_embeddings=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = ModelTrainer(tagger, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el contexto de NER (Reconocimiento de Entidades Nombradas) utilizando el formato CoNLL (Conference on Natural Language Learning), las anotaciones \"S\", \"B\", \"E\" e \"I\" se refieren a cómo las palabras dentro de una entidad nombrada son etiquetadas. Estas etiquetas son parte del esquema de etiquetado BIOES, también conocido como BIESO, que se usa para marcar los límites y la posición de las palabras dentro de las entidades. Aquí está el significado de cada etiqueta:\n",
    "\n",
    "- **B (Beginning)**: Marca el inicio de una entidad. Se utiliza para la primera palabra de una entidad.\n",
    "\n",
    "- **I (Inside)**: Se utiliza para una palabra que está dentro de una entidad, pero no es ni el inicio ni el final. Esta etiqueta se utiliza para las palabras que siguen a la etiqueta B en una entidad de múltiples palabras.\n",
    "\n",
    "- **E (End)**: Indica el final de una entidad. Se utiliza para la última palabra de una entidad de múltiples palabras.\n",
    "\n",
    "- **S (Single)**: Se utiliza cuando una entidad está compuesta por una sola palabra. Esta etiqueta indica que la palabra es simultáneamente el inicio y el final de la entidad.\n",
    "\n",
    "Este esquema de etiquetado ayuda a mejorar la precisión en la identificación de entidades, especialmente en casos donde las entidades están compuestas de múltiples palabras, permitiendo a los modelos de aprendizaje automático reconocer mejor los límites de las entidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y hacemos el fine-tuning del NER tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathered 9510 of total 50265\n",
      "Reducing vocab size by 81.0803%\n",
      "Reducing model size by 25.1115%\n",
      "Reducing training parameter count by 25.1115%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:42:19,580 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 12:42:19,582 Model: \"SequenceTagger(\n",
      "  (embeddings): TransformerWordEmbeddings(\n",
      "    (model): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(50266, 768)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (linear): Linear(in_features=768, out_features=17, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:42:19,584 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 12:42:19,586 Corpus: \"Corpus: 8323 train + 1915 dev + 1517 test sentences\"\n",
      "2024-04-17 12:42:19,587 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 12:42:19,588 Parameters:\n",
      "2024-04-17 12:42:19,589  - learning_rate: \"0.000020\"\n",
      "2024-04-17 12:42:19,590  - mini_batch_size: \"4\"\n",
      "2024-04-17 12:42:19,591  - patience: \"3\"\n",
      "2024-04-17 12:42:19,592  - anneal_factor: \"0.5\"\n",
      "2024-04-17 12:42:19,593  - max_epochs: \"8\"\n",
      "2024-04-17 12:42:19,595  - shuffle: \"False\"\n",
      "2024-04-17 12:42:19,596  - train_with_dev: \"False\"\n",
      "2024-04-17 12:42:19,596  - batch_growth_annealing: \"False\"\n",
      "2024-04-17 12:42:19,597 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 12:42:19,598 Model training base path: \"resources/taggers/ner-roberta-base\"\n",
      "2024-04-17 12:42:19,599 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 12:42:19,600 Device: cuda:0\n",
      "2024-04-17 12:42:19,601 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 12:42:19,602 Embeddings storage mode: none\n",
      "2024-04-17 12:42:19,604 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 12:42:41,060 epoch 1 - iter 208/2081 - loss 2.15701464 - time (sec): 21.46 - samples/sec: 1245.92 - lr: 0.000002\n",
      "2024-04-17 12:43:02,690 epoch 1 - iter 416/2081 - loss 1.27982237 - time (sec): 43.09 - samples/sec: 1278.80 - lr: 0.000005\n",
      "2024-04-17 12:43:24,120 epoch 1 - iter 624/2081 - loss 1.01624682 - time (sec): 64.52 - samples/sec: 1174.26 - lr: 0.000007\n",
      "2024-04-17 12:43:45,962 epoch 1 - iter 832/2081 - loss 0.82654770 - time (sec): 86.36 - samples/sec: 1177.32 - lr: 0.000010\n",
      "2024-04-17 12:44:07,845 epoch 1 - iter 1040/2081 - loss 0.68878216 - time (sec): 108.24 - samples/sec: 1196.40 - lr: 0.000012\n",
      "2024-04-17 12:44:29,673 epoch 1 - iter 1248/2081 - loss 0.60059402 - time (sec): 130.07 - samples/sec: 1214.62 - lr: 0.000015\n",
      "2024-04-17 12:44:51,468 epoch 1 - iter 1456/2081 - loss 0.53339588 - time (sec): 151.86 - samples/sec: 1227.85 - lr: 0.000017\n",
      "2024-04-17 12:45:12,835 epoch 1 - iter 1664/2081 - loss 0.48844954 - time (sec): 173.23 - samples/sec: 1224.46 - lr: 0.000020\n",
      "2024-04-17 12:45:34,469 epoch 1 - iter 1872/2081 - loss 0.44613543 - time (sec): 194.86 - samples/sec: 1238.42 - lr: 0.000020\n",
      "2024-04-17 12:45:55,747 epoch 1 - iter 2080/2081 - loss 0.41978514 - time (sec): 216.14 - samples/sec: 1224.15 - lr: 0.000019\n",
      "2024-04-17 12:45:55,822 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 12:45:55,823 EPOCH 1 done: loss 0.4196 - lr 0.000019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:14<00:00, 32.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:46:10,466 Evaluating as a multi-label problem: False\n",
      "2024-04-17 12:46:10,510 DEV : loss 0.12500788271427155 - f1-score (micro avg)  0.8026\n",
      "2024-04-17 12:46:10,538 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:46:31,701 epoch 2 - iter 208/2081 - loss 0.13478385 - time (sec): 21.16 - samples/sec: 1263.22 - lr: 0.000019\n",
      "2024-04-17 12:46:53,871 epoch 2 - iter 416/2081 - loss 0.12907947 - time (sec): 43.33 - samples/sec: 1271.54 - lr: 0.000019\n",
      "2024-04-17 12:47:15,513 epoch 2 - iter 624/2081 - loss 0.12617677 - time (sec): 64.97 - samples/sec: 1165.98 - lr: 0.000019\n",
      "2024-04-17 12:47:37,086 epoch 2 - iter 832/2081 - loss 0.12365354 - time (sec): 86.55 - samples/sec: 1174.74 - lr: 0.000018\n",
      "2024-04-17 12:47:59,238 epoch 2 - iter 1040/2081 - loss 0.11800991 - time (sec): 108.70 - samples/sec: 1191.36 - lr: 0.000018\n",
      "2024-04-17 12:48:22,078 epoch 2 - iter 1248/2081 - loss 0.12495660 - time (sec): 131.54 - samples/sec: 1201.04 - lr: 0.000018\n",
      "2024-04-17 12:48:43,892 epoch 2 - iter 1456/2081 - loss 0.12233771 - time (sec): 153.35 - samples/sec: 1215.93 - lr: 0.000018\n",
      "2024-04-17 12:49:05,307 epoch 2 - iter 1664/2081 - loss 0.11952784 - time (sec): 174.77 - samples/sec: 1213.69 - lr: 0.000017\n",
      "2024-04-17 12:49:27,648 epoch 2 - iter 1872/2081 - loss 0.11723403 - time (sec): 197.11 - samples/sec: 1224.32 - lr: 0.000017\n",
      "2024-04-17 12:49:49,660 epoch 2 - iter 2080/2081 - loss 0.11694628 - time (sec): 219.12 - samples/sec: 1207.51 - lr: 0.000017\n",
      "2024-04-17 12:49:49,791 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 12:49:49,792 EPOCH 2 done: loss 0.1169 - lr 0.000017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:18<00:00, 25.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:50:08,471 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:50:08,538 DEV : loss 0.12301015108823776 - f1-score (micro avg)  0.8465\n",
      "2024-04-17 12:50:08,576 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 12:50:30,314 epoch 3 - iter 208/2081 - loss 0.10279048 - time (sec): 21.74 - samples/sec: 1229.80 - lr: 0.000016\n",
      "2024-04-17 12:50:52,195 epoch 3 - iter 416/2081 - loss 0.09486375 - time (sec): 43.62 - samples/sec: 1263.18 - lr: 0.000016\n",
      "2024-04-17 12:51:13,128 epoch 3 - iter 624/2081 - loss 0.09347314 - time (sec): 64.55 - samples/sec: 1173.60 - lr: 0.000016\n",
      "2024-04-17 12:51:34,578 epoch 3 - iter 832/2081 - loss 0.09354814 - time (sec): 86.00 - samples/sec: 1182.19 - lr: 0.000016\n",
      "2024-04-17 12:51:55,958 epoch 3 - iter 1040/2081 - loss 0.09083844 - time (sec): 107.38 - samples/sec: 1205.97 - lr: 0.000015\n",
      "2024-04-17 12:52:18,300 epoch 3 - iter 1248/2081 - loss 0.09994466 - time (sec): 129.72 - samples/sec: 1217.85 - lr: 0.000015\n",
      "2024-04-17 12:52:40,366 epoch 3 - iter 1456/2081 - loss 0.09729757 - time (sec): 151.79 - samples/sec: 1228.45 - lr: 0.000015\n",
      "2024-04-17 12:53:01,941 epoch 3 - iter 1664/2081 - loss 0.09539346 - time (sec): 173.36 - samples/sec: 1223.51 - lr: 0.000014\n",
      "2024-04-17 12:53:23,268 epoch 3 - iter 1872/2081 - loss 0.09429407 - time (sec): 194.69 - samples/sec: 1239.52 - lr: 0.000014\n",
      "2024-04-17 12:53:44,599 epoch 3 - iter 2080/2081 - loss 0.09454047 - time (sec): 216.02 - samples/sec: 1224.83 - lr: 0.000014\n",
      "2024-04-17 12:53:44,706 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 12:53:44,708 EPOCH 3 done: loss 0.0945 - lr 0.000014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:17<00:00, 27.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:54:02,411 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:54:02,445 DEV : loss 0.12724953889846802 - f1-score (micro avg)  0.8688\n",
      "2024-04-17 12:54:02,471 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 12:54:23,659 epoch 4 - iter 208/2081 - loss 0.08630366 - time (sec): 21.19 - samples/sec: 1261.71 - lr: 0.000014\n",
      "2024-04-17 12:54:44,473 epoch 4 - iter 416/2081 - loss 0.07990218 - time (sec): 42.00 - samples/sec: 1311.83 - lr: 0.000013\n",
      "2024-04-17 12:55:06,372 epoch 4 - iter 624/2081 - loss 0.07636733 - time (sec): 63.90 - samples/sec: 1185.57 - lr: 0.000013\n",
      "2024-04-17 12:55:27,287 epoch 4 - iter 832/2081 - loss 0.07515861 - time (sec): 84.81 - samples/sec: 1198.73 - lr: 0.000013\n",
      "2024-04-17 12:55:48,300 epoch 4 - iter 1040/2081 - loss 0.07214483 - time (sec): 105.83 - samples/sec: 1223.67 - lr: 0.000013\n",
      "2024-04-17 12:56:09,712 epoch 4 - iter 1248/2081 - loss 0.07503518 - time (sec): 127.24 - samples/sec: 1241.61 - lr: 0.000012\n",
      "2024-04-17 12:56:30,702 epoch 4 - iter 1456/2081 - loss 0.07462983 - time (sec): 148.23 - samples/sec: 1257.95 - lr: 0.000012\n",
      "2024-04-17 12:56:51,754 epoch 4 - iter 1664/2081 - loss 0.07482917 - time (sec): 169.28 - samples/sec: 1253.01 - lr: 0.000012\n",
      "2024-04-17 12:57:12,967 epoch 4 - iter 1872/2081 - loss 0.07433563 - time (sec): 190.50 - samples/sec: 1266.82 - lr: 0.000011\n",
      "2024-04-17 12:57:33,867 epoch 4 - iter 2080/2081 - loss 0.07459842 - time (sec): 211.40 - samples/sec: 1251.64 - lr: 0.000011\n",
      "2024-04-17 12:57:33,967 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 12:57:33,968 EPOCH 4 done: loss 0.0746 - lr 0.000011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:16<00:00, 28.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:57:50,792 Evaluating as a multi-label problem: False\n",
      "2024-04-17 12:57:50,824 DEV : loss 0.13338154554367065 - f1-score (micro avg)  0.8767\n",
      "2024-04-17 12:57:50,850 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 12:58:12,367 epoch 5 - iter 208/2081 - loss 0.06134422 - time (sec): 21.52 - samples/sec: 1242.44 - lr: 0.000011\n",
      "2024-04-17 12:58:33,680 epoch 5 - iter 416/2081 - loss 0.05921549 - time (sec): 42.83 - samples/sec: 1286.48 - lr: 0.000011\n",
      "2024-04-17 12:58:54,725 epoch 5 - iter 624/2081 - loss 0.05679157 - time (sec): 63.87 - samples/sec: 1186.06 - lr: 0.000010\n",
      "2024-04-17 12:59:15,833 epoch 5 - iter 832/2081 - loss 0.05727826 - time (sec): 84.98 - samples/sec: 1196.38 - lr: 0.000010\n",
      "2024-04-17 12:59:39,237 epoch 5 - iter 1040/2081 - loss 0.05701575 - time (sec): 108.39 - samples/sec: 1194.80 - lr: 0.000010\n",
      "2024-04-17 13:00:03,355 epoch 5 - iter 1248/2081 - loss 0.05953548 - time (sec): 132.50 - samples/sec: 1192.29 - lr: 0.000009\n",
      "2024-04-17 13:00:25,087 epoch 5 - iter 1456/2081 - loss 0.05792990 - time (sec): 154.24 - samples/sec: 1208.97 - lr: 0.000009\n",
      "2024-04-17 13:00:46,607 epoch 5 - iter 1664/2081 - loss 0.05783981 - time (sec): 175.76 - samples/sec: 1206.86 - lr: 0.000009\n",
      "2024-04-17 13:01:08,327 epoch 5 - iter 1872/2081 - loss 0.05727838 - time (sec): 197.48 - samples/sec: 1222.04 - lr: 0.000009\n",
      "2024-04-17 13:01:29,891 epoch 5 - iter 2080/2081 - loss 0.05792698 - time (sec): 219.04 - samples/sec: 1207.96 - lr: 0.000008\n",
      "2024-04-17 13:01:29,993 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:01:29,995 EPOCH 5 done: loss 0.0579 - lr 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:18<00:00, 25.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:01:48,860 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:01:48,898 DEV : loss 0.17233379185199738 - f1-score (micro avg)  0.8522\n",
      "2024-04-17 13:01:48,929 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:02:10,332 epoch 6 - iter 208/2081 - loss 0.05027897 - time (sec): 21.40 - samples/sec: 1249.03 - lr: 0.000008\n",
      "2024-04-17 13:02:32,248 epoch 6 - iter 416/2081 - loss 0.04677513 - time (sec): 43.32 - samples/sec: 1271.94 - lr: 0.000008\n",
      "2024-04-17 13:02:54,646 epoch 6 - iter 624/2081 - loss 0.04507152 - time (sec): 65.72 - samples/sec: 1152.81 - lr: 0.000008\n",
      "2024-04-17 13:03:17,097 epoch 6 - iter 832/2081 - loss 0.04536561 - time (sec): 88.17 - samples/sec: 1153.15 - lr: 0.000007\n",
      "2024-04-17 13:03:38,988 epoch 6 - iter 1040/2081 - loss 0.04443522 - time (sec): 110.06 - samples/sec: 1176.64 - lr: 0.000007\n",
      "2024-04-17 13:04:01,482 epoch 6 - iter 1248/2081 - loss 0.04728063 - time (sec): 132.55 - samples/sec: 1191.86 - lr: 0.000007\n",
      "2024-04-17 13:04:24,364 epoch 6 - iter 1456/2081 - loss 0.04656553 - time (sec): 155.43 - samples/sec: 1199.64 - lr: 0.000006\n",
      "2024-04-17 13:04:46,589 epoch 6 - iter 1664/2081 - loss 0.04696082 - time (sec): 177.66 - samples/sec: 1193.93 - lr: 0.000006\n",
      "2024-04-17 13:05:09,117 epoch 6 - iter 1872/2081 - loss 0.04630661 - time (sec): 200.19 - samples/sec: 1205.49 - lr: 0.000006\n",
      "2024-04-17 13:05:30,671 epoch 6 - iter 2080/2081 - loss 0.04668045 - time (sec): 221.74 - samples/sec: 1193.24 - lr: 0.000006\n",
      "2024-04-17 13:05:30,765 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:05:30,766 EPOCH 6 done: loss 0.0467 - lr 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:17<00:00, 26.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:05:48,713 Evaluating as a multi-label problem: False\n",
      "2024-04-17 13:05:48,752 DEV : loss 0.16247394680976868 - f1-score (micro avg)  0.8771\n",
      "2024-04-17 13:05:48,784 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:06:09,995 epoch 7 - iter 208/2081 - loss 0.03868834 - time (sec): 21.21 - samples/sec: 1260.34 - lr: 0.000005\n",
      "2024-04-17 13:06:31,488 epoch 7 - iter 416/2081 - loss 0.03868234 - time (sec): 42.70 - samples/sec: 1290.25 - lr: 0.000005\n",
      "2024-04-17 13:06:52,550 epoch 7 - iter 624/2081 - loss 0.03678776 - time (sec): 63.77 - samples/sec: 1188.08 - lr: 0.000005\n",
      "2024-04-17 13:07:13,552 epoch 7 - iter 832/2081 - loss 0.03638934 - time (sec): 84.77 - samples/sec: 1199.40 - lr: 0.000004\n",
      "2024-04-17 13:07:34,555 epoch 7 - iter 1040/2081 - loss 0.03491890 - time (sec): 105.77 - samples/sec: 1224.34 - lr: 0.000004\n",
      "2024-04-17 13:07:55,985 epoch 7 - iter 1248/2081 - loss 0.03650993 - time (sec): 127.20 - samples/sec: 1242.00 - lr: 0.000004\n",
      "2024-04-17 13:08:17,049 epoch 7 - iter 1456/2081 - loss 0.03618516 - time (sec): 148.26 - samples/sec: 1257.66 - lr: 0.000004\n",
      "2024-04-17 13:08:37,897 epoch 7 - iter 1664/2081 - loss 0.03662307 - time (sec): 169.11 - samples/sec: 1254.27 - lr: 0.000003\n",
      "2024-04-17 13:08:58,719 epoch 7 - iter 1872/2081 - loss 0.03659454 - time (sec): 189.93 - samples/sec: 1270.56 - lr: 0.000003\n",
      "2024-04-17 13:09:19,775 epoch 7 - iter 2080/2081 - loss 0.03755229 - time (sec): 210.99 - samples/sec: 1254.04 - lr: 0.000003\n",
      "2024-04-17 13:09:19,884 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:09:19,885 EPOCH 7 done: loss 0.0375 - lr 0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:18<00:00, 25.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:09:38,740 Evaluating as a multi-label problem: False\n",
      "2024-04-17 13:09:38,777 DEV : loss 0.1679566204547882 - f1-score (micro avg)  0.8757\n",
      "2024-04-17 13:09:38,808 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:10:00,354 epoch 8 - iter 208/2081 - loss 0.04130037 - time (sec): 21.54 - samples/sec: 1240.76 - lr: 0.000003\n",
      "2024-04-17 13:10:21,935 epoch 8 - iter 416/2081 - loss 0.03790122 - time (sec): 43.13 - samples/sec: 1277.62 - lr: 0.000002\n",
      "2024-04-17 13:10:42,632 epoch 8 - iter 624/2081 - loss 0.03290789 - time (sec): 63.82 - samples/sec: 1187.00 - lr: 0.000002\n",
      "2024-04-17 13:11:03,818 epoch 8 - iter 832/2081 - loss 0.03338194 - time (sec): 85.01 - samples/sec: 1195.99 - lr: 0.000002\n",
      "2024-04-17 13:11:24,773 epoch 8 - iter 1040/2081 - loss 0.03045543 - time (sec): 105.96 - samples/sec: 1222.11 - lr: 0.000001\n",
      "2024-04-17 13:11:46,199 epoch 8 - iter 1248/2081 - loss 0.03398755 - time (sec): 127.39 - samples/sec: 1240.16 - lr: 0.000001\n",
      "2024-04-17 13:12:07,560 epoch 8 - iter 1456/2081 - loss 0.03365243 - time (sec): 148.75 - samples/sec: 1253.55 - lr: 0.000001\n",
      "2024-04-17 13:12:28,864 epoch 8 - iter 1664/2081 - loss 0.03410493 - time (sec): 170.05 - samples/sec: 1247.32 - lr: 0.000001\n",
      "2024-04-17 13:12:49,756 epoch 8 - iter 1872/2081 - loss 0.03439435 - time (sec): 190.95 - samples/sec: 1263.83 - lr: 0.000000\n",
      "2024-04-17 13:13:10,689 epoch 8 - iter 2080/2081 - loss 0.03576095 - time (sec): 211.88 - samples/sec: 1248.78 - lr: 0.000000\n",
      "2024-04-17 13:13:10,808 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:13:10,810 EPOCH 8 done: loss 0.0357 - lr 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:17<00:00, 27.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:13:28,260 Evaluating as a multi-label problem: False\n",
      "2024-04-17 13:13:28,295 DEV : loss 0.1680973768234253 - f1-score (micro avg)  0.879\n",
      "2024-04-17 13:13:29,101 ----------------------------------------------------------------------------------------------------\n",
      "2024-04-17 13:13:29,103 Testing using last state of model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [00:13<00:00, 28.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:13:42,413 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:13:42,467 0.8719\t0.8817\t0.8768\t0.8388\n",
      "2024-04-17 13:13:42,468 \n",
      "Results:\n",
      "- F-score (micro) 0.8768\n",
      "- F-score (macro) 0.8611\n",
      "- Accuracy 0.8388\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ORG     0.8549    0.8964    0.8752      1400\n",
      "         LOC     0.8731    0.8506    0.8617      1084\n",
      "         PER     0.9633    0.9633    0.9633       735\n",
      "        MISC     0.7441    0.7441    0.7441       340\n",
      "\n",
      "   micro avg     0.8719    0.8817    0.8768      3559\n",
      "   macro avg     0.8588    0.8636    0.8611      3559\n",
      "weighted avg     0.8722    0.8817    0.8767      3559\n",
      "\n",
      "2024-04-17 13:13:42,469 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.8767812238055323,\n",
       " 'dev_score_history': [0.8026315789473684,\n",
       "  0.8465116279069766,\n",
       "  0.868781378366043,\n",
       "  0.8766651355075793,\n",
       "  0.8522139160437033,\n",
       "  0.8770840519719444,\n",
       "  0.8757055638751295,\n",
       "  0.878979427651994],\n",
       " 'train_loss_history': [0.4196061343392371,\n",
       "  0.11689172179770276,\n",
       "  0.09449662407041759,\n",
       "  0.07456361308340184,\n",
       "  0.057899872601580235,\n",
       "  0.04665862448102976,\n",
       "  0.03753470630178461,\n",
       "  0.03574421755466544],\n",
       " 'dev_loss_history': [0.12500788271427155,\n",
       "  0.12301015108823776,\n",
       "  0.12724953889846802,\n",
       "  0.13338154554367065,\n",
       "  0.17233379185199738,\n",
       "  0.16247394680976868,\n",
       "  0.1679566204547882,\n",
       "  0.1680973768234253]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fine_tune(\n",
    "    base_path=\"resources/taggers/ner-roberta-base\",\n",
    "    train_with_dev=False,\n",
    "    max_epochs=8,\n",
    "    learning_rate=2.0e-5,\n",
    "    mini_batch_size=4,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y vemos como funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:39:43,255 SequenceTagger predicts: Dictionary with 17 tags: O, S-ORG, B-ORG, E-ORG, I-ORG, S-LOC, B-LOC, E-LOC, I-LOC, S-PER, B-PER, E-PER, I-PER, S-MISC, B-MISC, E-MISC, I-MISC\n",
      "Sentence[5]: \"George Washington fue a Washington\" → [\"George Washington\"/LOC, \"Washington\"/LOC]\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo preentrenado\n",
    "model = SequenceTagger.load('resources/taggers/ner-roberta-base/final-model.pt')\n",
    "\n",
    "\n",
    "# create example sentence\n",
    "from flair.data import Sentence\n",
    "sentence = Sentence(\"George Washington fue a Washington\")\n",
    "\n",
    "# predict tags and print\n",
    "model.predict(sentence)\n",
    "\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
