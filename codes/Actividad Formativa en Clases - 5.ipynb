{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIC-3670 NLP UC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Versiones de librerías, python 3.8.10\n",
    "\n",
    "- numpy 1.20.3\n",
    "- nltk 3.7\n",
    "- gensim 4.1.2\n",
    "- keras 2.9.0\n",
    "- tensorflow 2.9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad en clase\n",
    "\n",
    "Vamos a entrenar un POS tagger usndo el corpus **treenbank** de NLTK. Para esto haga lo siguiente:\n",
    "\n",
    "- Importe las tagged sents de treebank.\n",
    "- Preprocese las tagged sents como se mostró en clases.\n",
    "- Haga la partición de entrenamiento y test con **test_size = 0.2**.\n",
    "- Construye los sets de símbolos para tags y palabras en la partición de entrenamiento.\n",
    "- Pase a secuencias de entreos las tagged sents.\n",
    "- Construya las pad sequences en keras.\n",
    "- Defina la clase ignore accuracy.\n",
    "- Define el modelo usando Sequential() de keras. Su modelo va a tener **dos capas bidireccionales, ambas de dim = 256**. Use LCE y Adam a 0.001. \n",
    "- Compile el modelo.\n",
    "- Pase los símbolos de salida a variable categórica. \n",
    "- Entrene a **batch_size=64 y 20 epochs**. Si tarda mucho, disminuya los epochs. Use un **validation split de 10%**.\n",
    "- Evalúe el POS tagger sobre la partición de testing en base a accuracy.\n",
    "- ¿Qué hace la función ignore accuracy?\n",
    "- Interprete los resultados.\n",
    "- Cuanto termine, me avisa para entregarle una **L (logrado)**.\n",
    "- Recuerde que las L otorgan un bono en la nota final de la asignatura.\n",
    "\n",
    "\n",
    "***Tiene hasta el final de la clase.***\n",
    "\n",
    "Vea la descripción del dataset en: https://www.kaggle.com/datasets/crawford/20-newsgroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "Tagged sentences:  3914\n",
      "Tagged words: 100676\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    " \n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    " \n",
    "print(tagged_sentences[0])\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.' 'Vinken' 'is' 'chairman' 'of' 'Elsevier' 'N.V.' ',' 'the' 'Dutch'\n",
      " 'publishing' 'group' '.']\n",
      "['NNP' 'NNP' 'VBZ' 'NN' 'IN' 'NNP' 'NNP' ',' 'DT' 'NNP' 'VBG' 'NN' '.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "sentences, sentence_tags =[], [] \n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))\n",
    " \n",
    "print(sentences[1])\n",
    "print(sentence_tags[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "(train_sentences, test_sentences,  train_tags,  test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    " \n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    " \n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0   # The special value used to padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7678, 8768, 5131, 2447, 1425, 6817, 9197, 700, 2918, 2447, 727, 1655, 4511, 8860, 2929, 3550, 7897, 9905, 8212, 6428, 4020, 700, 3717, 1074, 5172, 9125, 9799, 2589, 8250, 5131, 2447, 9254, 9073, 7971, 451, 8768, 5131, 2447, 9180, 7807, 700, 2145, 6579, 3717, 2447, 7919, 5131, 4265, 5072, 1510, 4610, 8826, 343, 700, 3717, 7586, 4973, 5545, 4973, 8148, 9684, 2458, 8299, 3639, 4822, 6799, 2184, 9626, 6025, 6579, 3717, 6906, 7669, 5131, 7493, 4345, 7787, 700, 2918, 588, 293, 2447, 7724, 1544, 5131, 2447, 7772, 7367, 5444, 9938]\n",
      "[1017, 6319, 6565, 6976, 3717, 4546, 9036, 8846, 532, 9036, 2918, 2447, 3675, 4077, 7615, 5444]\n",
      "[38, 33, 19, 39, 15, 14, 45, 37, 19, 39, 14, 14, 19, 15, 42, 1, 37, 3, 4, 31, 33, 37, 29, 26, 15, 33, 19, 33, 42, 19, 39, 33, 33, 17, 5, 33, 19, 39, 14, 45, 37, 15, 15, 29, 39, 14, 19, 39, 33, 32, 26, 45, 37, 37, 29, 26, 41, 26, 41, 12, 26, 39, 33, 24, 37, 3, 45, 45, 37, 15, 29, 39, 33, 19, 33, 38, 45, 37, 19, 33, 19, 39, 15, 33, 19, 39, 4, 46, 34, 9]\n",
      "[33, 33, 14, 20, 29, 42, 33, 19, 42, 33, 19, 39, 42, 15, 14, 34]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    " \n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    " \n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    " \n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    " \n",
    "for s in test_tags:\n",
    "    test_tags_y.append([tag2index[t] for t in s])\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7678 8768 5131 2447 1425 6817 9197  700 2918 2447  727 1655 4511 8860\n",
      " 2929 3550 7897 9905 8212 6428 4020  700 3717 1074 5172 9125 9799 2589\n",
      " 8250 5131 2447 9254 9073 7971  451 8768 5131 2447 9180 7807  700 2145\n",
      " 6579 3717 2447 7919 5131 4265 5072 1510 4610 8826  343  700 3717 7586\n",
      " 4973 5545 4973 8148 9684 2458 8299 3639 4822 6799 2184 9626 6025 6579\n",
      " 3717 6906 7669 5131 7493 4345 7787  700 2918  588  293 2447 7724 1544\n",
      " 5131 2447 7772 7367 5444 9938    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "[1017 6319 6565 6976 3717 4546 9036 8846  532 9036 2918 2447 3675 4077\n",
      " 7615 5444    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "[38 33 19 39 15 14 45 37 19 39 14 14 19 15 42  1 37  3  4 31 33 37 29 26\n",
      " 15 33 19 33 42 19 39 33 33 17  5 33 19 39 14 45 37 15 15 29 39 14 19 39\n",
      " 33 32 26 45 37 37 29 26 41 26 41 12 26 39 33 24 37  3 45 45 37 15 29 39\n",
      " 33 19 33 38 45 37 19 33 19 39 15 33 19 39  4 46 34  9  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n",
      "[33 33 14 20 29 42 33 19 42 33 19 39 42 15 14 34  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 271, 128)          1296384   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 271, 512)         788480    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 271, 512)         1574912   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 271, 47)          24111     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 271, 47)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,683,887\n",
      "Trainable params: 3,683,887\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy', ignore_class_accuracy(0)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "print(cat_train_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45/45 [==============================] - 74s 2s/step - loss: 0.5829 - accuracy: 0.8895 - ignore_accuracy: 0.0802 - val_loss: 0.2938 - val_accuracy: 0.9170 - val_ignore_accuracy: 0.1317\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 67s 1s/step - loss: 0.2862 - accuracy: 0.9169 - ignore_accuracy: 0.1348 - val_loss: 0.2770 - val_accuracy: 0.9213 - val_ignore_accuracy: 0.1607\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.2759 - accuracy: 0.9205 - ignore_accuracy: 0.1628 - val_loss: 0.2723 - val_accuracy: 0.9215 - val_ignore_accuracy: 0.1635\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.2712 - accuracy: 0.9219 - ignore_accuracy: 0.1776 - val_loss: 0.2680 - val_accuracy: 0.9232 - val_ignore_accuracy: 0.1809\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.2647 - accuracy: 0.9238 - ignore_accuracy: 0.1957 - val_loss: 0.2630 - val_accuracy: 0.9239 - val_ignore_accuracy: 0.1895\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.2514 - accuracy: 0.9267 - ignore_accuracy: 0.2284 - val_loss: 0.3146 - val_accuracy: 0.9190 - val_ignore_accuracy: 0.1366\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.2328 - accuracy: 0.9321 - ignore_accuracy: 0.2881 - val_loss: 0.2052 - val_accuracy: 0.9398 - val_ignore_accuracy: 0.3590\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.1632 - accuracy: 0.9529 - ignore_accuracy: 0.5042 - val_loss: 0.1345 - val_accuracy: 0.9597 - val_ignore_accuracy: 0.5716\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.1009 - accuracy: 0.9716 - ignore_accuracy: 0.7070 - val_loss: 0.1006 - val_accuracy: 0.9714 - val_ignore_accuracy: 0.7006\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.0629 - accuracy: 0.9839 - ignore_accuracy: 0.8303 - val_loss: 0.0690 - val_accuracy: 0.9829 - val_ignore_accuracy: 0.8181\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.0416 - accuracy: 0.9899 - ignore_accuracy: 0.8958 - val_loss: 0.0554 - val_accuracy: 0.9868 - val_ignore_accuracy: 0.8592\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.0288 - accuracy: 0.9934 - ignore_accuracy: 0.9317 - val_loss: 0.0524 - val_accuracy: 0.9877 - val_ignore_accuracy: 0.8695\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.0226 - accuracy: 0.9950 - ignore_accuracy: 0.9481 - val_loss: 0.0470 - val_accuracy: 0.9895 - val_ignore_accuracy: 0.8879\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.0176 - accuracy: 0.9961 - ignore_accuracy: 0.9582 - val_loss: 0.0461 - val_accuracy: 0.9899 - val_ignore_accuracy: 0.8925\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.0154 - accuracy: 0.9966 - ignore_accuracy: 0.9642 - val_loss: 0.0444 - val_accuracy: 0.9904 - val_ignore_accuracy: 0.8975\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.0130 - accuracy: 0.9971 - ignore_accuracy: 0.9697 - val_loss: 0.0439 - val_accuracy: 0.9908 - val_ignore_accuracy: 0.9017\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.0109 - accuracy: 0.9976 - ignore_accuracy: 0.9754 - val_loss: 0.0429 - val_accuracy: 0.9913 - val_ignore_accuracy: 0.9072\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.0096 - accuracy: 0.9980 - ignore_accuracy: 0.9792 - val_loss: 0.0433 - val_accuracy: 0.9910 - val_ignore_accuracy: 0.9040\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.0087 - accuracy: 0.9982 - ignore_accuracy: 0.9812 - val_loss: 0.0428 - val_accuracy: 0.9913 - val_ignore_accuracy: 0.9076\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.0074 - accuracy: 0.9985 - ignore_accuracy: 0.9840 - val_loss: 0.0435 - val_accuracy: 0.9914 - val_ignore_accuracy: 0.9090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f73f0a03970>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=64, epochs=20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 9s 371ms/step - loss: 0.0472 - accuracy: 0.9911 - ignore_accuracy: 0.9056\n",
      "accuracy: 99.10788536071777\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
