{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIC-3670 NLP UC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Versiones de librerías, python 3.8.10\n",
    "\n",
    "- spacy 3.5.1\n",
    "- keras 2.9.0\n",
    "- tensorflow 2.9.1\n",
    "- pydantic 1.10.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a cargar un modelo de spacy para hacer el pipeline de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-23 11:13:41.293323: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-23 11:13:41.763978: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-23 11:13:41.764035: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-23 11:13:41.764042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-04-23 11:13:43.004864: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-04-23 11:13:43.004917: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-04-23 11:13:43.006388: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-04-23 11:13:43.006446: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-04-23 11:13:43.006483: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-04-23 11:13:43.006507: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/marcelo/.local/lib/python3.8/site-packages (from en-core-web-md==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.42.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.20.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.28.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.10.1)\n",
      "Requirement already satisfied: setuptools in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (69.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/marcelo/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/marcelo/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.6.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/marcelo/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2019.11.28)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/marcelo/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/marcelo/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/marcelo/.local/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
      "\u001b[33mDEPRECATION: orange3-imageanalytics 0.8.0 has a non-standard dependency specifier numpy>=1.16.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of orange3-imageanalytics or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: tensorflow-examples 907a3e7466aa72345c983b596cba58b255f2b1f7- has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of tensorflow-examples or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: gpg 1.13.1-unknown has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of gpg or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: en-core-web-md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed en-core-web-md-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora leemos una historias (cortas) y las tokenizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>Tokenized_Story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dan's parents were overweight. Dan was overwei...</td>\n",
       "      <td>[dan, 's, parents, were, overweight, ., dan, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carrie had just learned how to ride a bike. Sh...</td>\n",
       "      <td>[carrie, had, just, learned, how, to, ride, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morgan enjoyed long walks on the beach. She an...</td>\n",
       "      <td>[morgan, enjoyed, long, walks, on, the, beach,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane was working at a diner. Suddenly a custom...</td>\n",
       "      <td>[jane, was, working, at, a, diner, ., suddenly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was talking to my crush today. She continued...</td>\n",
       "      <td>[i, was, talking, to, my, crush, today, ., she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank had been drinking beer. He got a call fr...</td>\n",
       "      <td>[frank, had, been, drinking, beer, ., he, got,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dave was in the Bahamas on vacation. He decide...</td>\n",
       "      <td>[dave, was, in, the, bahamas, on, vacation, .,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sunny enjoyed going to the beach. As she stepp...</td>\n",
       "      <td>[sunny, enjoyed, going, to, the, beach, ., as,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sally was happy when her widowed mom found a n...</td>\n",
       "      <td>[sally, was, happy, when, her, widowed, mom, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dan hit his golf ball and watched it go. The b...</td>\n",
       "      <td>[dan, hit, his, golf, ball, and, watched, it, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Story  \\\n",
       "0  Dan's parents were overweight. Dan was overwei...   \n",
       "1  Carrie had just learned how to ride a bike. Sh...   \n",
       "2  Morgan enjoyed long walks on the beach. She an...   \n",
       "3  Jane was working at a diner. Suddenly a custom...   \n",
       "4  I was talking to my crush today. She continued...   \n",
       "5  Frank had been drinking beer. He got a call fr...   \n",
       "6  Dave was in the Bahamas on vacation. He decide...   \n",
       "7  Sunny enjoyed going to the beach. As she stepp...   \n",
       "8  Sally was happy when her widowed mom found a n...   \n",
       "9  Dan hit his golf ball and watched it go. The b...   \n",
       "\n",
       "                                     Tokenized_Story  \n",
       "0  [dan, 's, parents, were, overweight, ., dan, w...  \n",
       "1  [carrie, had, just, learned, how, to, ride, a,...  \n",
       "2  [morgan, enjoyed, long, walks, on, the, beach,...  \n",
       "3  [jane, was, working, at, a, diner, ., suddenly...  \n",
       "4  [i, was, talking, to, my, crush, today, ., she...  \n",
       "5  [frank, had, been, drinking, beer, ., he, got,...  \n",
       "6  [dave, was, in, the, bahamas, on, vacation, .,...  \n",
       "7  [sunny, enjoyed, going, to, the, beach, ., as,...  \n",
       "8  [sally, was, happy, when, her, widowed, mom, f...  \n",
       "9  [dan, hit, his, golf, ball, and, watched, it, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "train_stories = pd.read_csv('example_train_stories.csv', encoding='utf-8')\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "def text_to_tokens(text_seqs):\n",
    "    token_seqs = [[word.lower_ for word in nlp(text_seq)] for text_seq in text_seqs]\n",
    "    return token_seqs\n",
    "\n",
    "train_stories['Tokenized_Story'] = text_to_tokens(train_stories['Story'])\n",
    "    \n",
    "train_stories[['Story','Tokenized_Story']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construiremos el vocabulario de manera que cada palabra tenga un entero asociado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARY SAMPLE (1272 total items):\n",
      "{'dan': 2, \"'s\": 3, 'parents': 4, 'were': 5, 'overweight': 6, '.': 7, 'was': 8, 'as': 9, 'well': 10, 'the': 11, 'doctors': 12, 'told': 13, 'his': 14, 'it': 15, 'unhealthy': 16, 'understood': 17, 'and': 18, 'decided': 19, 'to': 20, 'make': 21}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def make_vocab(token_seqs, min_freq=1):\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    \n",
    "    vocab = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    \n",
    "    vocab = {token:idx + 2 for idx,token in enumerate(vocab)}\n",
    "    vocab[u'<UNK>'] = 1 \n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    print(\"VOCABULARY SAMPLE ({} total items):\".format(len(vocab)))\n",
    "    print(dict(list(vocab.items())[:20]))\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "vocab = make_vocab(token_seqs=train_stories['Tokenized_Story'], min_freq=1)\n",
    "\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocab lookup nos devuelve una palabra dado un id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOKUP SAMPLE:\n",
      "{2: 'dan', 3: \"'s\", 4: 'parents', 5: 'were', 6: 'overweight', 7: '.', 8: 'was', 9: 'as', 10: 'well', 11: 'the', 12: 'doctors', 13: 'told', 14: 'his', 15: 'it', 16: 'unhealthy', 17: 'understood', 18: 'and', 19: 'decided', 20: 'to', 21: 'make'}\n"
     ]
    }
   ],
   "source": [
    "def get_vocab_lookup(vocab):\n",
    "    vocab_lookup = {idx: vocab_item for vocab_item, idx in vocab.items()}\n",
    "    vocab_lookup[0] = \"\" \n",
    "    print(\"LOOKUP SAMPLE:\")\n",
    "    print(dict(list(vocab_lookup.items())[:20]))\n",
    "    return vocab_lookup\n",
    "\n",
    "vocab_lookup = get_vocab_lookup(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'into'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_lookup[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entonces pasamos desde palabras a enteros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized_Story</th>\n",
       "      <th>Story_Idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[dan, 's, parents, were, overweight, ., dan, w...</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 2, 8, 6, 9, 10, 7, 11, 12, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[carrie, had, just, learned, how, to, ride, a,...</td>\n",
       "      <td>[29, 30, 31, 32, 33, 20, 34, 22, 35, 7, 36, 37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[morgan, enjoyed, long, walks, on, the, beach,...</td>\n",
       "      <td>[57, 58, 59, 60, 27, 11, 61, 7, 36, 18, 41, 62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[jane, was, working, at, a, diner, ., suddenly...</td>\n",
       "      <td>[76, 8, 77, 78, 22, 79, 7, 80, 22, 81, 82, 83,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, was, talking, to, my, crush, today, ., she...</td>\n",
       "      <td>[98, 8, 99, 20, 100, 101, 102, 7, 36, 103, 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[frank, had, been, drinking, beer, ., he, got,...</td>\n",
       "      <td>[123, 30, 124, 125, 126, 7, 74, 25, 22, 127, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[dave, was, in, the, bahamas, on, vacation, .,...</td>\n",
       "      <td>[146, 8, 147, 11, 148, 27, 149, 7, 74, 19, 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[sunny, enjoyed, going, to, the, beach, ., as,...</td>\n",
       "      <td>[169, 58, 170, 20, 11, 61, 7, 9, 36, 171, 121,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[sally, was, happy, when, her, widowed, mom, f...</td>\n",
       "      <td>[182, 8, 183, 159, 41, 184, 185, 160, 22, 186,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[dan, hit, his, golf, ball, and, watched, it, ...</td>\n",
       "      <td>[2, 203, 14, 204, 205, 18, 206, 15, 63, 7, 11,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Tokenized_Story  \\\n",
       "0  [dan, 's, parents, were, overweight, ., dan, w...   \n",
       "1  [carrie, had, just, learned, how, to, ride, a,...   \n",
       "2  [morgan, enjoyed, long, walks, on, the, beach,...   \n",
       "3  [jane, was, working, at, a, diner, ., suddenly...   \n",
       "4  [i, was, talking, to, my, crush, today, ., she...   \n",
       "5  [frank, had, been, drinking, beer, ., he, got,...   \n",
       "6  [dave, was, in, the, bahamas, on, vacation, .,...   \n",
       "7  [sunny, enjoyed, going, to, the, beach, ., as,...   \n",
       "8  [sally, was, happy, when, her, widowed, mom, f...   \n",
       "9  [dan, hit, his, golf, ball, and, watched, it, ...   \n",
       "\n",
       "                                          Story_Idxs  \n",
       "0  [2, 3, 4, 5, 6, 7, 2, 8, 6, 9, 10, 7, 11, 12, ...  \n",
       "1  [29, 30, 31, 32, 33, 20, 34, 22, 35, 7, 36, 37...  \n",
       "2  [57, 58, 59, 60, 27, 11, 61, 7, 36, 18, 41, 62...  \n",
       "3  [76, 8, 77, 78, 22, 79, 7, 80, 22, 81, 82, 83,...  \n",
       "4  [98, 8, 99, 20, 100, 101, 102, 7, 36, 103, 20,...  \n",
       "5  [123, 30, 124, 125, 126, 7, 74, 25, 22, 127, 1...  \n",
       "6  [146, 8, 147, 11, 148, 27, 149, 7, 74, 19, 20,...  \n",
       "7  [169, 58, 170, 20, 11, 61, 7, 9, 36, 171, 121,...  \n",
       "8  [182, 8, 183, 159, 41, 184, 185, 160, 22, 186,...  \n",
       "9  [2, 203, 14, 204, 205, 18, 206, 15, 63, 7, 11,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens_to_idxs(token_seqs, vocab):\n",
    "    idx_seqs = [[vocab[token] if token in vocab else vocab['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "train_stories['Story_Idxs'] = tokens_to_idxs(token_seqs=train_stories['Tokenized_Story'],\n",
    "                                             vocab=vocab)\n",
    "                                   \n",
    "train_stories[['Tokenized_Story', 'Story_Idxs']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y usamos pad sequences para generar las secuencias de enteros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   22   28    7]\n",
      " [   0    0    0 ...   41   56    7]\n",
      " [   0    0    0 ...   41   75    7]\n",
      " ...\n",
      " [   0    0    0 ...  107   41    7]\n",
      " [   0    0    0 ...   11 1266    7]\n",
      " [   0    0    0 ...  176  121    7]]\n",
      "SHAPE: (100, 71)\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in train_stories['Story_Idxs']]) \n",
    "\n",
    "train_padded_idxs = pad_sequences(train_stories['Story_Idxs'], maxlen=max_seq_len + 1) \n",
    "print(train_padded_idxs) \n",
    "\n",
    "print(\"SHAPE:\", train_padded_idxs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un pandas dataframe nos muestra la relación palabra-palabra para la tarea next token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Word</th>\n",
       "      <th>Output Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>dan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dan</td>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'s</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>parents</td>\n",
       "      <td>were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>were</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>overweight</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>dan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dan</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>was</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>overweight</td>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>well</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the</td>\n",
       "      <td>doctors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doctors</td>\n",
       "      <td>told</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>told</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>his</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>parents</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>it</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>was</td>\n",
       "      <td>unhealthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>unhealthy</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>.</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>his</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>parents</td>\n",
       "      <td>understood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>understood</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>and</td>\n",
       "      <td>decided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>decided</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>to</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>make</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>a</td>\n",
       "      <td>change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>change</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>.</td>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>they</td>\n",
       "      <td>got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>got</td>\n",
       "      <td>themselves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>themselves</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>and</td>\n",
       "      <td>dan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>dan</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>on</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>a</td>\n",
       "      <td>diet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>diet</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Input Word Output Word\n",
       "0            -         dan\n",
       "1          dan          's\n",
       "2           's     parents\n",
       "3      parents        were\n",
       "4         were  overweight\n",
       "5   overweight           .\n",
       "6            .         dan\n",
       "7          dan         was\n",
       "8          was  overweight\n",
       "9   overweight          as\n",
       "10          as        well\n",
       "11        well           .\n",
       "12           .         the\n",
       "13         the     doctors\n",
       "14     doctors        told\n",
       "15        told         his\n",
       "16         his     parents\n",
       "17     parents          it\n",
       "18          it         was\n",
       "19         was   unhealthy\n",
       "20   unhealthy           .\n",
       "21           .         his\n",
       "22         his     parents\n",
       "23     parents  understood\n",
       "24  understood         and\n",
       "25         and     decided\n",
       "26     decided          to\n",
       "27          to        make\n",
       "28        make           a\n",
       "29           a      change\n",
       "30      change           .\n",
       "31           .        they\n",
       "32        they         got\n",
       "33         got  themselves\n",
       "34  themselves         and\n",
       "35         and         dan\n",
       "36         dan          on\n",
       "37          on           a\n",
       "38           a        diet\n",
       "39        diet           ."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip([\"-\"] + train_stories['Tokenized_Story'].loc[0],\n",
    "                          train_stories['Tokenized_Story'].loc[0])),\n",
    "                 columns=['Input Word', 'Output Word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usaremos un modelo muy simple para hacer generación autoregresiva (next token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model \n",
    "from keras.layers import Dense, Embedding, TimeDistributed, Input\n",
    "\n",
    "def create_model(seq_input_len, n_input_nodes, embedding_dim, \n",
    "                 stateful=False, batch_size=None):\n",
    "    \n",
    "    \n",
    "    input_layer = Input(batch_shape=(batch_size, seq_input_len), name='input_layer')\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=n_input_nodes, \n",
    "                                output_dim=embedding_dim, \n",
    "                                mask_zero=True, name='embedding_layer')(input_layer) #mask_zero ignora el padding\n",
    "    \n",
    "    output_layer = TimeDistributed(Dense(n_input_nodes, activation=\"softmax\"), \n",
    "                                   name='output_layer')(embedding_layer)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(seq_input_len=train_padded_idxs.shape[-1] - 1, #substract 1 from matrix length because of offset \n",
    "                     n_input_nodes = len(vocab) + 1, # Add 1 to account for 0 padding\n",
    "                     embedding_dim = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 70)]              0         \n",
      "                                                                 \n",
      " embedding_layer (Embedding)  (None, 70, 300)          381900    \n",
      "                                                                 \n",
      " output_layer (TimeDistribut  (None, 70, 1273)         383173    \n",
      " ed)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 765,073\n",
      "Trainable params: 765,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y el entrenamiento simplemente desplaza la secuencia de entrada en una posición y la coloca a la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=train_padded_idxs[:,:-1], y=train_padded_idxs[:, 1:, None], epochs=50, batch_size=5, verbose=False)\n",
    "model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leemos los pesos que aprendimos para inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_model = create_model(seq_input_len=1,\n",
    "                               n_input_nodes=len(vocab) + 1,\n",
    "                               embedding_dim = 300,\n",
    "                               stateful=True, \n",
    "                               batch_size = 1)\n",
    "\n",
    "predictor_model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a usar unas historias de test para explorar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stories = pd.read_csv('example_test_stories.csv', encoding='utf-8')\n",
    "test_stories['Tokenized_Story'] = text_to_tokens(test_stories['Story'])\n",
    "test_stories['Story_Idxs'] = tokens_to_idxs(token_seqs=test_stories['Tokenized_Story'], vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y hacemos generación autoregresiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ending(idx_seq):\n",
    "    \n",
    "    end_of_sent_tokens = [\".\", \"!\", \"?\"]\n",
    "    generated_ending = []\n",
    "    \n",
    "    \n",
    "    for word in idx_seq:\n",
    "        p_next_word = predictor_model.predict(np.array(word)[None,None], verbose=False)[0,0]\n",
    "        \n",
    "    while not generated_ending or vocab_lookup[next_word] not in end_of_sent_tokens:\n",
    "        #Randomly sample a word from the current probability distribution\n",
    "        next_word = np.random.choice(a=p_next_word.shape[-1], p=p_next_word)\n",
    "        # Append sampled word to generated ending\n",
    "        generated_ending.append(next_word)\n",
    "        # Get probabilities for next word by inputing sampled word\n",
    "        p_next_word = predictor_model.predict(np.array(next_word)[None,None], verbose=False)[0,0]\n",
    "    \n",
    "    model.reset_states() \n",
    "    \n",
    "    return generated_ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL STORY: lars went out skateboarding today . he skateboarded to the skate park . his friends taught him how to do a new trick . it is a difficult trick but he 's going to keep practicing .\n",
      "GIVEN ENDING: tomorrow he 'll teach his friends something new too .\n",
      "GENERATED ENDING: he needed and swam out of my egg hunt started crying . \n",
      "\n",
      "INITIAL STORY: abby is an avid scuba diver . abby 's dream has always been to scuba dive at the great barrier reef . one day abby got a letter from her father in the mail . as abby opened the letter she began crying with joy .\n",
      "GIVEN ENDING: abby 's dad was sending her to the great barrier reef to dive in may .\n",
      "GENERATED ENDING: frank had never a bath . \n",
      "\n",
      "INITIAL STORY: maggie was 100 years old . she knew her time was coming to an end soon . she gathered all of her family around her bed side . she told them her final goodbyes .\n",
      "GIVEN ENDING: maggie passed away minutes later .\n",
      "GENERATED ENDING: she bought her and took out . \n",
      "\n",
      "INITIAL STORY: july is the season for peaches . we got two buckets of peaches this year . we made jam from the peaches . we also made peach sauce for ice cream !\n",
      "GIVEN ENDING: it was hard work but all the peaches are gone now .\n",
      "GENERATED ENDING: dave was glad that was on the hospital to go away and offered her to go to make the baby . \n",
      "\n",
      "INITIAL STORY: kim was heading to the olympics . she was a well trained athlete . a month before the games she got hurt in practice . she tried her best to get better .\n",
      "GIVEN ENDING: unfortunately she had to pull out .\n",
      "GENERATED ENDING: after the candy filled eggs . \n",
      "\n",
      "INITIAL STORY: i woke up way too early . i wanted nothing more than to go back to sleep . my back started hurting and i could n't get comfortable . i rolled into every position imaginable .\n",
      "GIVEN ENDING: i finally gave up and got out of bed .\n",
      "GENERATED ENDING: her neighbor 's mom playing their rival . \n",
      "\n",
      "INITIAL STORY: my stomach was burning for several days . and i had trouble eating any kind of food . so my dad bought me a probiotic drink made with fermented cabbages . it tasted terrible .\n",
      "GIVEN ENDING: but it quickly helped my stomach .\n",
      "GENERATED ENDING: hal lured him . \n",
      "\n",
      "INITIAL STORY: anne was wearing her brand new dress . it cost her a fortune but she thought it was worth it . as she walked down the street a man with a coffee cup tripped by her . spilled coffee flew toward her .\n",
      "GIVEN ENDING: anne dodged the beverage just in time sparing her dress .\n",
      "GENERATED ENDING: he had laughing so they saw that she saw an audition came off the highway . \n",
      "\n",
      "INITIAL STORY: karen had two cats . her roommate had none . often the apartment smelled like cat urine . karen and her roommate got into constant argument .\n",
      "GIVEN ENDING: eventually karen moved out .\n",
      "GENERATED ENDING: his mom encouraged her car . \n",
      "\n",
      "INITIAL STORY: it started out as a dark and gloomy rainy day . i had to go to the store to grab some food for dinner . on the way i was passed by several ambulances . as i got closer to the store i saw the ambulances were stopped .\n",
      "GIVEN ENDING: it was a horrible crash scene and two people were dead .\n",
      "GENERATED ENDING: the voice in water and music and gets very grateful to the steward found it would ever . \n",
      "\n",
      "INITIAL STORY: i was doing mturk tasks this morning . i went to fb for a few minutes . i saw a post about jaden smith 's suicide . i found out it was a hoax .\n",
      "GIVEN ENDING: i decided to limit my fb viewing .\n",
      "GENERATED ENDING: we had to go out . \n",
      "\n",
      "INITIAL STORY: carol was trying to clean up around the apartment . she tried to vacuum . unfortunately it broke halfway through . carol did n't know how to fix it .\n",
      "GIVEN ENDING: she had to stop vacuuming entirely .\n",
      "GENERATED ENDING: the bit had to dance . \n",
      "\n",
      "INITIAL STORY: tammy was excited for christmas . she loved decorating . she started celebrating early by making a gingerbread house . it was fully detailed .\n",
      "GIVEN ENDING: all of tammy 's friends thought it looked great .\n",
      "GENERATED ENDING: he was in an hour . \n",
      "\n",
      "INITIAL STORY: tom worked in a store stock room . he had to pick up heavy things often . one time his back gave out . tom was n't able to continue working for the day .\n",
      "GIVEN ENDING: he was written up and warned .\n",
      "GENERATED ENDING: james was playing their rival . \n",
      "\n",
      "INITIAL STORY: gina posted her lemonade stand in front of her house . she spent all day sitting there and waiting for customers . several hours have past and no one showed up . gina had to throw away her lemonade jugs since they 're used .\n",
      "GIVEN ENDING: she lost a lot of her profit .\n",
      "GENERATED ENDING: they started meat luckily her how to take lessons at her belongings in pain . \n",
      "\n",
      "INITIAL STORY: krista wanted to make a nice meal for nate . she went to the store and got pasta tomatoes garlic and chicken . she went home and made her famous pasta sauce . nate came over and brought wine .\n",
      "GIVEN ENDING: krista and nate enjoyed a very nice dinner !\n",
      "GENERATED ENDING: to her car off but the producers had it . \n",
      "\n",
      "INITIAL STORY: sam was in a marathon . he was close to first place . all of a sudden he fell . he did n't even make the top ten .\n",
      "GIVEN ENDING: sam was terribly disappointed .\n",
      "GENERATED ENDING: his second day . \n",
      "\n",
      "INITIAL STORY: abby lived in a small city in florida . abby heard stories of local deer wandering into citizens yards . however abby had yet to ever see one . one day abby noticed a animal eating from her garden .\n",
      "GIVEN ENDING: abby was shocked to see a deer was eating her tomatoes .\n",
      "GENERATED ENDING: he was willing to make a few pounds in the deer were talking about her . \n",
      "\n",
      "INITIAL STORY: george was driving home . he was stuck behind a slow car . he tried to pass but that lane was taken up too . he tried flashing his high - beams but it did nothing .\n",
      "GIVEN ENDING: he had to wait the entire way .\n",
      "GENERATED ENDING: brenda was making a bunch of his house . \n",
      "\n",
      "INITIAL STORY: when i was a sophomore in college the occupy movement arrived . in my town college students were protesting with the homeless . i went one night to the protest site and met a boy . we left the protest together spray painted bill boards and drank .\n",
      "GIVEN ENDING: i learned that the protesters had no idea what they were doing .\n",
      "GENERATED ENDING: tim even had hundreds of coffee to ride lawn mowers . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for _, test_story in test_stories[:20].iterrows():\n",
    "    \n",
    "    ending_story_idx = len(list(nlp(test_story['Story']).sents)[-1])\n",
    "    print(\"INITIAL STORY:\", \" \".join(test_story['Tokenized_Story'][:-ending_story_idx]))\n",
    "    print(\"GIVEN ENDING:\", \" \".join(test_story['Tokenized_Story'][-ending_story_idx:]))\n",
    "    \n",
    "    generated_ending = generate_ending(test_story['Story_Idxs'][:-ending_story_idx])\n",
    "    generated_ending = \" \".join([vocab_lookup[word] if word in vocab_lookup else \"\"\n",
    "                                 for word in generated_ending]) \n",
    "    print(\"GENERATED ENDING:\", generated_ending, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
